{"cells":[{"cell_type":"markdown","metadata":{"id":"JBJSj7jKwVgO"},"source":["# Dataset Preparation"]},{"cell_type":"markdown","metadata":{"id":"flMLW2oSVGLQ"},"source":["## Dataset Download."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JzXEB4uf0ZrI","pycharm":{"is_executing":true}},"outputs":[],"source":["#!wget -r -N -c -np https://physionet.org/files/eegmmidb/1.0.0/\n","!pip uninstall tensorflow -y\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"C7ZWREXCJXzN"},"outputs":[],"source":["!pip install --upgrade tensorflow\n","!pip install --upgrade keras"]},{"cell_type":"markdown","metadata":{"id":"_TfZfJ4BVQN5"},"source":["## Project dependencies export"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IJEn8xQpTiNX"},"outputs":[],"source":["# import sys\n","\n","# !pip install -q --upgrade pip\n","# !pip install -q ipykernel\n","\n","# # 搜集当前notebook中的依赖\n","# !pip install -q nbconvert\n","# !jupyter nbconvert --to script EEG-User-Identification.ipynb\n","\n","# with open('EEG-User-Identification.py', 'r') as f:\n","#     lines = f.readlines()\n","\n","# ! pip install pipreqs\n","# !python -m  pipreqs.pipreqs ./\n","\n"]},{"cell_type":"markdown","metadata":{"id":"_r1xd-wpVmL0"},"source":["## Raw data clipping"]},{"cell_type":"markdown","metadata":{"id":"WrWYgI94JXzP"},"source":["### 原始eegmmidb下载数据集分割处理"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"XUBKAfC-2Ei0"},"outputs":[{"name":"stderr","output_type":"stream","text":["2023-12-12 01:28:04.311428: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n","2023-12-12 01:28:07.716159: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","2023-12-12 01:28:16.135442: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"]}],"source":["# import necessary packages*+--\n","import numpy as np\n","import mne\n","import tensorflow.keras\n","import tensorflow as tf\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Dropout, Flatten, LSTM\n","from tensorflow.keras.layers import Conv2D\n","from tensorflow.keras.layers import BatchNormalization, Activation, Reshape"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"A-Rt_uMpTiNZ","scrolled":true},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"bz_Hm3aPJXzQ"},"source":["#### data loading"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jyBafD_D2gyR"},"outputs":[],"source":["# data loading ...\n","# %cd /physionet.org/files/eegmmidb/1.0.0/\n","\n","SList = [\"%03d\"%i for i in range(1,110)]\n","RList = [\"%02d\"%i for i in range(1,15)]\n","Lists = []\n","for j in SList:\n","    name = 's'+str(j)\n","    name = np.zeros((64, 160))\n","    name = np.expand_dims(name, axis=0)\n","    for i in RList:\n","        file = os.getcwd() + '/S'+str(j)+'/S'+str(j)+'R'+str(i)+'.edf'\n","        data = mne.io.read_raw_edf(file)\n","        raw_data = data.get_data()\n","\n","        # 1 sec parts\n","        for k in range(int((np.size(raw_data,1))/160)):\n","          name = np.concatenate((name, np.expand_dims(raw_data[:, (k)*160:(k+1)*160], axis=0)), axis=0)\n","\n","        # free up memory\n","        path = []\n","        cwd = os.getcwd()\n","        file = os.getcwd()+'/S'+str(j)+'/S'+str(j)+'R'+str(i)+'.edf'\n","        path.append(os.path.join(cwd, file))\n","        os.remove(path[0])\n","    name = name[1:np.size(name, 0), :, :]\n","    subj = 'S'+str(j)\n","    np.save(subj, name)\n"]},{"cell_type":"markdown","metadata":{"id":"qE7B9yv2BS0P"},"source":["#### Channel selection"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HymjwsoL3wkZ"},"outputs":[],"source":["# Selecting Channels\n","SList = [\"%03d\"%i for i in range(1,110)]\n","RList = [\"%02d\"%i for i in range(1,15)]\n","\n","# For 3 Channels\n","channels3 = [8,10,12]\n","for j in SList:\n","  name = 'S'+str(j)+'.npy'\n","  a = np.load(name)\n","  a = a[:, channels3, :]\n","  subject = 's'+str(j)\n","  np.save(subject, a)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sva8cih3Axxp"},"outputs":[],"source":["# Selecting Channels\n","SList = [\"%03d\"%i for i in range(1,110)]\n","RList = [\"%02d\"%i for i in range(1,15)]\n","\n","# For 4 Channels\n","channels4 = [31,35,48,52]\n","for j in SList:\n","  name = 'S'+str(j)+'.npy'\n","  a = np.load(name)\n","  a = a[:, channels4, :]\n","  subject = 's'+str(j)\n","  np.save(subject, a)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ivfF-LQGA07H"},"outputs":[],"source":["# Selecting Channels\n","SList = [\"%03d\"%i for i in range(1,110)]\n","RList = [\"%02d\"%i for i in range(1,15)]\n","\n","# For 16 Channels\n","channels16 = [8,10,12,21,23,29,31,35,37,40,41,46,48,50,52,54]\n","for j in SList:\n","  name = 'S'+str(j)+'.npy'\n","  a = np.load(name)\n","  a = a[:, channels16, :]\n","  subject = 's'+str(j)\n","  np.save(subject, a)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WLaGsxhfA3Ru"},"outputs":[],"source":["# Selecting Channels\n","SList = [\"%03d\"%i for i in range(1,110)]\n","RList = [\"%02d\"%i for i in range(1,15)]\n","\n","# For 32 Channels\n","channels32 = [0,2,3,4,6,8,10,12,14,16,17,18,20,21,22,23,26,29,31,33,35,37,40,41,46,48,50,52,54,57,60,62]\n","for j in SList:\n","  name = 'S'+str(j)+'.npy'\n","  a = np.load(name)\n","  a = a[:, channels32, :]\n","  subject = 's'+str(j)\n","  np.save(subject, a)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HLPwQpghA6Nv"},"outputs":[],"source":["# Assigning p% of data for training\n","SList = [\"%03d\"%i for i in range(1,110)]\n","RList = [\"%02d\"%i for i in range(1,15)]\n","p = 0.9\n","for j in SList:\n","  name = 's'+str(j)+'.npy'\n","  a = np.load(name)\n","  shuffle = list(range(np.size(a, 0)))\n","  np.random.shuffle(shuffle)\n","  a = a[shuffle, :, :]\n","  num_tr = int(p*np.size(a, 0))\n","  x_tr = a[0:num_tr, :, :]\n","  x_te = a[num_tr+1:np.size(a,0), :, :]\n","  subject_tr = 's'+str(j)+'_train'\n","  subject_te = 's'+str(j)+'_test'\n","  np.save(subject_tr, x_tr)\n","  np.save(subject_te, x_te)\n","  print(j)"]},{"cell_type":"markdown","metadata":{"id":"HdIrUjdeUVNa"},"source":["#### Train set and Test set"]},{"cell_type":"markdown","metadata":{"id":"QCPFL4BSJXzT"},"source":["##### 非图像版本"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hrrbn8DlBHtG","scrolled":true},"outputs":[],"source":["import numpy as np\n","from tqdm import tqdm\n","SList = [\"%03d\"%i for i in range(1,110)]\n","RList = [\"%02d\"%i for i in range(1,15)]\n","\n","\n","# note that n_ch should be equal to what you selected in channel selection step!\n","# here we chose 4\n","n_ch = 32\n","y_train = np.zeros((1, 1))\n","x_train = np.zeros((1, n_ch, 160))\n","print(\"Train Dataset----------------------------------\")\n","for j in tqdm(SList):\n","    name = 's'+str(j)+'_train.npy'\n","    a = np.load(name)\n","    #print(a.shape)\n","    x_train = np.concatenate((x_train, a), axis=0)\n","    y = (int(j)-1)*np.ones((np.size(a, 0), 1))\n","    y_train = np.concatenate((y_train, y))\n","    # print(j)\n","    del a\n","    del y\n","\n","print(\"Test Dataset----------------------------------\")\n","# note that n_ch should be equal to what you selected in channel selection step!\n","# here we chose 4\n","y_test = np.zeros((1, 1))\n","x_test = np.zeros((1, n_ch, 160))\n","for j in tqdm(SList):\n","    name = 's'+str(j)+'_test.npy'\n","    a = np.load(name)\n","    x_test = np.concatenate((x_test, a), axis=0)\n","    y = (int(j)-1)*np.ones((np.size(a, 0), 1))\n","    y_test = np.concatenate((y_test, y))\n","    # print(j)\n","    del a\n","    del y\n"]},{"cell_type":"markdown","metadata":{"id":"Vds7WpBtC2NN","outputId":"5166943a-7877-4570-e3e1-4a683ed8d0ad"},"source":["##### 图像版本(本想在这里直接处理掉数据集的图像化，但是需要接近100GB内存才能放得下。暂时放弃。)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7N3bYAjgJXzT"},"outputs":[],"source":["import numpy as np\n","from tqdm import tqdm\n","\n","SList = [\"%03d\"%i for i in range(1,110)]\n","RList = [\"%02d\"%i for i in range(1,15)]\n","\n","# note that n_ch should be equal to what you selected in channel selection step!\n","n_ch = 32\n","y_train = np.zeros((1, 1))\n","x_train = np.zeros((1, 224, 224,3))\n","for j in tqdm(SList):\n","    name = 's'+str(j)+'_train.npy'\n","    a = np.load(name)\n","    a = a[..., np.newaxis]\n","    # 转化为tensor\n","    a_tensor = tf.convert_to_tensor(a, dtype=tf.float32)\n","    # 将后两个维度按照比例缩放到 224 x 224\n","    a_resized = tf.image.resize(a_tensor, [224, 224])\n","    # 转回numpy数组\n","    a_resized = a_resized.numpy()\n","    a = np.repeat(a_resized, 3, axis=-1)\n","    # print(a.shape)\n","    x_train = np.concatenate((x_train, a), axis=0)\n","    y = (int(j)-1)*np.ones((np.size(a, 0), 1))\n","    y_train = np.concatenate((y_train, y))\n","    del a,a_tensor,a_resized,y\n","\n","y_test = np.zeros((1, 1))\n","x_test = np.zeros((1, 224, 224,3))\n","for j in tqdm(SList):\n","    name = 's'+str(j)+'_test.npy'\n","    a = np.load(name)\n","    a = a[..., np.newaxis]\n","    # 转化为tensor\n","    a_tensor = tf.convert_to_tensor(a, dtype=tf.float32)\n","    # 将后两个维度按照比例缩放到 224 x 224\n","    a_resized = tf.image.resize(a_tensor, [224, 224])\n","    # 转回numpy数组\n","    a_resized = a_resized.numpy()\n","    a = np.repeat(a_resized, 3, axis=-1)\n","    # print(a.shape)\n","    x_test = np.concatenate((x_test, a), axis=0)\n","    y = (int(j)-1)*np.ones((np.size(a, 0), 1))\n","    y_test = np.concatenate((y_test, y))\n","    del a,a_tensor,a_resized,y\n"]},{"cell_type":"markdown","metadata":{"id":"5lCyDYzpUkkn"},"source":["#### Data preparation"]},{"cell_type":"markdown","metadata":{"id":"9zj0kYyDJXzU"},"source":["#### 图像格式处理转换"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dL3wITp79YCc"},"outputs":[],"source":["import tensorflow as tf\n","import numpy as np\n","# 存储为npy文件\n","np.save('EEG-32c-x_train.npy', x_train)\n","np.save('EEG-32c-x_test.npy', x_test)\n","np.save('EEG-32c-y_train.npy', y_train)\n","np.save('EEG-32c-y_test.npy', y_test)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Vf13_e9QDAlw"},"outputs":[],"source":["# 供直接加载使用\n","x_train = np.load('EEG-32c-x_train.npy')\n","y_train = np.load('EEG-32c-y_train.npy')\n","x_test = np.load('EEG-32c-x_test.npy')\n","y_test = np.load('EEG-32c-y_test.npy')\n","\n","# deleting one unwanted index\n","x_train = x_train[1:np.size(x_train, 0), :, :]\n","x_test = x_test[1:np.size(x_test, 0), :, :]\n","y_train = y_train[1:np.size(y_train, 0), :]\n","y_test = y_test[1:np.size(y_test, 0), :]\n","\n","# data shape\n","print('Shape of x_train:', np.shape(x_train))\n","print('Shape of x_test:', np.shape(x_test))\n","print('Shape of y_train:', np.shape(y_train))\n","print('Shape of y_test:', np.shape(y_test))\n","input_shape = (x_train.shape[1], x_train.shape[2], 1)\n","print('Input shape:', input_shape)\n","\n","# normalizing your data\n","maxMat = np.amax(x_train, axis=0, keepdims=True)\n","x_train = np.divide(x_train, maxMat)\n","x_test = np.divide(x_test, maxMat)\n","del maxMat\n","\n","# shuffling data\n","shuffle = list(range(np.size(x_train, 0)))\n","np.random.shuffle(shuffle)\n","x_train = x_train[shuffle, :, :]\n","y_train = y_train[shuffle, ]\n","\n","shuffle = list(range(np.size(x_test, 0)))\n","np.random.shuffle(shuffle)\n","x_test = x_test[shuffle, :, :]\n","y_test = y_test[shuffle, ]\n","\n","print('Shape of x_train after shuffling:', np.shape(x_train))\n","print('Shape of x_test after shuffling:', np.shape(x_test))\n","print('Shape of y_train after shuffling:', np.shape(y_train))\n","print('Shape of y_test after shuffling:', np.shape(y_test))\n","input_shape = (x_train.shape[1], x_train.shape[2], 1)\n","print('Input shape after shuffling:', input_shape)\n","\n","del shuffle\n","\n","x_train = x_train.reshape(x_train.shape[0], x_train.shape[1], x_train.shape[2], 1)\n","x_test = x_test.reshape(x_test.shape[0], x_test.shape[1], x_test.shape[2], 1)\n","\n","from tensorflow import keras\n","from tensorflow.keras.utils import to_categorical\n","\n","num_classes = 109\n","y_train = np.reshape(y_train, [np.size(y_train, 0), ])\n","y_train = to_categorical(y_train, num_classes)\n","y_test = np.reshape(y_test, [np.size(y_test, 0), ])\n","y_test = to_categorical(y_test, num_classes)\n","\n","print('Shape of x_train after reshaping:', np.shape(x_train))\n","print('Shape of x_test after reshaping:', np.shape(x_test))\n","print('Shape of y_train after reshaping:', np.shape(y_train))\n","print('Shape of y_test after reshaping:', np.shape(y_test))\n","input_shape = (x_train.shape[1], x_train.shape[2], 1)\n","print('Input shape after reshaping:', input_shape)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"V6VkNI0kJXzV"},"outputs":[],"source":["print(y_test[1])"]},{"cell_type":"markdown","metadata":{"id":"lVd42u1UtliH"},"source":["这里的input shape之所以是(n_ch, 160, 1)而不是(n_ch, 160)，感觉是为了跟图像识别领域的三个通道对应。"]},{"cell_type":"markdown","metadata":{"id":"mxdLSIYp9XTR"},"source":["##### EEG to PIC"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rW741XB09YlD"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","import numpy as np\n","\n","# 从 x_train 中提取出一个 32x160 的 tensor生成一个32x160的灰度图像\n","tensor = x_train[0]\n","\n","# 在最后一维添加一维，转换为 32x160x1 的 tensor\n","tensor = np.expand_dims(tensor, axis=-1)\n","gray_img = tensor\n","\n","# 显示灰度图像\n","plt.imshow(gray_img.squeeze(), cmap='gray')\n","plt.show()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pogfN4cx9dJF"},"outputs":[],"source":["for i in range(10):\n","    # 从 x_train 中提取出一个 32x160 的 tensor生成一个32x160的灰度图像\n","    tensor = x_train[i]\n","\n","    # 在最后一维添加一维，转换为 32x160x1 的 tensor\n","    # tensor = np.expand_dims(tensor, axis=-1)\n","\n","    a_tensor = tf.convert_to_tensor(tensor, dtype=tf.float32)\n","    # 将后两个维度按照比例缩放到 224 x 224\n","    a_resized = tf.image.resize(a_tensor, [224, 224])\n","    # 转回numpy数组\n","    a_resized = a_resized.numpy()\n","\n","    gray_img = a_resized\n","\n","    # 显示灰度图像\n","    plt.imshow(gray_img.squeeze() , cmap='gray')\n","    plt.show()\n"]},{"cell_type":"markdown","metadata":{"id":"hOhfyU6wJXzV"},"source":["### M3CV database"]},{"cell_type":"markdown","metadata":{"id":"fNW7eAd8JXzW"},"source":["####  静态加载"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wRBUXuv7JXzW"},"outputs":[],"source":["import numpy as np\n","# 供直接加载使用\n","X_train = np.load('Baidu-X_train.npy')\n","y_train = np.load('Baidu-X_train.npy')\n","X_val = np.load('Baidu-X_val.npy')\n","y_val = np.load('Baidu-y_val.npy')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VfdfWXpqJXzW","outputId":"3805a452-b4ca-474d-fe97-c6ce5ddff267"},"outputs":[{"name":"stderr","output_type":"stream","text":["2023-05-31 16:33:00.219414: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n","To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"]},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_46917/318204173.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmne\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtyping\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_typing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtools\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmodule_util\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_module_util\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlazy_loader\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLazyLoader\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_LazyLoader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow/python/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;31m# pylint: disable=wildcard-import,g-bad-import-order,g-import-not-at-top\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpywrap_tensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_pywrap_tensorflow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meager\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow/python/pywrap_tensorflow.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     60\u001b[0m   \u001b[0;31m# pylint: disable=wildcard-import,g-import-not-at-top,line-too-long,undefined-variable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pywrap_tensorflow_internal\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m   \u001b[0;31m# This try catch logic is because there is no bazel equivalent for py_extension.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m   \u001b[0;31m# Externally in opensource we must enable exceptions to load the shared object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["# import necessary packages*+--\n","import numpy as np\n","import mne\n","import tensorflow.keras\n","import tensorflow as tf\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Dropout, Flatten, LSTM\n","from tensorflow.keras.layers import Conv2D\n","from tensorflow.keras.layers import BatchNormalization, Activation, Reshape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EDSWTxxGJXzX"},"outputs":[],"source":["import os\n","import numpy as np\n","import scipy.io as scio\n","import pandas as pd\n","from sklearn.utils import shuffle\n","\n","os.chdir('/home/u200810216/jupyter/')\n","\n","# 读取数据\n","train_images = pd.read_csv('data/data151025/Enrollment_Info.csv')\n","val_images = pd.read_csv('data/data151025/Calibration_Info.csv')\n","\n","train_images = shuffle(train_images, random_state=0)\n","val_images = shuffle(val_images)\n","# 划分训练集和校验集\n","\n","train_image_list = train_images\n","val_image_list = val_images\n","\n","df = train_image_list\n","train_img_list = train_image_list['EpochID'].values\n","train_label_list = train_image_list['SubjectID'].values\n","\n","\n","val_img_list = val_image_list['EpochID'].values\n","val_label_list = val_image_list['SubjectID'].values"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Dkx2UoLFJXzX"},"outputs":[],"source":["import os\n","import tensorflow as tf\n","import pandas as pd\n","import numpy as np\n","import scipy.io as scio\n","from tensorflow.data import Dataset\n","from tqdm import tqdm\n","\n","class MyDataset:\n","    def __init__(self, train_img_list, val_img_list, train_label_list, val_label_list, mode='train', n_samples=10):\n","        self.mode = mode\n","        self.train_images = train_img_list[:n_samples]\n","        self.test_images = val_img_list[:n_samples]\n","        self.train_label = train_label_list[:n_samples]\n","        self.test_label = val_label_list[:n_samples]\n","\n","    def load_eeg(self, eeg_path):\n","        data = scio.loadmat(eeg_path)\n","        return data['epoch_data']\n","\n","    def __len__(self):\n","        if self.mode == 'train':\n","            return len(self.train_images)\n","        else:\n","            return len(self.test_images)\n","\n","    def get_data(self):\n","        if self.mode == 'train':\n","            image_list = self.train_images\n","            label_list = self.train_label\n","        else:\n","            image_list = self.test_images\n","            label_list = self.test_label\n","\n","        eeg_data = []\n","        eeg_labels = []\n","\n","        for img, la in tqdm(zip(image_list, label_list), total=len(image_list), desc=\"Loading EEG data\"):\n","            if self.mode == 'train':\n","                eeg_path = os.path.join('data/data151025/train/', img + '.mat')\n","            else:\n","                eeg_path = os.path.join('data/data151025/val/', img + '.mat')\n","\n","            data = self.load_eeg(eeg_path)\n","            eeg_data.append(data)\n","            eeg_labels.append(int(la[4:]) - 1)\n","\n","        eeg_data = np.array(eeg_data).astype('float32')\n","        eeg_data = eeg_data[..., np.newaxis]\n","        eeg_labels = np.array(eeg_labels).astype('int32')\n","\n","        return eeg_data, eeg_labels\n","n_samples =  None\n","# n_samples =  100\n","train_dataset = MyDataset(train_img_list, val_img_list, train_label_list, val_label_list, mode='train', n_samples=n_samples)\n","val_dataset = MyDataset(train_img_list, val_img_list, train_label_list, val_label_list, mode='test', n_samples=n_samples)\n","\n","X_train, y_train = train_dataset.get_data()\n","X_val, y_val = val_dataset.get_data()\n","\n","\n","# # Convert the data to TensorFlow Dataset\n","# train_data = Dataset.from_tensor_slices((X_train, y_train))\n","# val_data = Dataset.from_tensor_slices((X_val, y_val))\n","\n","# # Shuffle and batch the dataa\n","# train_data = train_data.shuffle(buffer_size=len(train_dataset)).batch(32)\n","# val_data = val_data.batch(32)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_bLLYhkhJXzY"},"outputs":[],"source":["train_data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mhfCMre8JXzY"},"outputs":[],"source":["import tensorflow as tf\n","import numpy as np\n","# 存储为npy文件\n","np.save('Baidu-X_train.npy', X_train)\n","np.save('Baidu-X_val.npy', X_val)\n","np.save('Baidu-y_train.npy', y_train)\n","np.save('Baidu-y_val.npy', y_val)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"e5D6NQfrJXzY"},"outputs":[],"source":["!pwd"]},{"cell_type":"markdown","metadata":{"id":"EIWR6SfEJXzY"},"source":["#### 动态加载"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Vn3v48w2JXzY","outputId":"1b9316e1-6053-4025-a124-86c9b521037b"},"outputs":[{"name":"stderr","output_type":"stream","text":["2023-05-31 16:57:13.379445: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n","To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","2023-05-31 16:57:14.385289: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n","2023-05-31 16:57:14.385425: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n","2023-05-31 16:57:14.385433: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"]},{"name":"stdout","output_type":"stream","text":["           EpochID subject  session  condition  usage\n","44959  epoch044960  sub074        1          7      1\n","26978  epoch026979  sub045        1          3      1\n","35644  epoch035645  sub059        1         10      1\n","36966  epoch036967  sub061        1          8      1\n","50637  epoch050638  sub084        1         12      1\n","['epoch044960' 'epoch026979' 'epoch035645' ... 'epoch042614' 'epoch043568'\n"," 'epoch002733'] ['sub074' 'sub045' 'sub059' ... 'sub070' 'sub072' 'sub005']\n"]}],"source":["import numpy as np\n","import mne\n","import tensorflow.keras\n","import tensorflow as tf\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Dropout, Flatten, LSTM\n","from tensorflow.keras.layers import Conv2D\n","from tensorflow.keras.layers import BatchNormalization, Activation, Reshape\n","import os\n","import numpy as np\n","import scipy.io as scio\n","import pandas as pd\n","from sklearn.utils import shuffle\n","from tensorflow.keras.utils import to_categorical\n","# os.chdir('/home/u200810216/jupyter/')\n","os.chdir('/home/u200810216/jupyter/kaggle_m3cv')\n","# # 读取数据\n","# train_images = pd.read_csv('data/data151025/Enrollment_Info.csv')\n","# val_images = pd.read_csv('data/data151025/Calibration_Info.csv')\n","\n","# 读取数据\n","train_images = pd.read_csv('Enrollment_Info.csv')\n","val_images = pd.read_csv('Calibration_Info.csv')\n","\n","# print(train_images.head(5))\n","\n","\n","train_images = shuffle(train_images, random_state=0)\n","val_images = shuffle(val_images)\n","# 划分训练集和校验集\n","\n","train_image_list = train_images\n","val_image_list = val_images\n","\n","df = train_image_list\n","train_img_list = train_image_list['EpochID'].values\n","# train_label_list = train_image_list['SubjectID'].values\n","print(train_image_list.head())\n","train_label_list = train_image_list['subject'].values\n","\n","\n","val_img_list = val_image_list['EpochID'].values\n","val_label_list = val_image_list['SubjectID'].values\n","# print(val_image_list.head())\n","# val_label_list = val_image_list['subject'].values\n","print(train_img_list,train_label_list)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kl7BaEG4JXzZ"},"outputs":[],"source":["import os\n","import tensorflow as tf\n","import pandas as pd\n","import numpy as np\n","import scipy.io as scio\n","from tensorflow.data import Dataset\n","from tqdm import tqdm\n","\n","class MyDataset:\n","    def __init__(self, train_img_list, val_img_list, train_label_list, val_label_list, mode='train', n_samples=10):\n","        self.mode = mode\n","        self.train_images = train_img_list[:n_samples]\n","        self.test_images = val_img_list[:n_samples]\n","        self.train_label = train_label_list[:n_samples]\n","        self.test_label = val_label_list[:n_samples]\n","\n","    def load_eeg(self, eeg_path):\n","        data = scio.loadmat(eeg_path)\n","        return data['epoch_data']\n","\n","    def __len__(self):\n","        if self.mode == 'train':\n","            return len(self.train_images)\n","        else:\n","            return len(self.test_images)\n","    def get_data_generator(self):\n","        if self.mode == 'train':\n","            image_list = self.train_images\n","            label_list = self.train_label\n","        else:\n","            image_list = self.test_images\n","            label_list = self.test_label\n","\n","#         for img, la in tqdm(zip(image_list, label_list), total=len(image_list), desc=\"Loading EEG data\"):\n","        for img, la in zip(image_list, label_list):\n","            if self.mode == 'train':\n","                eeg_path = os.path.join('/home/u200810216/jupyter/kaggle_m3cv/train/', img + '.mat')\n","            else:\n","                eeg_path = os.path.join('/home/u200810216/jupyter/kaggle_m3cv/val/', img + '.mat')\n","\n","            data = self.load_eeg(eeg_path)\n","            data = data[..., np.newaxis]\n","#             label = (int(la[4:])-1)*np.ones((np.size(95, 0), 1))\n","\n","            label = to_categorical((int(la[4:])-1), 95)\n","#             print(data,label)\n","            yield data, label\n","\n","n_samples= None\n","# 创建MyDataset实例\n","train_dataset = MyDataset(train_img_list, val_img_list, train_label_list, val_label_list, mode='train', n_samples=n_samples)\n","val_dataset = MyDataset(train_img_list, val_img_list, train_label_list, val_label_list, mode='test', n_samples=n_samples)\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qPyep6pbJXzZ"},"outputs":[],"source":["train_data = Dataset.from_generator(\n","    train_dataset.get_data_generator,\n","    output_signature=(\n","        tf.TensorSpec(shape=(65, 1000,1), dtype=tf.float32),\n","        tf.TensorSpec(shape=(95), dtype=tf.int32)\n","    )\n",")\n","val_data = Dataset.from_generator(\n","    val_dataset.get_data_generator,\n","    output_signature=(\n","        tf.TensorSpec(shape=(65, 1000,1), dtype=tf.float32),\n","        tf.TensorSpec(shape=(95), dtype=tf.int32)\n","    )\n",")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fcb4PUcAJXzZ","outputId":"55853cb5-ed14-41c4-dca0-9aa3ff0c893a"},"outputs":[{"data":{"text/plain":["<BatchDataset element_spec=(TensorSpec(shape=(None, 65, 1000, 1), dtype=tf.float32, name=None), TensorSpec(shape=(None, 95), dtype=tf.int32, name=None))>"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["# train_data = train_data.shuffle(buffer_size=len(train_dataset)).batch(256)\n","train_data = train_data.batch(128)\n","val_data = val_data.batch(128)\n","val_data"]},{"cell_type":"markdown","metadata":{"id":"6H6rPoyqUu3p"},"source":["# Network Architecture"]},{"cell_type":"markdown","metadata":{"id":"GmwMXz0UWCqy"},"source":["## Transfomer"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"i6Oe2YC0TiNg"},"outputs":[],"source":["modelname = \"Transformer\"\n","\n","# import keras\n","# from keras.models import Sequential\n","# from keras.layers import Dense, Dropout, Flatten, LSTM\n","# from keras.layers import Conv2D\n","# from keras.layers import BatchNormalization, Activation, Reshape\n","\n","# model = Sequential()\n","\n","# #layer1\n","# model.add(Conv2D(128, kernel_size=(n_ch, 1), padding=\"valid\", input_shape=input_shape))\n","# model.add(BatchNormalization())\n","# model.add(Activation('relu'))\n","# model.add(Reshape((model.output_shape[3],model.output_shape[2],model.output_shape[1])))\n","\n","\n","# ## Layer 2\n","# model.add(Conv2D(256, kernel_size=(128, 1), padding=\"valid\"))\n","# model.add(BatchNormalization())\n","# model.add(Activation('relu'))\n","# model.add(Reshape((model.output_shape[3],model.output_shape[2],model.output_shape[1])))\n","\n","\n","# ## Layer 3\n","# model.add(Conv2D(512, kernel_size=(256, 1), padding=\"valid\"))\n","# model.add(BatchNormalization())\n","# model.add(Activation('relu'))\n","# model.add(Reshape((model.output_shape[3],model.output_shape[2],model.output_shape[1])))\n","\n","# model.add(Flatten())\n","\n","\n","# #output layer\n","# model.add(Dense(num_classes))\n","# model.add(Activation('softmax'))\n","\n","# model.summary()\n","\n","import numpy as np\n","import tensorflow as tf\n","from tensorflow.keras import layers\n","\n","def scaled_dot_product_attention(query, key, value):\n","    matmul_qk = tf.matmul(query, key, transpose_b=True)\n","\n","    dk = tf.cast(tf.shape(key)[-1], tf.float32)\n","    scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n","\n","    attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)\n","\n","    output = tf.matmul(attention_weights, value)\n","    return output, attention_weights\n","\n","class MultiHeadAttention(layers.Layer):\n","    def __init__(self, d_model, num_heads):\n","        super(MultiHeadAttention, self).__init__()\n","        self.num_heads = num_heads\n","        self.d_model = d_model\n","\n","        assert d_model % self.num_heads == 0\n","\n","        self.depth = d_model // self.num_heads\n","\n","        self.wq = layers.Dense(d_model)\n","        self.wk = layers.Dense(d_model)\n","        self.wv = layers.Dense(d_model)\n","\n","        self.dense = layers.Dense(d_model)\n","\n","    def split_heads(self, x, batch_size):\n","        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n","        return tf.transpose(x, perm=[0, 2, 1, 3])\n","\n","    def call(self, v, k, q):\n","        batch_size = tf.shape(q)[0]\n","\n","        q = self.wq(q)\n","        k = self.wk(k)\n","        v = self.wv(v)\n","\n","        q = self.split_heads(q, batch_size)\n","        k = self.split_heads(k, batch_size)\n","        v = self.split_heads(v, batch_size)\n","\n","        scaled_attention, attention_weights = scaled_dot_product_attention(q, k, v)\n","\n","        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])\n","\n","        concat_attention = tf.reshape(scaled_attention, (batch_size, -1, self.d_model))\n","\n","        output = self.dense(concat_attention)\n","        return output\n","\n","def point_wise_feed_forward_network(d_model, dff):\n","    return tf.keras.Sequential([\n","        layers.Dense(dff, activation='relu'),\n","        layers.Dense(d_model)\n","    ])\n","\n","class EncoderLayer(layers.Layer):\n","    def __init__(self, d_model, num_heads, dff, rate=0.1):\n","        super(EncoderLayer, self).__init__()\n","\n","        self.mha = MultiHeadAttention(d_model, num_heads)\n","        self.ffn = point_wise_feed_forward_network(d_model, dff)\n","\n","        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n","        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n","\n","        self.dropout1 = layers.Dropout(rate)\n","        self.dropout2 = layers.Dropout(rate)\n","\n","    def call(self, x, training):\n","        attn_output = self.mha(x, x, x)\n","        attn_output = self.dropout1(attn_output, training=training)\n","        out1 = self.layernorm1(x + attn_output)\n","\n","        ffn_output = self.ffn(out1)\n","        ffn_output = self.dropout2(ffn_output, training=training)\n","        out2 = self.layernorm2(out1 + ffn_output)\n","\n","        return out2\n","class Transformer(layers.Layer):\n","    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size, maximum_position_encoding, rate=0.1):\n","        super(Transformer, self).__init__()\n","\n","        self.d_model = d_model\n","        self.num_layers = num_layers\n","\n","        self.embedding = layers.Embedding(input_vocab_size, d_model)\n","        self.pos_encoding = positional_encoding(maximum_position_encoding, self.d_model)\n","\n","        self.enc_layers = [EncoderLayer(d_model, num_heads, dff, rate) for _ in range(num_layers)]\n","\n","        self.dropout = layers.Dropout(rate)\n","\n","    def call(self, x, training):\n","        seq_len = tf.shape(x)[1]\n","\n","        x = self.embedding(x)\n","        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n","        x += self.pos_encoding[:, :seq_len, :]\n","\n","        x = self.dropout(x, training=training)\n","\n","        for i in range(self.num_layers):\n","            x = self.enc_layers[i](x, training)\n","\n","        return x\n","\n","def positional_encoding(position, d_model):\n","    angle_rads = get_angles(np.arange(position)[:, np.newaxis],\n","                            np.arange(d_model)[np.newaxis, :] // 2,\n","                            d_model)\n","\n","    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n","    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n","\n","    pos_encoding = angle_rads[np.newaxis, ...]\n","\n","    return tf.cast(pos_encoding, dtype=tf.float32)\n","\n","def get_angles(pos, i, d_model):\n","    angle_rates = 1 / np.power(10000, (2 * (i // 2)) / np.float32(d_model))\n","    return pos * angle_rates\n","\n","\n","# Model hyperparameters\n","num_layers = 4\n","d_model = 128\n","dff = 512\n","num_heads = 8\n","input_vocab_size = 640\n","maximum_position_encoding = 640\n","dropout_rate = 0.1\n","\n","# Build the model\n","inputs = tf.keras.Input(shape=(4, 160, 1))\n","x = layers.Flatten()(inputs)\n","x = Transformer(num_layers, d_model, num_heads, dff, input_vocab_size, maximum_position_encoding, dropout_rate)(x)\n","# x = layers.GlobalAveragePooling1D()(x)\n","x = layers.Flatten()(x)\n","x = layers.Dropout(0.1)(x)\n","x = layers.Dense(64, activation=\"relu\")(x)\n","x = layers.Dropout(0.1)(x)\n","outputs = layers.Dense(num_classes, activation=\"softmax\")(x)\n","\n","model = tf.keras.Model(inputs=inputs, outputs=outputs)\n","model.summary()\n","\n"]},{"cell_type":"markdown","metadata":{"id":"cKe0tlKPtliI"},"source":["## Vision Transformer"]},{"cell_type":"markdown","metadata":{"id":"xQUc7P2rtliI"},"source":["**不同于简单的Audio classification或Image classification！而是介于两者之间**"]},{"cell_type":"markdown","metadata":{"id":"wzIZc_XgsPMm"},"source":["### Preprocess"]},{"cell_type":"markdown","metadata":{"id":"F05Dk9x9sPMm"},"source":["The next step is to load a ViT image processor to process the image into a tensor:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RvKYDoY8sPMn"},"outputs":[],"source":["from transformers import AutoImageProcessor\n","\n","checkpoint = \"google/vit-base-patch16-224-in21k\""]},{"cell_type":"markdown","metadata":{"id":"Pir0TKBasPMn"},"source":["To avoid overfitting and to make the model more robust, add some data augmentation to the training part of the dataset.\n","Here we use Keras preprocessing layers to define the transformations for the training data (includes data augmentation),\n","and transformations for the validation data (only center cropping, resizing and normalizing). You can use `tf.image`or\n","any other library you prefer."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ymzka9bqsPMn"},"outputs":[],"source":["from tensorflow import keras\n","from tensorflow.keras import layers\n","\n","size = (224, 224)"]},{"cell_type":"markdown","metadata":{"id":"bvmILYQrsPMo"},"source":["As a final preprocessing step, create a batch of examples using `DefaultDataCollator`. Unlike other data collators in 🤗 Transformers, the\n","`DefaultDataCollator` does not apply additional preprocessing, such as padding."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Hf3UbR7xsPMo"},"outputs":[],"source":["from transformers import DefaultDataCollator\n","\n","data_collator = DefaultDataCollator(return_tensors=\"tf\")"]},{"cell_type":"markdown","metadata":{"id":"JF9XImJOsPMo"},"source":["### Evaluate"]},{"cell_type":"markdown","metadata":{"id":"o1OgtdV1sPMo"},"source":["Including a metric during training is often helpful for evaluating your model's performance. You can quickly load an\n","evaluation method with the 🤗 [Evaluate](https://huggingface.co/docs/evaluate/index) library. For this task, load\n","the [accuracy](https://huggingface.co/spaces/evaluate-metric/accuracy) metric (see the 🤗 Evaluate [quick tour](https://huggingface.co/docs/evaluate/a_quick_tour) to learn more about how to load and compute a metric):"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3VyP9-WesPMo"},"outputs":[],"source":["import evaluate\n","\n","accuracy = evaluate.load(\"accuracy\")"]},{"cell_type":"markdown","metadata":{"id":"B0l_Z-BEsPMo"},"source":["Then create a function that passes your predictions and labels to [compute](https://huggingface.co/docs/evaluate/main/en/package_reference/main_classes#evaluate.EvaluationModule.compute) to calculate the accuracy:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_ri90sLhsPMp"},"outputs":[],"source":["import numpy as np\n","\n","\n","def compute_metrics(eval_pred):\n","    predictions, labels = eval_pred\n","    predictions = np.argmax(predictions, axis=1)\n","    return accuracy.compute(predictions=predictions, references=labels)"]},{"cell_type":"markdown","metadata":{"id":"ZwReSeEFsPMp"},"source":["Your `compute_metrics` function is ready to go now, and you'll return to it when you set up your training."]},{"cell_type":"markdown","metadata":{"id":"FNXYWYl3sPMp"},"source":["### Train"]},{"cell_type":"markdown","metadata":{"id":"06GqhNMBsPMp"},"source":["<Tip>\n","\n","If you are unfamiliar with fine-tuning a model with Keras, check out the [basic tutorial](https://huggingface.co/docs/transformers/main/en/tasks/./training#train-a-tensorflow-model-with-keras) first!\n","\n","</Tip>\n","\n","To fine-tune a model in TensorFlow, follow these steps:\n","1. Define the training hyperparameters, and set up an optimizer and a learning rate schedule.\n","2. Instantiate a pre-treined model.\n","3. Convert a 🤗 Dataset to a `tf.data.Dataset`.\n","4. Compile your model.\n","5. Add callbacks and use the `fit()` method to run the training.\n","6. Upload your model to 🤗 Hub to share with the community.\n","\n","Start by defining the hyperparameters, optimizer and learning rate schedule:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hn3m-5AmsPMp"},"outputs":[],"source":["from transformers import create_optimizer\n","\n","batch_size = 16\n","num_epochs = 5\n","num_train_steps = len(x_train) * num_epochs\n","learning_rate = 3e-5\n","weight_decay_rate = 0.01\n","\n","optimizer, lr_schedule = create_optimizer(\n","    init_lr=learning_rate,\n","    num_train_steps=num_train_steps,\n","    weight_decay_rate=weight_decay_rate,\n","    num_warmup_steps=0,\n",")"]},{"cell_type":"markdown","metadata":{"id":"Wb6CDbNBsPMp"},"source":["Then, load ViT with [TFAutoModelForImageClassification](https://huggingface.co/docs/transformers/main/en/model_doc/auto#transformers.TFAutoModelForImageClassification) along with the label mappings:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Mlgc1XylsPMp"},"outputs":[],"source":["from transformers import TFAutoModelForImageClassification\n","\n","model = TFAutoModelForImageClassification.from_pretrained(\n","    checkpoint,\n","\n",")"]},{"cell_type":"markdown","metadata":{"id":"2utXMbbDsPMp"},"source":["Convert your datasets to the `tf.data.Dataset` format using the [to_tf_dataset](https://huggingface.co/docs/datasets/main/en/package_reference/main_classes#datasets.Dataset.to_tf_dataset) and your `data_collator`:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yrhNltYYsPMq"},"outputs":[],"source":["# 转换x_train和y_train为tf.data.Dataset格式\n","train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n","\n","# 转换x_test和y_test为tf.data.Dataset格式\n","test_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test))\n","\n","# 对数据进行shuffle、batch等操作\n","tf_train_dataset = train_dataset.batch(batch_size)\n","tf_eval_dataset = test_dataset.batch(batch_size)"]},{"cell_type":"markdown","metadata":{"id":"_gVsisYxsPMq"},"source":["Configure the model for training with `compile()`:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FtNNZScwsPMq"},"outputs":[],"source":["from tensorflow.keras.losses import SparseCategoricalCrossentropy\n","\n","loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n","model.compile(optimizer=optimizer, loss=loss)"]},{"cell_type":"markdown","metadata":{"id":"OT9zDQo2sPMq"},"source":["To compute the accuracy from the predictions and push your model to the 🤗 Hub, use [Keras callbacks](https://huggingface.co/docs/transformers/main/en/tasks/../main_classes/keras_callbacks).\n","Pass your `compute_metrics` function to [KerasMetricCallback](https://huggingface.co/docs/transformers/main/en/tasks/../main_classes/keras_callbacks#transformers.KerasMetricCallback),\n","and use the [PushToHubCallback](https://huggingface.co/docs/transformers/main/en/tasks/../main_classes/keras_callbacks#transformers.PushToHubCallback) to upload the model:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"z-yy2CLUsPMj"},"outputs":[],"source":["from huggingface_hub import notebook_login\n","\n","notebook_login()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1gpJEkmqsPMq"},"outputs":[],"source":["from transformers.keras_callbacks import KerasMetricCallback, PushToHubCallback\n","\n","metric_callback = KerasMetricCallback(metric_fn=compute_metrics, eval_dataset=tf_eval_dataset)\n","# push_to_hub_callback = PushToHubCallback(\n","#     output_dir=\"EEG_classifier\",\n","#     tokenizer=image_processor,\n","#     save_strategy=\"no\",\n","# )\n","callbacks = [metric_callback]"]},{"cell_type":"markdown","metadata":{"id":"TzGb0ETisPMq"},"source":["Finally, you are ready to train your model! Call `fit()` with your training and validation datasets, the number of epochs,\n","and your callbacks to fine-tune the model:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2QDBPChxsPMq"},"outputs":[],"source":["model.fit(tf_train_dataset, validation_data=tf_eval_dataset, epochs=num_epochs, callbacks=callbacks)"]},{"cell_type":"markdown","metadata":{"id":"DlZdefyitliJ"},"source":["## Vision Transfomer 新实现(eegmmidb)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0mXHBe-5JXzf"},"outputs":[],"source":["import tensorflow as tf\n","from transformers import TFViTModel, ViTConfig\n","from tensorflow.keras.callbacks import ModelCheckpoint\n","os.chdir('/home/u200810216/jupyter/')\n","\n","# 加载预先训练好的 ViT 模型的配置\n","config = ViTConfig.from_pretrained(\"google/vit-base-patch16-224-in21k\", num_labels=109)\n","\n","# 修改配置，使其适用于单通道图像分类任务\n","config.input_channels = 1\n","config.hidden_dropout_prob = 0.0\n","config.attention_probs_dropout_prob = 0.0\n","\n","# 加载预先训练好的 ViT 模型的权重\n","vit = TFViTModel.from_pretrained(\"google/vit-base-patch16-224-in21k\", config=config)\n","#vit.save_pretrained(\"./vitmodel\")\n","\n","# 使用单通道图像进行分类\n","inputs = tf.keras.layers.Input(shape=(32, 160, 1))\n","x = tf.image.resize(inputs, (224, 224))\n","x = tf.keras.layers.Concatenate(axis=-1)([x, x, x])\n","x = tf.keras.layers.Lambda(lambda x: tf.transpose(x, [0, 3, 1, 2]))(x)\n","outputs = vit(x).last_hidden_state[:, 0, :]\n","\n","# 添加输出层\n","outputs = tf.keras.layers.Dense(109, activation='softmax')(outputs)\n","model = tf.keras.models.Model(inputs=inputs, outputs=outputs)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"P0qa9_f9JXzf"},"outputs":[],"source":["from transformers import create_optimizer\n","from tensorflow.keras.losses import SparseCategoricalCrossentropy\n","from tensorflow.keras.models import load_model\n","batch_size = 16\n","num_epochs = 5\n","num_train_steps = len(x_train) * num_epochs\n","learning_rate = 3e-5\n","weight_decay_rate = 0.01\n","epochs = 100\n","\n","optimizer, lr_schedule = create_optimizer(\n","    init_lr=learning_rate,\n","    num_train_steps=num_train_steps,\n","    weight_decay_rate=weight_decay_rate,\n","    num_warmup_steps=0,\n",")\n","loss = tf.keras.losses.CategoricalCrossentropy(from_logits=False)\n","\n","\n","# 记录日志\n","\n","log_dir = \"logs/transfomer\"\n","tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n","\n","\n","model.compile(loss=loss,\n","              optimizer=optimizer,\n","              metrics=['accuracy',\n","                       'FalsePositives',\n","                       'FalseNegatives'])\n","\n","\n","# 定义一个回调函数来保存模型和历史记录\n","checkpoint_callback = ModelCheckpoint(\n","    filepath='transfomer.h5',\n","    save_weights_only=False,\n","    save_best_only=True,\n","    monitor='val_loss',\n","    verbose=1,\n",")\n","\n","\n","# 训练模型\n","history = model.fit(x_train, y_train,\n","          batch_size=batch_size,\n","          epochs=epochs,\n","          shuffle=True,\n","          verbose=1,\n","          validation_split=0.2, callbacks=[tensorboard_callback, checkpoint_callback])\n","\n","# 保存模型\n","model.save('logs/transfomer/vision_transfomer_test.h5')  # 保存为 .h5 文件格式\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mF9-1ft-JXzg"},"outputs":[],"source":["# 保存模型\n","model.save('vision_transfomer_last.h5')  # 保存为 .h5 文件格式"]},{"cell_type":"markdown","metadata":{"id":"oN9TaJ7QJXzg"},"source":["## Vision Transfomer 新实现(百度数据集)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PAPZ8rViJXzg"},"outputs":[],"source":["import os\n","import tensorflow as tf\n","from transformers import TFViTModel, ViTConfig\n","from tensorflow.keras.callbacks import ModelCheckpoint\n","# os.chdir('/home/u200810216/jupyter/')\n","# tf.debugging.set_log_device_placement(True)\n","\n","# 加载预先训练好的 ViT 模型的配置\n","config = ViTConfig.from_pretrained(\"google/vit-base-patch16-224-in21k\", num_labels=95)\n","\n","# 修改配置，使其适用于单通道图像分类任务\n","config.input_channels = 1\n","config.hidden_dropout_prob = 0.0\n","config.attention_probs_dropout_prob = 0.0"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CcYGPn8sJXzg"},"outputs":[],"source":["from transformers import create_optimizer\n","from tensorflow.keras.losses import SparseCategoricalCrossentropy\n","from tensorflow.keras.models import load_model\n","from tensorflow.keras.callbacks import ModelCheckpoint\n","import tensorflow as tf\n","\n","# 之后是您的其他代码\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MsEX5uRFJXzg","outputId":"ffbedd2f-9889-4f5e-a452-42a2664a9afb"},"outputs":[{"name":"stdout","output_type":"stream","text":["INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0',)\n"]}],"source":["# 使用tf.distribute.MirroredStrategy\n","strategy = tf.distribute.MirroredStrategy()\n","#"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JSB5-QjoJXzh","outputId":"bccb3e93-17d2-4c8e-b29c-313d3f5afa52"},"outputs":[{"name":"stderr","output_type":"stream","text":["All model checkpoint layers were used when initializing TFViTModel.\n","\n","All the layers of TFViTModel were initialized from the model checkpoint at google/vit-base-patch16-224-in21k.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use TFViTModel for predictions without further training.\n"]}],"source":["batch_size = 16\n","epochs = 5\n","num_train_steps = len(train_img_list) * epochs\n","learning_rate = 3e-5\n","weight_decay_rate = 0.01\n","\n","\n","\n","\n","# with strategy.scope():\n","    # 加载预先训练好的 ViT 模型的权重\n","vit = TFViTModel.from_pretrained(\"google/vit-base-patch16-224-in21k\", config=config)\n","#vit.save_pretrained(\"./vitmodel\")\n","\n","# 使用单通道图像进行分类\n","inputs = tf.keras.layers.Input(shape=(65, 1000, 1))# 这里等待修改\n","x = tf.image.resize(inputs, (224, 224))\n","x = tf.keras.layers.Concatenate(axis=-1)([x, x, x])\n","x = tf.keras.layers.Lambda(lambda x: tf.transpose(x, [0, 3, 1, 2]))(x)\n","outputs = vit(x).last_hidden_state[:, 0, :]\n","\n","# 添加输出层\n","outputs = tf.keras.layers.Dense(95, activation='softmax')(outputs)\n","# outputs = tf.keras.layers.Lambda(lambda x: tf.argmax(x, axis=1))(outputs)\n","\n","model = tf.keras.models.Model(inputs=inputs, outputs=outputs)\n","optimizer, lr_schedule = create_optimizer(\n","    init_lr=learning_rate,\n","    num_train_steps=num_train_steps,\n","    weight_decay_rate=weight_decay_rate,\n","    num_warmup_steps=1,\n",")\n","loss = tf.keras.losses.CategoricalCrossentropy(from_logits=False)\n","# loss = tf.keras.losses.SparseCategoricalCrossentropy()\n","\n","\n","model.compile(loss=loss,\n","              optimizer=optimizer,\n","              metrics=['accuracy',\n","                       'FalsePositives',\n","                       'FalseNegatives'])\n","\n","\n","# 记录日志\n","log_dir = \"logs/transfomer\"\n","tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n","# 定义一个回调函数来保存模型和历史记录\n","checkpoint_callback = ModelCheckpoint(\n","    filepath='transfomer.h5',\n","    save_weights_only=True,\n","    save_best_only=False,\n","    monitor='val_loss',\n","    verbose=1,\n",")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IxX7pK0yJXzh"},"outputs":[],"source":["len(train_img_list)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"W1pJQpakJXzh"},"outputs":[],"source":["# 训练模型\n","history = model.fit(train_data,\n","                    epochs=25,\n","                    verbose=1,\n","                    validation_data=val_data,\n","                    callbacks=[tensorboard_callback, checkpoint_callback])\n","# 保存模型权重而不是整个模型\n","model.save_weights('/home/u200810216/jupyter/kaggle_m3cv/vision_transfomer_test_kaggle_batch128_weights.h5')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2YFSppyJJXzi"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xUs0bunWJXzi","outputId":"50153d8f-e048-489e-ee42-a47454ae8f1c"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/2\n","     96/Unknown - 108s 1s/step - loss: 0.3789 - accuracy: 0.8876 - false_positives: 794.0000 - false_negatives: 1815.0000\n","Epoch 1: saving model to transfomer.h5\n","96/96 [==============================] - 230s 2s/step - loss: 0.3789 - accuracy: 0.8876 - false_positives: 794.0000 - false_negatives: 1815.0000 - val_loss: 0.2092 - val_accuracy: 0.9446 - val_false_positives: 398.0000 - val_false_negatives: 949.0000\n","Epoch 2/2\n","96/96 [==============================] - ETA: 0s - loss: 0.1722 - accuracy: 0.9568 - false_positives: 311.0000 - false_negatives: 750.0000\n","Epoch 2: saving model to transfomer.h5\n","96/96 [==============================] - 233s 2s/step - loss: 0.1722 - accuracy: 0.9568 - false_positives: 311.0000 - false_negatives: 750.0000 - val_loss: 0.1037 - val_accuracy: 0.9767 - val_false_positives: 171.0000 - val_false_negatives: 421.0000\n"]}],"source":["#\n","model.load_weights('/home/u200810216/jupyter/kaggle_m3cv/vision_transfomer_test_kaggle_batch128_weights_val_1.h5')\n","history = model.fit(val_data,\n","                    epochs=2,\n","                    verbose=1,\n","                    validation_data=val_data,\n","                    callbacks=[tensorboard_callback, checkpoint_callback])\n","\n","# 保存模型\n","model.save_weights('/home/u200810216/jupyter/kaggle_m3cv/vision_transfomer_test_kaggle_batch128_weights_val_3.h5')  # 保存为 .h5 文件格式\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ojITEfqqJXzi","outputId":"26cd96d7-5413-4cd7-821d-cd710d3e139e"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/3\n","    452/Unknown - 1503s 3s/step - loss: 0.0604 - accuracy: 0.9852 - false_positives: 614.0000 - false_negatives: 1015.0000\n","Epoch 1: saving model to transfomer.h5\n","452/452 [==============================] - 1755s 4s/step - loss: 0.0604 - accuracy: 0.9852 - false_positives: 614.0000 - false_negatives: 1015.0000 - val_loss: 2.9653 - val_accuracy: 0.4057 - val_false_positives: 5381.0000 - val_false_negatives: 7663.0000\n","Epoch 2/3\n","452/452 [==============================] - ETA: 0s - loss: 0.0021 - accuracy: 1.0000 - false_positives: 2.0000 - false_negatives: 2.0000\n","Epoch 2: saving model to transfomer.h5\n","452/452 [==============================] - 1017s 2s/step - loss: 0.0021 - accuracy: 1.0000 - false_positives: 2.0000 - false_negatives: 2.0000 - val_loss: 3.0420 - val_accuracy: 0.4031 - val_false_positives: 5474.0000 - val_false_negatives: 7679.0000\n","Epoch 3/3\n","452/452 [==============================] - ETA: 0s - loss: 0.0011 - accuracy: 1.0000 - false_positives: 0.0000e+00 - false_negatives: 0.0000e+00\n","Epoch 3: saving model to transfomer.h5\n","452/452 [==============================] - 748s 2s/step - loss: 0.0011 - accuracy: 1.0000 - false_positives: 0.0000e+00 - false_negatives: 0.0000e+00 - val_loss: 3.1344 - val_accuracy: 0.3949 - val_false_positives: 5617.0000 - val_false_negatives: 7744.0000\n"]}],"source":["model.load_weights('/home/u200810216/jupyter/kaggle_m3cv/vision_transfomer_test_kaggle_batch128_weights_val_3.h5')\n","history = model.fit(train_data,\n","                    epochs=3,\n","                    verbose=1,\n","                    validation_data=val_data,\n","                    callbacks=[tensorboard_callback, checkpoint_callback])\n","\n","# 保存模型\n","model.save_weights('/home/u200810216/jupyter/kaggle_m3cv/vision_transfomer_test_kaggle_batch128_weights_val_3_train_3.h5')  # 保存为 .h5 文件格式\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"K9HBKl-eJXzi","outputId":"9c81d2a3-a24c-4ec0-d28b-bf6a7c52929c"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/64\n","Training on train_data\n","    452/Unknown - 1362s 3s/step - loss: 0.0280 - accuracy: 0.9936 - false_positives: 261.0000 - false_negatives: 500.0000\n","Epoch 1: saving model to transfomer.h5\n","452/452 [==============================] - 1369s 3s/step - loss: 0.0280 - accuracy: 0.9936 - false_positives: 261.0000 - false_negatives: 500.0000\n","Epoch 2/64\n","Training on val_data\n","     96/Unknown - 185s 2s/step - loss: 0.1188 - accuracy: 0.9651 - false_positives: 297.0000 - false_negatives: 555.0000\n","Epoch 1: saving model to transfomer.h5\n","96/96 [==============================] - 191s 2s/step - loss: 0.1188 - accuracy: 0.9651 - false_positives: 297.0000 - false_negatives: 555.0000\n","Epoch 3/64\n","Training on train_data\n","    452/Unknown - 716s 2s/step - loss: 0.0218 - accuracy: 0.9956 - false_positives: 191.0000 - false_negatives: 344.0000\n","Epoch 1: saving model to transfomer.h5\n","452/452 [==============================] - 722s 2s/step - loss: 0.0218 - accuracy: 0.9956 - false_positives: 191.0000 - false_negatives: 344.0000\n","Epoch 4/64\n","Training on val_data\n","     96/Unknown - 149s 2s/step - loss: 0.0775 - accuracy: 0.9784 - false_positives: 206.0000 - false_negatives: 339.0000\n","Epoch 1: saving model to transfomer.h5\n","96/96 [==============================] - 154s 2s/step - loss: 0.0775 - accuracy: 0.9784 - false_positives: 206.0000 - false_negatives: 339.0000\n","Epoch 5/64\n","Training on train_data\n","    452/Unknown - 854s 2s/step - loss: 0.0163 - accuracy: 0.9969 - false_positives: 135.0000 - false_negatives: 237.0000\n","Epoch 1: saving model to transfomer.h5\n","452/452 [==============================] - 859s 2s/step - loss: 0.0163 - accuracy: 0.9969 - false_positives: 135.0000 - false_negatives: 237.0000\n","Epoch 6/64\n","Training on val_data\n","     96/Unknown - 252s 3s/step - loss: 0.0587 - accuracy: 0.9835 - false_positives: 168.0000 - false_negatives: 248.0000\n","Epoch 1: saving model to transfomer.h5\n","96/96 [==============================] - 257s 3s/step - loss: 0.0587 - accuracy: 0.9835 - false_positives: 168.0000 - false_negatives: 248.0000\n","Epoch 7/64\n","Training on train_data\n","    452/Unknown - 869s 2s/step - loss: 0.0129 - accuracy: 0.9978 - false_positives: 91.0000 - false_negatives: 183.0000\n","Epoch 1: saving model to transfomer.h5\n","452/452 [==============================] - 875s 2s/step - loss: 0.0129 - accuracy: 0.9978 - false_positives: 91.0000 - false_negatives: 183.0000\n","Epoch 8/64\n","Training on val_data\n","     96/Unknown - 182s 2s/step - loss: 0.0413 - accuracy: 0.9892 - false_positives: 104.0000 - false_negatives: 171.0000\n","Epoch 1: saving model to transfomer.h5\n","96/96 [==============================] - 187s 2s/step - loss: 0.0413 - accuracy: 0.9892 - false_positives: 104.0000 - false_negatives: 171.0000\n","Epoch 9/64\n","Training on train_data\n","    452/Unknown - 715s 2s/step - loss: 0.0336 - accuracy: 0.9916 - false_positives: 322.0000 - false_negatives: 622.0000\n","Epoch 1: saving model to transfomer.h5\n","452/452 [==============================] - 721s 2s/step - loss: 0.0336 - accuracy: 0.9916 - false_positives: 322.0000 - false_negatives: 622.0000\n","Epoch 10/64\n","Training on val_data\n","     96/Unknown - 141s 1s/step - loss: 0.0358 - accuracy: 0.9917 - false_positives: 77.0000 - false_negatives: 132.0000\n","Epoch 1: saving model to transfomer.h5\n","96/96 [==============================] - 146s 2s/step - loss: 0.0358 - accuracy: 0.9917 - false_positives: 77.0000 - false_negatives: 132.0000\n","Epoch 11/64\n","Training on train_data\n","    452/Unknown - 585s 1s/step - loss: 0.0098 - accuracy: 0.9980 - false_positives: 95.0000 - false_negatives: 142.0000\n","Epoch 1: saving model to transfomer.h5\n","452/452 [==============================] - 591s 1s/step - loss: 0.0098 - accuracy: 0.9980 - false_positives: 95.0000 - false_negatives: 142.0000\n","Epoch 12/64\n","Training on val_data\n","     96/Unknown - 114s 1s/step - loss: 0.0228 - accuracy: 0.9951 - false_positives: 45.0000 - false_negatives: 71.0000\n","Epoch 1: saving model to transfomer.h5\n","96/96 [==============================] - 119s 1s/step - loss: 0.0228 - accuracy: 0.9951 - false_positives: 45.0000 - false_negatives: 71.0000\n","Epoch 13/64\n","Training on train_data\n","    452/Unknown - 553s 1s/step - loss: 0.0172 - accuracy: 0.9957 - false_positives: 212.0000 - false_negatives: 289.0000\n","Epoch 1: saving model to transfomer.h5\n","452/452 [==============================] - 558s 1s/step - loss: 0.0172 - accuracy: 0.9957 - false_positives: 212.0000 - false_negatives: 289.0000\n","Epoch 14/64\n","Training on val_data\n","     96/Unknown - 115s 1s/step - loss: 0.0225 - accuracy: 0.9952 - false_positives: 48.0000 - false_negatives: 78.0000\n","Epoch 1: saving model to transfomer.h5\n","96/96 [==============================] - 120s 1s/step - loss: 0.0225 - accuracy: 0.9952 - false_positives: 48.0000 - false_negatives: 78.0000\n","Epoch 15/64\n","Training on train_data\n","    452/Unknown - 557s 1s/step - loss: 0.0133 - accuracy: 0.9969 - false_positives: 156.0000 - false_negatives: 215.0000\n","Epoch 1: saving model to transfomer.h5\n","452/452 [==============================] - 563s 1s/step - loss: 0.0133 - accuracy: 0.9969 - false_positives: 156.0000 - false_negatives: 215.0000\n","Epoch 16/64\n","Training on val_data\n","     96/Unknown - 114s 1s/step - loss: 0.0225 - accuracy: 0.9945 - false_positives: 53.0000 - false_negatives: 78.0000\n","Epoch 1: saving model to transfomer.h5\n","96/96 [==============================] - 120s 1s/step - loss: 0.0225 - accuracy: 0.9945 - false_positives: 53.0000 - false_negatives: 78.0000\n","Epoch 17/64\n","Training on train_data\n","    452/Unknown - 759s 2s/step - loss: 0.0090 - accuracy: 0.9982 - false_positives: 86.0000 - false_negatives: 128.0000\n","Epoch 1: saving model to transfomer.h5\n","452/452 [==============================] - 765s 2s/step - loss: 0.0090 - accuracy: 0.9982 - false_positives: 86.0000 - false_negatives: 128.0000\n","Epoch 18/64\n","Training on val_data\n","     96/Unknown - 235s 2s/step - loss: 0.0195 - accuracy: 0.9962 - false_positives: 38.0000 - false_negatives: 66.0000\n","Epoch 1: saving model to transfomer.h5\n","96/96 [==============================] - 240s 2s/step - loss: 0.0195 - accuracy: 0.9962 - false_positives: 38.0000 - false_negatives: 66.0000\n","Epoch 19/64\n","Training on train_data\n","    452/Unknown - 884s 2s/step - loss: 0.0182 - accuracy: 0.9953 - false_positives: 221.0000 - false_negatives: 323.0000\n","Epoch 1: saving model to transfomer.h5\n","452/452 [==============================] - 890s 2s/step - loss: 0.0182 - accuracy: 0.9953 - false_positives: 221.0000 - false_negatives: 323.0000\n","Epoch 20/64\n","Training on val_data\n","     96/Unknown - 141s 1s/step - loss: 0.0288 - accuracy: 0.9923 - false_positives: 76.0000 - false_negatives: 113.0000\n","Epoch 1: saving model to transfomer.h5\n","96/96 [==============================] - 146s 2s/step - loss: 0.0288 - accuracy: 0.9923 - false_positives: 76.0000 - false_negatives: 113.0000\n","Epoch 21/64\n","Training on train_data\n","    452/Unknown - 712s 2s/step - loss: 0.0142 - accuracy: 0.9964 - false_positives: 174.0000 - false_negatives: 247.0000\n","Epoch 1: saving model to transfomer.h5\n","452/452 [==============================] - 717s 2s/step - loss: 0.0142 - accuracy: 0.9964 - false_positives: 174.0000 - false_negatives: 247.0000\n","Epoch 22/64\n","Training on val_data\n","     96/Unknown - 142s 1s/step - loss: 0.0169 - accuracy: 0.9963 - false_positives: 38.0000 - false_negatives: 53.0000\n","Epoch 1: saving model to transfomer.h5\n","96/96 [==============================] - 147s 2s/step - loss: 0.0169 - accuracy: 0.9963 - false_positives: 38.0000 - false_negatives: 53.0000\n","Epoch 23/64\n","Training on train_data\n","    452/Unknown - 817s 2s/step - loss: 0.0146 - accuracy: 0.9961 - false_positives: 188.0000 - false_negatives: 259.0000\n","Epoch 1: saving model to transfomer.h5\n","452/452 [==============================] - 823s 2s/step - loss: 0.0146 - accuracy: 0.9961 - false_positives: 188.0000 - false_negatives: 259.0000\n","Epoch 24/64\n","Training on val_data\n","     96/Unknown - 148s 2s/step - loss: 0.0197 - accuracy: 0.9952 - false_positives: 46.0000 - false_negatives: 74.0000\n","Epoch 1: saving model to transfomer.h5\n","96/96 [==============================] - 153s 2s/step - loss: 0.0197 - accuracy: 0.9952 - false_positives: 46.0000 - false_negatives: 74.0000\n","Epoch 25/64\n","Training on train_data\n","    452/Unknown - 825s 2s/step - loss: 0.0136 - accuracy: 0.9964 - false_positives: 181.0000 - false_negatives: 234.0000\n","Epoch 1: saving model to transfomer.h5\n","452/452 [==============================] - 830s 2s/step - loss: 0.0136 - accuracy: 0.9964 - false_positives: 181.0000 - false_negatives: 234.0000\n","Epoch 26/64\n","Training on val_data\n","     96/Unknown - 168s 2s/step - loss: 0.0146 - accuracy: 0.9965 - false_positives: 34.0000 - false_negatives: 52.0000\n","Epoch 1: saving model to transfomer.h5\n","96/96 [==============================] - 173s 2s/step - loss: 0.0146 - accuracy: 0.9965 - false_positives: 34.0000 - false_negatives: 52.0000\n","Epoch 27/64\n","Training on train_data\n","     96/Unknown - 116s 1s/step - loss: 0.0142 - accuracy: 0.9968 - false_positives: 30.0000 - false_negatives: 45.0000\n","Epoch 1: saving model to transfomer.h5\n","96/96 [==============================] - 121s 1s/step - loss: 0.0142 - accuracy: 0.9968 - false_positives: 30.0000 - false_negatives: 45.0000\n","Epoch 29/64\n","Training on train_data\n","    452/Unknown - 673s 1s/step - loss: 0.0125 - accuracy: 0.9970 - false_positives: 148.0000 - false_negatives: 203.0000\n","Epoch 1: saving model to transfomer.h5\n","452/452 [==============================] - 679s 2s/step - loss: 0.0125 - accuracy: 0.9970 - false_positives: 148.0000 - false_negatives: 203.0000\n","Epoch 30/64\n","Training on val_data\n","     96/Unknown - 143s 1s/step - loss: 0.0156 - accuracy: 0.9965 - false_positives: 34.0000 - false_negatives: 49.0000\n","Epoch 1: saving model to transfomer.h5\n","96/96 [==============================] - 148s 2s/step - loss: 0.0156 - accuracy: 0.9965 - false_positives: 34.0000 - false_negatives: 49.0000\n","Epoch 31/64\n","Training on train_data\n","    452/Unknown - 726s 2s/step - loss: 0.0085 - accuracy: 0.9980 - false_positives: 105.0000 - false_negatives: 133.0000\n","Epoch 1: saving model to transfomer.h5\n","452/452 [==============================] - 732s 2s/step - loss: 0.0085 - accuracy: 0.9980 - false_positives: 105.0000 - false_negatives: 133.0000\n","Epoch 32/64\n","Training on val_data\n","     96/Unknown - 138s 1s/step - loss: 0.0098 - accuracy: 0.9981 - false_positives: 20.0000 - false_negatives: 25.0000\n","Epoch 1: saving model to transfomer.h5\n","96/96 [==============================] - 144s 1s/step - loss: 0.0098 - accuracy: 0.9981 - false_positives: 20.0000 - false_negatives: 25.0000\n","Epoch 33/64\n","Training on train_data\n","    452/Unknown - 647s 1s/step - loss: 0.0172 - accuracy: 0.9951 - false_positives: 236.0000 - false_negatives: 316.0000\n","Epoch 1: saving model to transfomer.h5\n","452/452 [==============================] - 652s 1s/step - loss: 0.0172 - accuracy: 0.9951 - false_positives: 236.0000 - false_negatives: 316.0000\n","Epoch 34/64\n","Training on val_data\n","     96/Unknown - 127s 1s/step - loss: 0.0162 - accuracy: 0.9959 - false_positives: 44.0000 - false_negatives: 61.0000\n","Epoch 1: saving model to transfomer.h5\n","96/96 [==============================] - 132s 1s/step - loss: 0.0162 - accuracy: 0.9959 - false_positives: 44.0000 - false_negatives: 61.0000\n","Epoch 35/64\n","Training on train_data\n","    452/Unknown - 631s 1s/step - loss: 0.0101 - accuracy: 0.9975 - false_positives: 126.0000 - false_negatives: 177.0000\n","Epoch 1: saving model to transfomer.h5\n","452/452 [==============================] - 637s 1s/step - loss: 0.0101 - accuracy: 0.9975 - false_positives: 126.0000 - false_negatives: 177.0000\n","Epoch 36/64\n","Training on val_data\n","     96/Unknown - 120s 1s/step - loss: 0.0115 - accuracy: 0.9969 - false_positives: 33.0000 - false_negatives: 45.0000\n","Epoch 1: saving model to transfomer.h5\n","96/96 [==============================] - 126s 1s/step - loss: 0.0115 - accuracy: 0.9969 - false_positives: 33.0000 - false_negatives: 45.0000\n","Epoch 37/64\n","Training on train_data\n","    452/Unknown - 851s 2s/step - loss: 0.0059 - accuracy: 0.9987 - false_positives: 65.0000 - false_negatives: 84.0000\n","Epoch 1: saving model to transfomer.h5\n","452/452 [==============================] - 856s 2s/step - loss: 0.0059 - accuracy: 0.9987 - false_positives: 65.0000 - false_negatives: 84.0000\n","Epoch 38/64\n","Training on val_data\n","     96/Unknown - 205s 2s/step - loss: 0.0135 - accuracy: 0.9961 - false_positives: 43.0000 - false_negatives: 56.0000\n","Epoch 1: saving model to transfomer.h5\n","96/96 [==============================] - 210s 2s/step - loss: 0.0135 - accuracy: 0.9961 - false_positives: 43.0000 - false_negatives: 56.0000\n","Epoch 39/64\n","Training on train_data\n","    452/Unknown - 829s 2s/step - loss: 0.0122 - accuracy: 0.9970 - false_positives: 148.0000 - false_negatives: 197.0000\n","Epoch 1: saving model to transfomer.h5\n","452/452 [==============================] - 835s 2s/step - loss: 0.0122 - accuracy: 0.9970 - false_positives: 148.0000 - false_negatives: 197.0000\n","Epoch 40/64\n","Training on val_data\n","     96/Unknown - 186s 2s/step - loss: 0.0193 - accuracy: 0.9944 - false_positives: 60.0000 - false_negatives: 76.0000\n","Epoch 1: saving model to transfomer.h5\n","96/96 [==============================] - 191s 2s/step - loss: 0.0193 - accuracy: 0.9944 - false_positives: 60.0000 - false_negatives: 76.0000\n","Epoch 41/64\n","Training on train_data\n","    452/Unknown - 774s 2s/step - loss: 0.0123 - accuracy: 0.9965 - false_positives: 168.0000 - false_negatives: 222.0000\n","Epoch 1: saving model to transfomer.h5\n","452/452 [==============================] - 779s 2s/step - loss: 0.0123 - accuracy: 0.9965 - false_positives: 168.0000 - false_negatives: 222.0000\n","Epoch 42/64\n","Training on val_data\n","     96/Unknown - 120s 1s/step - loss: 0.0120 - accuracy: 0.9965 - false_positives: 38.0000 - false_negatives: 44.0000\n","Epoch 1: saving model to transfomer.h5\n","96/96 [==============================] - 126s 1s/step - loss: 0.0120 - accuracy: 0.9965 - false_positives: 38.0000 - false_negatives: 44.0000\n","Epoch 43/64\n","Training on train_data\n","    452/Unknown - 680s 2s/step - loss: 0.0053 - accuracy: 0.9989 - false_positives: 54.0000 - false_negatives: 70.0000\n","Epoch 1: saving model to transfomer.h5\n","452/452 [==============================] - 685s 2s/step - loss: 0.0053 - accuracy: 0.9989 - false_positives: 54.0000 - false_negatives: 70.0000\n","Epoch 44/64\n","Training on val_data\n","     96/Unknown - 139s 1s/step - loss: 0.0110 - accuracy: 0.9975 - false_positives: 25.0000 - false_negatives: 39.0000\n","Epoch 1: saving model to transfomer.h5\n","96/96 [==============================] - 144s 1s/step - loss: 0.0110 - accuracy: 0.9975 - false_positives: 25.0000 - false_negatives: 39.0000\n","Epoch 45/64\n","Training on train_data\n","    452/Unknown - 606s 1s/step - loss: 0.0099 - accuracy: 0.9974 - false_positives: 129.0000 - false_negatives: 174.0000\n","Epoch 1: saving model to transfomer.h5\n","452/452 [==============================] - 612s 1s/step - loss: 0.0099 - accuracy: 0.9974 - false_positives: 129.0000 - false_negatives: 174.0000\n","Epoch 46/64\n","Training on val_data\n","     96/Unknown - 117s 1s/step - loss: 0.0097 - accuracy: 0.9977 - false_positives: 27.0000 - false_negatives: 29.0000\n","Epoch 1: saving model to transfomer.h5\n","96/96 [==============================] - 123s 1s/step - loss: 0.0097 - accuracy: 0.9977 - false_positives: 27.0000 - false_negatives: 29.0000\n","Epoch 47/64\n","Training on train_data\n","    452/Unknown - 555s 1s/step - loss: 0.0111 - accuracy: 0.9970 - false_positives: 153.0000 - false_negatives: 198.0000\n","Epoch 1: saving model to transfomer.h5\n","452/452 [==============================] - 561s 1s/step - loss: 0.0111 - accuracy: 0.9970 - false_positives: 153.0000 - false_negatives: 198.0000\n","Epoch 48/64\n","Training on val_data\n","     96/Unknown - 115s 1s/step - loss: 0.0116 - accuracy: 0.9971 - false_positives: 30.0000 - false_negatives: 41.0000\n","Epoch 1: saving model to transfomer.h5\n","96/96 [==============================] - 121s 1s/step - loss: 0.0116 - accuracy: 0.9971 - false_positives: 30.0000 - false_negatives: 41.0000\n","Epoch 49/64\n","Training on train_data\n","    452/Unknown - 574s 1s/step - loss: 0.0116 - accuracy: 0.9968 - false_positives: 160.0000 - false_negatives: 212.0000\n","Epoch 1: saving model to transfomer.h5\n","452/452 [==============================] - 580s 1s/step - loss: 0.0116 - accuracy: 0.9968 - false_positives: 160.0000 - false_negatives: 212.0000\n","Epoch 50/64\n","Training on val_data\n"]},{"name":"stdout","output_type":"stream","text":["     96/Unknown - 126s 1s/step - loss: 0.0090 - accuracy: 0.9973 - false_positives: 25.0000 - false_negatives: 35.0000\n","Epoch 1: saving model to transfomer.h5\n","96/96 [==============================] - 132s 1s/step - loss: 0.0090 - accuracy: 0.9973 - false_positives: 25.0000 - false_negatives: 35.0000\n","Epoch 51/64\n","Training on train_data\n","    452/Unknown - 626s 1s/step - loss: 0.0075 - accuracy: 0.9980 - false_positives: 100.0000 - false_negatives: 131.0000\n","Epoch 1: saving model to transfomer.h5\n","452/452 [==============================] - 631s 1s/step - loss: 0.0075 - accuracy: 0.9980 - false_positives: 100.0000 - false_negatives: 131.0000\n","Epoch 52/64\n","Training on val_data\n","     96/Unknown - 190s 2s/step - loss: 0.0093 - accuracy: 0.9975 - false_positives: 27.0000 - false_negatives: 35.0000\n","Epoch 1: saving model to transfomer.h5\n","96/96 [==============================] - 195s 2s/step - loss: 0.0093 - accuracy: 0.9975 - false_positives: 27.0000 - false_negatives: 35.0000\n","Epoch 53/64\n","Training on train_data\n","    452/Unknown - 726s 2s/step - loss: 0.0075 - accuracy: 0.9978 - false_positives: 116.0000 - false_negatives: 141.0000\n","Epoch 1: saving model to transfomer.h5\n","452/452 [==============================] - 732s 2s/step - loss: 0.0075 - accuracy: 0.9978 - false_positives: 116.0000 - false_negatives: 141.0000\n","Epoch 54/64\n","Training on val_data\n","     96/Unknown - 120s 1s/step - loss: 0.0090 - accuracy: 0.9973 - false_positives: 29.0000 - false_negatives: 36.0000\n","Epoch 1: saving model to transfomer.h5\n","96/96 [==============================] - 125s 1s/step - loss: 0.0090 - accuracy: 0.9973 - false_positives: 29.0000 - false_negatives: 36.0000\n","Epoch 55/64\n","Training on train_data\n","    452/Unknown - 557s 1s/step - loss: 0.0041 - accuracy: 0.9991 - false_positives: 44.0000 - false_negatives: 60.0000\n","Epoch 1: saving model to transfomer.h5\n","452/452 [==============================] - 562s 1s/step - loss: 0.0041 - accuracy: 0.9991 - false_positives: 44.0000 - false_negatives: 60.0000\n","Epoch 56/64\n","Training on val_data\n","     96/Unknown - 113s 1s/step - loss: 0.0180 - accuracy: 0.9953 - false_positives: 51.0000 - false_negatives: 64.0000\n","Epoch 1: saving model to transfomer.h5\n","96/96 [==============================] - 118s 1s/step - loss: 0.0180 - accuracy: 0.9953 - false_positives: 51.0000 - false_negatives: 64.0000\n","Epoch 57/64\n","Training on train_data\n","    452/Unknown - 768s 2s/step - loss: 0.0109 - accuracy: 0.9971 - false_positives: 151.0000 - false_negatives: 190.0000\n","Epoch 1: saving model to transfomer.h5\n","452/452 [==============================] - 773s 2s/step - loss: 0.0109 - accuracy: 0.9971 - false_positives: 151.0000 - false_negatives: 190.0000\n","Epoch 58/64\n","Training on val_data\n","     96/Unknown - 205s 2s/step - loss: 0.0098 - accuracy: 0.9973 - false_positives: 29.0000 - false_negatives: 35.0000\n","Epoch 1: saving model to transfomer.h5\n","96/96 [==============================] - 211s 2s/step - loss: 0.0098 - accuracy: 0.9973 - false_positives: 29.0000 - false_negatives: 35.0000\n","Epoch 59/64\n","Training on train_data\n","    452/Unknown - 909s 2s/step - loss: 0.0113 - accuracy: 0.9967 - false_positives: 162.0000 - false_negatives: 213.0000\n","Epoch 1: saving model to transfomer.h5\n","452/452 [==============================] - 915s 2s/step - loss: 0.0113 - accuracy: 0.9967 - false_positives: 162.0000 - false_negatives: 213.0000\n","Epoch 60/64\n","Training on val_data\n","     96/Unknown - 182s 2s/step - loss: 0.0101 - accuracy: 0.9968 - false_positives: 32.0000 - false_negatives: 45.0000\n","Epoch 1: saving model to transfomer.h5\n","96/96 [==============================] - 187s 2s/step - loss: 0.0101 - accuracy: 0.9968 - false_positives: 32.0000 - false_negatives: 45.0000\n","Epoch 61/64\n","Training on train_data\n","    452/Unknown - 777s 2s/step - loss: 0.0062 - accuracy: 0.9986 - false_positives: 72.0000 - false_negatives: 87.0000\n","Epoch 1: saving model to transfomer.h5\n","452/452 [==============================] - 783s 2s/step - loss: 0.0062 - accuracy: 0.9986 - false_positives: 72.0000 - false_negatives: 87.0000\n","Epoch 62/64\n","Training on val_data\n","     96/Unknown - 149s 2s/step - loss: 0.0074 - accuracy: 0.9982 - false_positives: 21.0000 - false_negatives: 24.0000\n","Epoch 1: saving model to transfomer.h5\n","96/96 [==============================] - 155s 2s/step - loss: 0.0074 - accuracy: 0.9982 - false_positives: 21.0000 - false_negatives: 24.0000\n","Epoch 63/64\n","Training on train_data\n","    452/Unknown - 766s 2s/step - loss: 0.0051 - accuracy: 0.9988 - false_positives: 60.0000 - false_negatives: 77.0000\n","Epoch 1: saving model to transfomer.h5\n","452/452 [==============================] - 771s 2s/step - loss: 0.0051 - accuracy: 0.9988 - false_positives: 60.0000 - false_negatives: 77.0000\n","Epoch 64/64\n","Training on val_data\n","     96/Unknown - 185s 2s/step - loss: 0.0088 - accuracy: 0.9977 - false_positives: 24.0000 - false_negatives: 31.0000\n","Epoch 1: saving model to transfomer.h5\n","96/96 [==============================] - 190s 2s/step - loss: 0.0088 - accuracy: 0.9977 - false_positives: 24.0000 - false_negatives: 31.0000\n"]}],"source":["import os\n","epochs = 64\n","\n","weights_path = '/home/u200810216/jupyter/kaggle_m3cv/'\n","initial_weights_file = 'vision_transfomer_test_kaggle_batch128_weights_val_3_train_3_roll_6.h5'\n","final_weights_file = f'vision_transfomer_test_kaggle_batch128_weights_val_3_train_3_roll_6_roll{epochs}.h5'\n","\n","# 加载预训练权重\n","model.load_weights(os.path.join(weights_path, initial_weights_file))\n","\n","# 训练循环\n","for epoch in range(epochs):\n","    print(f\"Epoch {epoch + 1}/{epochs}\")\n","\n","    if epoch % 2 == 0:\n","        print(\"Training on train_data\")\n","        data = train_data\n","    else:\n","        print(\"Training on val_data\")\n","        data = val_data\n","\n","    history = model.fit(data,\n","                        epochs=1,\n","                        verbose=1,\n","                        callbacks=[tensorboard_callback, checkpoint_callback])\n","\n","    # 保存模型参数\n","    model.save_weights(os.path.join(weights_path, \"{}_epoch_{}.h5\".format(final_weights_file.replace(\".h5\",\"\"), epoch+1)))\n","\n","# 保存最终模型参数\n","model.save_weights(os.path.join(weights_path, final_weights_file))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nmKrPJCKJXzj","outputId":"bdc10569-7917-46ea-be39-e53d34026bbd"},"outputs":[{"name":"stdout","output_type":"stream","text":["Defaulting to user installation because normal site-packages is not writeable\n","Collecting tensorflow-addons\n","  Downloading tensorflow_addons-0.19.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m858.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n","\u001b[?25hCollecting typeguard>=2.7\n","  Downloading typeguard-3.0.2-py3-none-any.whl (30 kB)\n","Requirement already satisfied: packaging in /opt/conda/lib/python3.7/site-packages (from tensorflow-addons) (21.3)\n","Requirement already satisfied: typing-extensions>=4.4.0 in /home/u200810216/.local/lib/python3.7/site-packages (from typeguard>=2.7->tensorflow-addons) (4.5.0)\n","Requirement already satisfied: importlib-metadata>=3.6 in /opt/conda/lib/python3.7/site-packages (from typeguard>=2.7->tensorflow-addons) (4.8.1)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging->tensorflow-addons) (3.0.4)\n","Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata>=3.6->typeguard>=2.7->tensorflow-addons) (3.6.0)\n","Installing collected packages: typeguard, tensorflow-addons\n","Successfully installed tensorflow-addons-0.19.0 typeguard-3.0.2\n"]}],"source":["!pip install tensorflow-addons\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wPkNf65XJXzj","outputId":"42d012fe-92ab-4f2b-ef47-f39d2543dc40"},"outputs":[{"ename":"TypeError","evalue":"object.__init__() takes exactly one argument (the instance to initialize)","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_24248/1536022752.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;31m# 加载保存的模型\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'vision_transfomer_last.h5'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_objects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'TFViTModel'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTFViTModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'AdamWeightDecay'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mCustomAdamW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'WarmUp'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mWarmUp\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m~/.local/lib/python3.7/site-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_24248/1536022752.py\u001b[0m in \u001b[0;36mfrom_config\u001b[0;34m(cls, config, custom_objects)\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m\"power\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m             \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"power\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_objects\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mWarmUp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mschedules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLearningRateSchedule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow_addons/optimizers/weight_decay_optimizers.py\u001b[0m in \u001b[0;36mfrom_config\u001b[0;34m(cls, config, custom_objects)\u001b[0m\n\u001b[1;32m    116\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"learning_rate\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m                 config[\"learning_rate\"] = tf.keras.optimizers.schedules.deserialize(\n\u001b[0;32m--> 118\u001b[0;31m                     \u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"learning_rate\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_objects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m                 )\n\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_24248/1536022752.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, initial_learning_rate, decay_schedule_fn, warmup_steps, **kwargs)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mWarmUp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mschedules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLearningRateSchedule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial_learning_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecay_schedule_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwarmup_steps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitial_learning_rate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minitial_learning_rate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecay_schedule_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecay_schedule_fn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: object.__init__() takes exactly one argument (the instance to initialize)"]}],"source":["import tensorflow as tf\n","from tensorflow.keras.optimizers import Adam\n","import tensorflow_addons as tfa\n","class CustomAdamW(tfa.optimizers.AdamW):\n","    @classmethod\n","    def from_config(cls, config, custom_objects=None):\n","        if \"power\" in config:\n","            config.pop(\"power\")\n","        return super().from_config(config, custom_objects)\n","\n","class WarmUp(tf.keras.optimizers.schedules.LearningRateSchedule):\n","    def __init__(self, initial_learning_rate, decay_schedule_fn, warmup_steps, **kwargs):\n","        super().__init__(**kwargs)\n","        self.initial_learning_rate = initial_learning_rate\n","        self.decay_schedule_fn = decay_schedule_fn\n","        self.warmup_steps = warmup_steps\n","\n","    def __call__(self, step):\n","        global_step_float = tf.cast(step, tf.float32)\n","        warmup_steps_float = tf.cast(self.warmup_steps, tf.float32)\n","        warmup_percent_done = global_step_float / warmup_steps_float\n","        warmup_learning_rate = self.initial_learning_rate * warmup_percent_done\n","\n","        return tf.cond(\n","            global_step_float < warmup_steps_float,\n","            lambda: warmup_learning_rate,\n","            lambda: self.decay_schedule_fn(step - self.warmup_steps),\n","        )\n","\n","    def get_config(self):\n","        return {\n","            \"initial_learning_rate\": self.initial_learning_rate,\n","            \"decay_schedule_fn\": self.decay_schedule_fn,\n","            \"warmup_steps\": self.warmup_steps,\n","        }\n","\n","# 加载保存的模型\n","model = tf.keras.models.load_model('vision_transfomer_last.h5', custom_objects={'TFViTModel': TFViTModel, 'AdamWeightDecay': CustomAdamW, 'WarmUp': WarmUp})\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1jZa3zUZJXzj","outputId":"1f621939-694d-4587-ede1-90d79224993a"},"outputs":[{"ename":"NameError","evalue":"name 'AdamWeightDecay' is not defined","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_67/4213670892.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# 加载保存的模型\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mloaded_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'vision_transfomer_last.h5'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_objects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'TFViTModel'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTFViTModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'AdamWeightDecay'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAdamWeightDecay\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# # (可选) 如果您想更改学习速率或优化器，您可以在这里设置\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'AdamWeightDecay' is not defined"]}],"source":["import tensorflow as tf\n","from tensorflow.keras.optimizers import Adam\n","\n","# 加载保存的模型\n","loaded_model = tf.keras.models.load_model('vision_transfomer_last.h5', custom_objects={'TFViTModel': TFViTModel, 'AdamWeightDecay': AdamWeightDecay})\n","\n","# # (可选) 如果您想更改学习速率或优化器，您可以在这里设置\n","# optimizer = Adam(learning_rate=0.0001)\n","# loaded_model.compile(optimizer=optimizer, loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n","#                      metrics=['accuracy'])\n","\n","# 继续训练\n","new_epochs = 20  # 设置新的训练轮数\n","history = loaded_model.fit(train_data,\n","                    epochs=new_epochs,\n","                    verbose=1,\n","                    validation_data=val_data,\n","                    callbacks=[tensorboard_callback, checkpoint_callback])\n","\n","# 保存模型\n","model.save('logs/transfomer/vision_transfomer_test.h5')  # 保存为 .h5 文件格式\n","\n","\n","\n","# 保存模型\n","model.save('vision_transfomer_last.h5')  # 保存为 .h5 文件格式\n","# 保存继续训练后的模型\n","loaded_model.save('vision_transfomer_trained_further.h5')  # 保存为 .h5 文件格式\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ouGvn21EJXzj","outputId":"d7663dc2-21f0-4f39-99e8-bcc3b2e28b8a"},"outputs":[{"name":"stdout","output_type":"stream","text":["AdamWeightDecay\n"]}],"source":["print(type(optimizer).__name__)\n"]},{"cell_type":"markdown","metadata":{"id":"9s9rTCasWHmS"},"source":["# Model Training"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"D5ijogP5EdYO"},"outputs":[],"source":["import tensorflow as tf\n","from tensorflow.keras.callbacks import ModelCheckpoint\n","\n","batch_size = 256\n","epochs = 10000\n","\n","# 记录日志\n","log_dir = \"logs/fit\"\n","tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n","\n","\n","# 定义优化器\n","class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n","    def __init__(self, d_model, warmup_steps=4000):\n","        super(CustomSchedule, self).__init__()\n","\n","        self.d_model = tf.constant(d_model, dtype=tf.float32)\n","        self.warmup_steps = warmup_steps\n","\n","    def __call__(self, step):\n","        step = tf.cast(step, tf.float32)\n","        arg1 = tf.math.rsqrt(step)\n","        arg2 = step * (self.warmup_steps ** -1.5)\n","\n","        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n","\n","    def get_config(self):  # 添加 get_config 方法\n","        return {\n","            'd_model': self.d_model.numpy(),\n","            'warmup_steps': self.warmup_steps\n","        }\n","\n","\n","learning_rate = CustomSchedule(d_model)\n","optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)\n","\n","\n","model.compile(loss=keras.losses.categorical_crossentropy,\n","              optimizer=optimizer,\n","              metrics=['accuracy',\n","                       'FalsePositives',\n","                       'FalseNegatives'])\n","\n","\n","# 定义一个回调函数来保存模型和历史记录\n","checkpoint_callback = ModelCheckpoint(\n","    filepath='transfomer.h5',\n","    save_weights_only=False,\n","    save_best_only=True,\n","    monitor='val_loss',\n","    verbose=1,\n",")\n","\n","\n","history = model.fit(x_train, y_train,\n","          batch_size=batch_size,\n","          epochs=epochs,\n","          shuffle=True,\n","          verbose=1,\n","          validation_split=0.2, callbacks=[tensorboard_callback, checkpoint_callback])\n","\n","# 保存模型\n","model.save('transfomer_test.h5')  # 保存为 .h5 文件格式"]},{"cell_type":"markdown","metadata":{"id":"4b148e6HWWOB"},"source":["# Model Evaluation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"S21V6RM0ElWu"},"outputs":[],"source":["os.chdir('/home/u200810216/jupyter/')\n","\n","score_train = model.evaluate(x_train, y_train, verbose=0)\n","score_test = model.evaluate(x_test, y_test, verbose=0)\n","print(\"mean train accuracy is:   \", (score_train[1]))\n","print(\"mean test accuracy is:   \", (score_test[1]))\n","\n","print(\"FAR is:   \", (score_train[2]+score_test[2])/(np.size(y_train,0)+np.size(y_test,0)))\n","print(\"FRR is:   \", (score_train[3]+score_test[3])/(np.size(y_train,0)+np.size(y_test,0)))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"N0DGtOmOTiNi","scrolled":true},"outputs":[],"source":["! tensorboard --logdir logs/fit --port 6007"]},{"cell_type":"markdown","metadata":{"id":"yCsCDRvmWl6j"},"source":["## Loss and Accuracy curves"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4XM3f7pJE7yQ"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","# plt.style.use(['science','high-vis','grid'])\n","# \"Accuracy\"\n","plt.plot(history.history['accuracy'])\n","plt.plot(history.history['val_accuracy'])\n","plt.title('model accuracy')\n","plt.ylabel('accuracy')\n","plt.xlabel('epoch')\n","plt.legend(['train', 'validation'], loc='upper left')\n","plt.grid(True)\n","plt.savefig('vit_accuracy.png', dpi=600)\n","plt.show()\n","\n","# \"Loss\"\n","plt.plot(history.history['loss'])\n","plt.plot(history.history['val_loss'])\n","plt.title('model loss')\n","plt.ylabel('loss')\n","plt.xlabel('epoch')\n","plt.legend(['train', 'validation'], loc='upper left')\n","plt.grid(True)\n","plt.savefig('vit_accuracy.png.png', dpi=600)\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GaH1t5MpJXzl"},"outputs":[],"source":["import re\n","\n","# 输入的字符串\n","log_string = \"\"\"\n","Epoch 1/100\n","WARNING:tensorflow:Gradients do not exist for variables ['tf_vi_t_model_2/vit/pooler/dense/kernel:0', 'tf_vi_t_model_2/vit/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n","WARNING:tensorflow:Gradients do not exist for variables ['tf_vi_t_model_2/vit/pooler/dense/kernel:0', 'tf_vi_t_model_2/vit/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n","7820/7821 [============================>.] - ETA: 0s - loss: 0.5048 - accuracy: 0.9068 - false_positives: 1999.0000 - false_negatives: 23218.0000\n","Epoch 1: val_loss improved from inf to 0.07635, saving model to transfomer.h5\n","7821/7821 [==============================] - 714s 90ms/step - loss: 0.5048 - accuracy: 0.9068 - false_positives: 1999.0000 - false_negatives: 23218.0000 - val_loss: 0.0764 - val_accuracy: 0.9802 - val_false_positives: 456.0000 - val_false_negatives: 797.0000\n","Epoch 2/100\n","7820/7821 [============================>.] - ETA: 0s - loss: 0.0489 - accuracy: 0.9874 - false_positives: 1222.0000 - false_negatives: 1896.0000\n","Epoch 2: val_loss improved from 0.07635 to 0.04349, saving model to transfomer.h5\n","7821/7821 [==============================] - 700s 90ms/step - loss: 0.0489 - accuracy: 0.9874 - false_positives: 1222.0000 - false_negatives: 1896.0000 - val_loss: 0.0435 - val_accuracy: 0.9879 - val_false_positives: 318.0000 - val_false_negatives: 437.0000\n","Epoch 3/100\n","7820/7821 [============================>.] - ETA: 0s - loss: 0.0279 - accuracy: 0.9924 - false_positives: 800.0000 - false_negatives: 1116.0000\n","Epoch 3: val_loss improved from 0.04349 to 0.02177, saving model to transfomer.h5\n","7821/7821 [==============================] - 700s 89ms/step - loss: 0.0279 - accuracy: 0.9924 - false_positives: 800.0000 - false_negatives: 1116.0000 - val_loss: 0.0218 - val_accuracy: 0.9940 - val_false_positives: 148.0000 - val_false_negatives: 221.0000\n","Epoch 4/100\n","7820/7821 [============================>.] - ETA: 0s - loss: 0.0209 - accuracy: 0.9945 - false_positives: 583.0000 - false_negatives: 793.0000\n","Epoch 4: val_loss did not improve from 0.02177\n","7821/7821 [==============================] - 698s 89ms/step - loss: 0.0209 - accuracy: 0.9945 - false_positives: 583.0000 - false_negatives: 793.0000 - val_loss: 0.0221 - val_accuracy: 0.9947 - val_false_positives: 145.0000 - val_false_negatives: 180.0000\n","Epoch 5/100\n","7820/7821 [============================>.] - ETA: 0s - loss: 0.0167 - accuracy: 0.9956 - false_positives: 487.0000 - false_negatives: 613.0000\n","Epoch 5: val_loss did not improve from 0.02177\n","7821/7821 [==============================] - 699s 89ms/step - loss: 0.0167 - accuracy: 0.9956 - false_positives: 487.0000 - false_negatives: 613.0000 - val_loss: 0.0508 - val_accuracy: 0.9876 - val_false_positives: 354.0000 - val_false_negatives: 410.0000\n","Epoch 6/100\n","7820/7821 [============================>.] - ETA: 0s - loss: 0.0133 - accuracy: 0.9966 - false_positives: 388.0000 - false_negatives: 487.0000\n","Epoch 6: val_loss improved from 0.02177 to 0.01470, saving model to transfomer.h5\n","7821/7821 [==============================] - 700s 90ms/step - loss: 0.0133 - accuracy: 0.9966 - false_positives: 388.0000 - false_negatives: 487.0000 - val_loss: 0.0147 - val_accuracy: 0.9957 - val_false_positives: 115.0000 - val_false_negatives: 148.0000\n","Epoch 7/100\n","7820/7821 [============================>.] - ETA: 0s - loss: 0.0106 - accuracy: 0.9972 - false_positives: 304.0000 - false_negatives: 372.0000\n","Epoch 7: val_loss did not improve from 0.01470\n","7821/7821 [==============================] - 698s 89ms/step - loss: 0.0106 - accuracy: 0.9972 - false_positives: 304.0000 - false_negatives: 372.0000 - val_loss: 0.0195 - val_accuracy: 0.9942 - val_false_positives: 158.0000 - val_false_negatives: 195.0000\n","Epoch 8/100\n","7820/7821 [============================>.] - ETA: 0s - loss: 0.0107 - accuracy: 0.9972 - false_positives: 317.0000 - false_negatives: 389.0000\n","Epoch 8: val_loss did not improve from 0.01470\n","7821/7821 [==============================] - 699s 89ms/step - loss: 0.0107 - accuracy: 0.9972 - false_positives: 317.0000 - false_negatives: 389.0000 - val_loss: 0.0196 - val_accuracy: 0.9954 - val_false_positives: 114.0000 - val_false_negatives: 160.0000\n","Epoch 9/100\n","7820/7821 [============================>.] - ETA: 0s - loss: 0.0084 - accuracy: 0.9979 - false_positives: 247.0000 - false_negatives: 280.0000\n","Epoch 9: val_loss improved from 0.01470 to 0.00727, saving model to transfomer.h5\n","7821/7821 [==============================] - 700s 90ms/step - loss: 0.0084 - accuracy: 0.9979 - false_positives: 247.0000 - false_negatives: 280.0000 - val_loss: 0.0073 - val_accuracy: 0.9981 - val_false_positives: 51.0000 - val_false_negatives: 62.0000\n","Epoch 10/100\n","7820/7821 [============================>.] - ETA: 0s - loss: 0.0083 - accuracy: 0.9978 - false_positives: 246.0000 - false_negatives: 299.0000\n","Epoch 10: val_loss did not improve from 0.00727\n","7821/7821 [==============================] - 699s 89ms/step - loss: 0.0083 - accuracy: 0.9978 - false_positives: 246.0000 - false_negatives: 299.0000 - val_loss: 0.0103 - val_accuracy: 0.9969 - val_false_positives: 86.0000 - val_false_negatives: 101.0000\n","Epoch 11/100\n","7820/7821 [============================>.] - ETA: 0s - loss: 0.0078 - accuracy: 0.9981 - false_positives: 216.0000 - false_negatives: 256.0000\n","Epoch 11: val_loss improved from 0.00727 to 0.00611, saving model to transfomer.h5\n","7821/7821 [==============================] - 700s 89ms/step - loss: 0.0078 - accuracy: 0.9981 - false_positives: 216.0000 - false_negatives: 256.0000 - val_loss: 0.0061 - val_accuracy: 0.9985 - val_false_positives: 42.0000 - val_false_negatives: 49.0000\n","Epoch 12/100\n","7820/7821 [============================>.] - ETA: 0s - loss: 0.0073 - accuracy: 0.9982 - false_positives: 213.0000 - false_negatives: 240.0000\n","Epoch 12: val_loss did not improve from 0.00611\n","7821/7821 [==============================] - 700s 90ms/step - loss: 0.0073 - accuracy: 0.9982 - false_positives: 213.0000 - false_negatives: 240.0000 - val_loss: 0.0113 - val_accuracy: 0.9967 - val_false_positives: 84.0000 - val_false_negatives: 115.0000\n","Epoch 13/100\n","7820/7821 [============================>.] - ETA: 0s - loss: 0.0054 - accuracy: 0.9986 - false_positives: 162.0000 - false_negatives: 187.0000\n","Epoch 13: val_loss did not improve from 0.00611\n","7821/7821 [==============================] - 700s 90ms/step - loss: 0.0054 - accuracy: 0.9986 - false_positives: 162.0000 - false_negatives: 187.0000 - val_loss: 0.0072 - val_accuracy: 0.9979 - val_false_positives: 62.0000 - val_false_negatives: 68.0000\n","Epoch 14/100\n","7820/7821 [============================>.] - ETA: 0s - loss: 0.0060 - accuracy: 0.9983 - false_positives: 192.0000 - false_negatives: 221.0000\n","Epoch 14: val_loss did not improve from 0.00611\n","7821/7821 [==============================] - 701s 90ms/step - loss: 0.0060 - accuracy: 0.9983 - false_positives: 192.0000 - false_negatives: 221.0000 - val_loss: 0.0084 - val_accuracy: 0.9980 - val_false_positives: 56.0000 - val_false_negatives: 68.0000\n","Epoch 15/100\n","7820/7821 [============================>.] - ETA: 0s - loss: 0.0051 - accuracy: 0.9988 - false_positives: 138.0000 - false_negatives: 159.0000\n","Epoch 15: val_loss did not improve from 0.00611\n","7821/7821 [==============================] - 700s 89ms/step - loss: 0.0051 - accuracy: 0.9988 - false_positives: 138.0000 - false_negatives: 159.0000 - val_loss: 0.0493 - val_accuracy: 0.9864 - val_false_positives: 365.0000 - val_false_negatives: 461.0000\n","Epoch 16/100\n","7820/7821 [============================>.] - ETA: 0s - loss: 0.0053 - accuracy: 0.9987 - false_positives: 156.0000 - false_negatives: 177.0000\n","Epoch 16: val_loss did not improve from 0.00611\n","7821/7821 [==============================] - 700s 90ms/step - loss: 0.0053 - accuracy: 0.9987 - false_positives: 156.0000 - false_negatives: 177.0000 - val_loss: 0.0129 - val_accuracy: 0.9971 - val_false_positives: 82.0000 - val_false_negatives: 98.0000\n","Epoch 17/100\n","7820/7821 [============================>.] - ETA: 0s - loss: 0.0046 - accuracy: 0.9987 - false_positives: 143.0000 - false_negatives: 172.0000\n","Epoch 17: val_loss did not improve from 0.00611\n","7821/7821 [==============================] - 700s 89ms/step - loss: 0.0046 - accuracy: 0.9987 - false_positives: 143.0000 - false_negatives: 172.0000 - val_loss: 0.0067 - val_accuracy: 0.9984 - val_false_positives: 46.0000 - val_false_negatives: 57.0000\n","Epoch 18/100\n","7820/7821 [============================>.] - ETA: 0s - loss: 0.0046 - accuracy: 0.9990 - false_positives: 122.0000 - false_negatives: 141.0000\n","Epoch 18: val_loss did not improve from 0.00611\n","7821/7821 [==============================] - 701s 90ms/step - loss: 0.0046 - accuracy: 0.9990 - false_positives: 122.0000 - false_negatives: 141.0000 - val_loss: 0.0076 - val_accuracy: 0.9982 - val_false_positives: 49.0000 - val_false_negatives: 63.0000\n","Epoch 19/100\n","7820/7821 [============================>.] - ETA: 0s - loss: 0.0041 - accuracy: 0.9989 - false_positives: 122.0000 - false_negatives: 148.0000\n","Epoch 19: val_loss did not improve from 0.00611\n","7821/7821 [==============================] - 700s 90ms/step - loss: 0.0041 - accuracy: 0.9989 - false_positives: 122.0000 - false_negatives: 148.0000 - val_loss: 0.0090 - val_accuracy: 0.9976 - val_false_positives: 69.0000 - val_false_negatives: 77.0000\n","Epoch 20/100\n","7820/7821 [============================>.] - ETA: 0s - loss: 0.0050 - accuracy: 0.9988 - false_positives: 138.0000 - false_negatives: 159.0000\n","Epoch 20: val_loss improved from 0.00611 to 0.00391, saving model to transfomer.h5\n","7821/7821 [==============================] - 702s 90ms/step - loss: 0.0050 - accuracy: 0.9988 - false_positives: 138.0000 - false_negatives: 159.0000 - val_loss: 0.0039 - val_accuracy: 0.9988 - val_false_positives: 30.0000 - val_false_negatives: 42.0000\n","Epoch 21/100\n","7820/7821 [============================>.] - ETA: 0s - loss: 0.0036 - accuracy: 0.9992 - false_positives: 99.0000 - false_negatives: 107.0000\n","Epoch 21: val_loss did not improve from 0.00391\n","7821/7821 [==============================] - 700s 89ms/step - loss: 0.0036 - accuracy: 0.9992 - false_positives: 99.0000 - false_negatives: 107.0000 - val_loss: 0.0395 - val_accuracy: 0.9936 - val_false_positives: 192.0000 - val_false_negatives: 212.0000\n","Epoch 22/100\n","7820/7821 [============================>.] - ETA: 0s - loss: 0.0036 - accuracy: 0.9991 - false_positives: 104.0000 - false_negatives: 112.0000\n","Epoch 22: val_loss did not improve from 0.00391\n","7821/7821 [==============================] - 701s 90ms/step - loss: 0.0036 - accuracy: 0.9991 - false_positives: 104.0000 - false_negatives: 112.0000 - val_loss: 0.0060 - val_accuracy: 0.9986 - val_false_positives: 38.0000 - val_false_negatives: 49.0000\n","Epoch 23/100\n","7820/7821 [============================>.] - ETA: 0s - loss: 0.0026 - accuracy: 0.9992 - false_positives: 91.0000 - false_negatives: 102.0000\n","Epoch 23: val_loss did not improve from 0.00391\n","7821/7821 [==============================] - 703s 90ms/step - loss: 0.0026 - accuracy: 0.9992 - false_positives: 91.0000 - false_negatives: 102.0000 - val_loss: 0.0055 - val_accuracy: 0.9986 - val_false_positives: 39.0000 - val_false_negatives: 46.0000\n","Epoch 24/100\n","7820/7821 [============================>.] - ETA: 0s - loss: 0.0033 - accuracy: 0.9992 - false_positives: 99.0000 - false_negatives: 107.0000\n","Epoch 24: val_loss did not improve from 0.00391\n","7821/7821 [==============================] - 703s 90ms/step - loss: 0.0033 - accuracy: 0.9992 - false_positives: 99.0000 - false_negatives: 107.0000 - val_loss: 0.0108 - val_accuracy: 0.9973 - val_false_positives: 73.0000 - val_false_negatives: 86.0000\n","Epoch 25/100\n","7820/7821 [============================>.] - ETA: 0s - loss: 0.0034 - accuracy: 0.9992 - false_positives: 99.0000 - false_negatives: 113.0000\n","Epoch 25: val_loss did not improve from 0.00391\n","7821/7821 [==============================] - 701s 90ms/step - loss: 0.0034 - accuracy: 0.9992 - false_positives: 99.0000 - false_negatives: 113.0000 - val_loss: 0.0054 - val_accuracy: 0.9988 - val_false_positives: 30.0000 - val_false_negatives: 40.0000\n","Epoch 26/100\n","7820/7821 [============================>.] - ETA: 0s - loss: 0.0027 - accuracy: 0.9993 - false_positives: 86.0000 - false_negatives: 95.0000\n","Epoch 26: val_loss did not improve from 0.00391\n","7821/7821 [==============================] - 701s 90ms/step - loss: 0.0027 - accuracy: 0.9993 - false_positives: 86.0000 - false_negatives: 95.0000 - val_loss: 0.0095 - val_accuracy: 0.9976 - val_false_positives: 63.0000 - val_false_negatives: 78.0000\n","Epoch 27/100\n","7820/7821 [============================>.] - ETA: 0s - loss: 0.0028 - accuracy: 0.9992 - false_positives: 86.0000 - false_negatives: 94.0000\n","Epoch 27: val_loss did not improve from 0.00391\n","7821/7821 [==============================] - 700s 89ms/step - loss: 0.0028 - accuracy: 0.9992 - false_positives: 86.0000 - false_negatives: 94.0000 - val_loss: 0.0056 - val_accuracy: 0.9987 - val_false_positives: 38.0000 - val_false_negatives: 43.0000\n","Epoch 28/100\n","7820/7821 [============================>.] - ETA: 0s - loss: 0.0030 - accuracy: 0.9993 - false_positives: 89.0000 - false_negatives: 98.0000\n","Epoch 28: val_loss did not improve from 0.00391\n","7821/7821 [==============================] - 701s 90ms/step - loss: 0.0030 - accuracy: 0.9993 - false_positives: 89.0000 - false_negatives: 98.0000 - val_loss: 0.0090 - val_accuracy: 0.9979 - val_false_positives: 65.0000 - val_false_negatives: 74.0000\n","Epoch 29/100\n","7820/7821 [============================>.] - ETA: 0s - loss: 0.0025 - accuracy: 0.9994 - false_positives: 70.0000 - false_negatives: 80.0000\n","Epoch 29: val_loss did not improve from 0.00391\n","7821/7821 [==============================] - 702s 90ms/step - loss: 0.0025 - accuracy: 0.9994 - false_positives: 70.0000 - false_negatives: 80.0000 - val_loss: 0.0055 - val_accuracy: 0.9985 - val_false_positives: 43.0000 - val_false_negatives: 49.0000\n","Epoch 30/100\n","7820/7821 [============================>.] - ETA: 0s - loss: 0.0023 - accuracy: 0.9994 - false_positives: 66.0000 - false_negatives: 75.0000\n","Epoch 30: val_loss did not improve from 0.00391\n","7821/7821 [==============================] - 701s 90ms/step - loss: 0.0023 - accuracy: 0.9994 - false_positives: 66.0000 - false_negatives: 75.0000 - val_loss: 0.0065 - val_accuracy: 0.9985 - val_false_positives: 40.0000 - val_false_negatives: 57.0000\n","\"\"\"\n","\n","train_loss = []\n","train_accuracy = []\n","val_loss = []\n","val_accuracy = []\n","\n","# 逐行处理字符串，提取出需要的数据\n","for line in log_string.split(\"\\n\"):\n","    if not line.strip():\n","        continue\n","    train_loss_match = re.search(r'loss: \\[(.*)\\]', line)\n","    if train_loss_match:\n","        train_loss.extend([float(x) for x in train_loss_match.group(1).split(',')])\n","    train_accuracy_match = re.search(r'accuracy: \\[(.*)\\]', line)\n","    if train_accuracy_match:\n","        train_accuracy.extend([float(x) for x in train_accuracy_match.group(1).split(',')])\n","    val_loss_match = re.search(r'val_loss: \\[(.*)\\]', line)\n","    if val_loss_match:\n","        val_loss.extend([float(x) for x in val_loss_match.group(1).split(',')])\n","    val_accuracy_match = re.search(r'val_accuracy: \\[(.*)\\]', line)\n","    if val_accuracy_match:\n","        val_accuracy.extend([float(x) for x in val_accuracy_match.group(1).split(',')])\n","\n","# 输出结果\n","print(\"Train Loss:\", train_loss)\n","print(\"Train Accuracy:\", train_accuracy)\n","print(\"Validation Loss:\", val_loss)\n","print(\"Validation Accuracy:\", val_accuracy)\n"]},{"cell_type":"markdown","metadata":{"id":"6JjAKSEHJXzl"},"source":["## 百度Evaluation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MS0z5qsRJXzl","outputId":"5bf0f45a-b672-4765-a57e-339cf7891a3e"},"outputs":[{"name":"stderr","output_type":"stream","text":["  0%|          | 34/52342 [00:07<3:20:07,  4.36it/s]\n"]},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_23571/3041610645.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0meeg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meeg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m...\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnewaxis\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meeg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/.local/lib/python3.7/site-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/.local/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    559\u001b[0m             \u001b[0mlayout_map_lib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_map_subclass_model_variable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_layout_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    560\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 561\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    562\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mdoc_controls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdoc_in_current_and_subclasses\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/.local/lib/python3.7/site-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/.local/lib/python3.7/site-packages/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1130\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compute_dtype_object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1131\u001b[0m                 ):\n\u001b[0;32m-> 1132\u001b[0;31m                     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_activity_regularizer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/.local/lib/python3.7/site-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0mbound_signature\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"_keras_call_info_injected\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/.local/lib/python3.7/site-packages/keras/engine/functional.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs, training, mask)\u001b[0m\n\u001b[1;32m    509\u001b[0m             \u001b[0ma\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mof\u001b[0m \u001b[0mtensors\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mthere\u001b[0m \u001b[0mare\u001b[0m \u001b[0mmore\u001b[0m \u001b[0mthan\u001b[0m \u001b[0mone\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    510\u001b[0m         \"\"\"\n\u001b[0;32m--> 511\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_internal_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcompute_output_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/.local/lib/python3.7/site-packages/keras/engine/functional.py\u001b[0m in \u001b[0;36m_run_internal_graph\u001b[0;34m(self, inputs, training, mask)\u001b[0m\n\u001b[1;32m    666\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    667\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_arguments\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 668\u001b[0;31m                 \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    669\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    670\u001b[0m                 \u001b[0;31m# Update tensor_dict.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/.local/lib/python3.7/site-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/.local/lib/python3.7/site-packages/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1056\u001b[0m         ):\n\u001b[1;32m   1057\u001b[0m             inputs = tf.nest.map_structure(\n\u001b[0;32m-> 1058\u001b[0;31m                 \u001b[0m_convert_numpy_or_python_types\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1059\u001b[0m             )\n\u001b[1;32m   1060\u001b[0m             \u001b[0minput_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["import os\n","import numpy as np\n","import pandas as pd\n","import scipy.io as scio\n","import tensorflow as tf\n","\n","# Load the model\n","model = model # Load your TensorFlow model here\n","\n","# Read the test data\n","test_image = pd.read_csv('/home/u200810216/jupyter/kaggle_m3cv/Testing_Info.csv')\n","test_image_path_list = test_image['Row'].values\n","\n","eeg_list = list()\n","\n","labeled_img_list = []\n","for img in test_image_path_list:\n","    eeg_list.append('/home/u200810216/jupyter/kaggle_m3cv/test/'+img+'.mat')\n","    labeled_img_list.append(img)\n","\n","def load_eeg(eeg_path):\n","    # Load data\n","    data = scio.loadmat(eeg_path)\n","    return data['epoch_data']\n","\n","pre_list = []\n","for i in tqdm(range(len(eeg_list))):\n","    data = load_eeg(eeg_path=eeg_list[i])\n","    dy_x_data = np.array(data).astype('float32')\n","    dy_x_data = dy_x_data[np.newaxis, :, :]\n","    eeg = tf.convert_to_tensor(dy_x_data)\n","    eeg = eeg[..., np.newaxis]\n","\n","    out = model(eeg)\n","    out = np.argmax(out.numpy())\n","\n","    pre_list.append(int(out)+1)\n","\n","img_test = pd.DataFrame(labeled_img_list)\n","img_pre = pd.DataFrame(labeled_img_list)\n","\n","img_test = img_test.rename(columns = {0:\"EpochID\"})\n","img_pre['SubjectID'] = pre_list\n","pre_info = img_pre['SubjectID'].values\n","test_info = test_image['SubjectID'].values\n","\n","result_cnn = list()\n","\n","for i, j in zip(test_info, pre_info):\n","    if i == 'None':\n","        result_cnn.append(j)\n","    elif int(i[4:])==j :\n","        print(i[4:])\n","        result_cnn.append(int(1))\n","    else:\n","        result_cnn.append(int(0))\n","\n","img_test['Prediction'] = result_cnn\n","img_test.to_csv('result_vit_kaggle.csv', index=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oZ-oqV8IJXzm","outputId":"5a9d87cc-979e-4e02-920a-bc5a9ab934d1"},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|██████████| 103/103 [09:19<00:00,  5.43s/it]\n"]},{"name":"stdout","output_type":"stream","text":["29\n","59\n","51\n","76\n","94\n","88\n","29\n","29\n","88\n","26\n","65\n","31\n","88\n","24\n","59\n","55\n","52\n","72\n","91\n","59\n","88\n","94\n","71\n","27\n","52\n","29\n","71\n","39\n","39\n","23\n","29\n","31\n","95\n","29\n","79\n","29\n","94\n","76\n","59\n","44\n","72\n","45\n","59\n","76\n","65\n","29\n","72\n","94\n","35\n","88\n","57\n","72\n","84\n","88\n","44\n","29\n","76\n","29\n","71\n","77\n","84\n","42\n","52\n","91\n","88\n","53\n","59\n","31\n","94\n","76\n","94\n","23\n","23\n","76\n","31\n","88\n","29\n","94\n","25\n","43\n","23\n","36\n","29\n","28\n","29\n","77\n","29\n","42\n","71\n","29\n","88\n","29\n","32\n","25\n","29\n","21\n","32\n","71\n","74\n","32\n","23\n","59\n","42\n","88\n","52\n","29\n","31\n","91\n","29\n","59\n","43\n","58\n","77\n","41\n","21\n","57\n","24\n","91\n","72\n","65\n","29\n","29\n","32\n","31\n","29\n","32\n","38\n","23\n","46\n","35\n","52\n","65\n","52\n","42\n","29\n","78\n","30\n","45\n","29\n","57\n","59\n","24\n","83\n","29\n","71\n","38\n","91\n","83\n","27\n","59\n","94\n","42\n","29\n","52\n","75\n","59\n","29\n","94\n","38\n","24\n","95\n","84\n","83\n","83\n","93\n","29\n","88\n","57\n","54\n","46\n","23\n","77\n","29\n","32\n","57\n","52\n","59\n","88\n","30\n","25\n","83\n","29\n","45\n","71\n","29\n","88\n","24\n","59\n","31\n","24\n","29\n","59\n","29\n","88\n","93\n","94\n","31\n","52\n","59\n","46\n","29\n","40\n","29\n","29\n","27\n","77\n","76\n","23\n","77\n","23\n","23\n","35\n","44\n","76\n","27\n","24\n","57\n","70\n","29\n","60\n","94\n","53\n","59\n","42\n","88\n","94\n","30\n","59\n","29\n","46\n","78\n","29\n","39\n","60\n","84\n","53\n","76\n","32\n","52\n","29\n","52\n","94\n","25\n","27\n","42\n","87\n","72\n","57\n","60\n","71\n","88\n","42\n","29\n","23\n","40\n","43\n","25\n","29\n","30\n","76\n","40\n","58\n","43\n","71\n","77\n","66\n","47\n","32\n","29\n","59\n","78\n","45\n","32\n","57\n","76\n","31\n","77\n","29\n","76\n","42\n","42\n","75\n","52\n","23\n","52\n","71\n","71\n","29\n","23\n","76\n","52\n","56\n","25\n","26\n","91\n","76\n","71\n","83\n","32\n","76\n","94\n","77\n","88\n","70\n","30\n","59\n","77\n","29\n","88\n","32\n","23\n","94\n","52\n","94\n","44\n","28\n","27\n","94\n","94\n","44\n","62\n","83\n","40\n","88\n","29\n","77\n","30\n","29\n","76\n","52\n","29\n","52\n","75\n","55\n","31\n","56\n","58\n","23\n","83\n","74\n","76\n","91\n"]}],"source":["import os\n","import numpy as np\n","import pandas as pd\n","import scipy.io as scio\n","import tensorflow as tf\n","from tqdm import tqdm\n","\n","# Load the model\n","model = model # Load your TensorFlow model here\n","\n","# Read the test data\n","test_image = pd.read_csv('/home/u200810216/jupyter/kaggle_m3cv/Testing_Info.csv')\n","test_image_path_list = test_image['Row'].values\n","\n","eeg_list = ['/home/u200810216/jupyter/kaggle_m3cv/test/'+img+'.mat' for img in test_image_path_list]\n","\n","def load_eeg(eeg_path):\n","    data = scio.loadmat(eeg_path)\n","    return data['epoch_data']\n","\n","# Set batch size\n","batch_size = 512\n","\n","# Preallocate lists for results\n","pre_list = []\n","\n","# Iterate over dataset with steps of batch_size\n","for i in tqdm(range(0, len(eeg_list), batch_size)):\n","    batch_eeg_list = eeg_list[i:i+batch_size]\n","    batch_data = np.array([load_eeg(eeg_path) for eeg_path in batch_eeg_list]).astype('float32')\n","    batch_data = batch_data[..., np.newaxis]\n","\n","    # Convert to tensor and feed to the model\n","    batch_data_tensor = tf.convert_to_tensor(batch_data)\n","    batch_out = model(batch_data_tensor)\n","    batch_pre_list = np.argmax(batch_out.numpy(), axis=-1) + 1\n","\n","    pre_list.extend(batch_pre_list)\n","\n","# Post-processing and saving results\n","img_test = pd.DataFrame(test_image_path_list, columns=[\"EpochID\"])\n","img_pre = pd.DataFrame(test_image_path_list, columns=[\"EpochID\"])\n","img_pre['SubjectID'] = pre_list\n","pre_info = img_pre['SubjectID'].values\n","test_info = test_image['SubjectID'].values\n","\n","result_cnn = list()\n","\n","for i, j in zip(test_info, pre_info):\n","    if i == 'None':\n","        result_cnn.append(j)\n","    elif int(i[4:])==j :\n","        print(i[4:])\n","        result_cnn.append(int(1))\n","    else:\n","        result_cnn.append(int(0))\n","\n","img_test['Prediction'] = result_cnn\n","img_test.to_csv('result_vit_kaggle_roll64.csv', index=False)\n"]},{"cell_type":"markdown","metadata":{"id":"91T1RsJMW2gx"},"source":["# Environment Debugging Code"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9uORQIaETiNj","outputId":"9f183219-059c-46a7-da73-b8ad8db04638"},"outputs":[{"name":"stderr","output_type":"stream","text":["2023-05-06 00:29:55.574764: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n","To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","2023-05-06 00:29:59.771098: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n","2023-05-06 00:30:22.878649: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n","2023-05-06 00:30:22.879038: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n","2023-05-06 00:30:22.879056: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"]},{"name":"stdout","output_type":"stream","text":["2.11.0\n"]}],"source":["import tensorflow as tf\n","print(tf.__version__)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XDIHxu5EJXzm","outputId":"74379f86-1151-4e26-da5f-b7bd5d1e6c97"},"outputs":[{"name":"stdout","output_type":"stream","text":["Defaulting to user installation because normal site-packages is not writeable\n","Requirement already satisfied: tqdm in /opt/conda/lib/python3.7/site-packages (4.62.3)\n","Hit:1 https://mirrors.tuna.tsinghua.edu.cn/ubuntu focal InRelease\n","Get:2 https://mirrors.tuna.tsinghua.edu.cn/ubuntu focal-updates InRelease [114 kB]\n","Get:3 https://mirrors.tuna.tsinghua.edu.cn/ubuntu focal-backports InRelease [108 kB]\n","Get:4 https://mirrors.tuna.tsinghua.edu.cn/ubuntu focal-security InRelease [114 kB]\n","Fetched 336 kB in 1s (580 kB/s)                              \n","Reading package lists... Done\n"]},{"name":"stderr","output_type":"stream","text":["\r\n","  0%|          | 0/5 [00:00<?, ?it/s]"]},{"name":"stdout","output_type":"stream","text":["Reading package lists...\n","Building dependency tree...\n","Reading state information...\n"]},{"name":"stderr","output_type":"stream","text":["\r\n"," 20%|██        | 1/5 [00:01<00:04,  1.21s/it]"]},{"name":"stdout","output_type":"stream","text":["texlive-latex-recommended is already the newest version (2019.20200218-1).\n","The following packages were automatically installed and are no longer required:\n","  dh-python gir1.2-harfbuzz-0.0 ibverbs-providers icu-devtools libblkid-dev\n","  libboost-atomic-dev libboost-atomic1.65-dev libboost-atomic1.65.1\n","  libboost-chrono-dev libboost-chrono1.65-dev libboost-chrono1.65.1\n","  libboost-container-dev libboost-container1.65-dev libboost-container1.65.1\n","  libboost-context-dev libboost-context1.65-dev libboost-context1.65.1\n","  libboost-coroutine-dev libboost-coroutine1.65-dev libboost-coroutine1.65.1\n","  libboost-date-time-dev libboost-date-time1.65-dev libboost-date-time1.65.1\n","  libboost-dev libboost-exception-dev libboost-exception1.65-dev\n","  libboost-fiber-dev libboost-fiber1.65-dev libboost-fiber1.65.1\n","  libboost-filesystem-dev libboost-filesystem1.65-dev\n","  libboost-filesystem1.65.1 libboost-graph-dev libboost-graph-parallel-dev\n","  libboost-graph-parallel1.65-dev libboost-graph-parallel1.65.1\n","  libboost-graph1.65-dev libboost-graph1.65.1 libboost-iostreams-dev\n","  libboost-iostreams1.65-dev libboost-iostreams1.65.1 libboost-locale-dev\n","  libboost-locale1.65-dev libboost-locale1.65.1 libboost-log-dev\n","  libboost-log1.65-dev libboost-log1.65.1 libboost-math-dev\n","  libboost-math1.65-dev libboost-math1.65.1 libboost-mpi-dev\n","  libboost-mpi1.65-dev libboost-mpi1.65.1 libboost-numpy-dev\n","  libboost-numpy1.65-dev libboost-numpy1.65.1 libboost-program-options-dev\n","  libboost-program-options1.65-dev libboost-program-options1.65.1\n","  libboost-python-dev libboost-python1.65-dev libboost-python1.65.1\n","  libboost-random-dev libboost-random1.65-dev libboost-random1.65.1\n","  libboost-regex-dev libboost-regex1.65-dev libboost-regex1.65.1\n","  libboost-serialization-dev libboost-serialization1.65-dev\n","  libboost-serialization1.65.1 libboost-signals-dev libboost-signals1.65-dev\n","  libboost-signals1.65.1 libboost-stacktrace-dev libboost-stacktrace1.65-dev\n","  libboost-stacktrace1.65.1 libboost-system-dev libboost-system1.65-dev\n","  libboost-system1.65.1 libboost-test-dev libboost-test1.65-dev\n","  libboost-test1.65.1 libboost-thread-dev libboost-thread1.65-dev\n","  libboost-thread1.65.1 libboost-timer-dev libboost-timer1.65-dev\n","  libboost-timer1.65.1 libboost-tools-dev libboost-type-erasure-dev\n","  libboost-type-erasure1.65-dev libboost-type-erasure1.65.1 libboost-wave-dev\n","  libboost-wave1.65-dev libboost-wave1.65.1 libboost1.65-dev\n","  libboost1.65-tools-dev libexpat1-dev libfabric1 libffi-dev libglib2.0-bin\n","  libglib2.0-dev libglib2.0-dev-bin libgraphite2-dev libharfbuzz-dev\n","  libharfbuzz-gobject0 libhwloc-dev libhwloc-plugins libhwloc5 libibverbs-dev\n","  libibverbs1 libicu-dev libicu-le-hb-dev libicu-le-hb0 libiculx60\n","  libmount-dev libnl-3-200 libnl-route-3-200 libnuma-dev libnuma1\n","  libopenmpi-dev libopenmpi2 libpcre16-3 libpcre2-16-0 libpcre2-32-0\n","  libpcre2-dev libpcre2-posix2 libpcre3-dev libpcre32-3 libpcrecpp0v5\n","  libperl5.26 libpsm-infinipath1 libpython-dev libpython-stdlib libpython2.7\n","  libpython2.7-dev libpython2.7-minimal libpython2.7-stdlib libpython3-dev\n","  libpython3.6-dev libpython3.8 libpython3.8-dev librdmacm1 libselinux1-dev\n","  libsepol1-dev mpi-default-bin mpi-default-dev ocl-icd-libopencl1 openmpi-bin\n","  openmpi-common perl-modules-5.26 python python-dev python-minimal python2.7\n","  python2.7-dev python2.7-minimal python3-dev python3-distutils\n","  python3-lib2to3 python3.6 python3.6-dev python3.6-minimal python3.8-dev\n","  uuid-dev zlib1g-dev\n","Use 'sudo apt autoremove' to remove them.\n","0 upgraded, 0 newly installed, 0 to remove and 532 not upgraded.\n","Reading package lists...\n","Building dependency tree...\n","Reading state information...\n"]},{"name":"stderr","output_type":"stream","text":["\r\n"," 40%|████      | 2/5 [00:02<00:03,  1.17s/it]"]},{"name":"stdout","output_type":"stream","text":["dvipng is already the newest version (1.15-1.1).\n","The following packages were automatically installed and are no longer required:\n","  dh-python gir1.2-harfbuzz-0.0 ibverbs-providers icu-devtools libblkid-dev\n","  libboost-atomic-dev libboost-atomic1.65-dev libboost-atomic1.65.1\n","  libboost-chrono-dev libboost-chrono1.65-dev libboost-chrono1.65.1\n","  libboost-container-dev libboost-container1.65-dev libboost-container1.65.1\n","  libboost-context-dev libboost-context1.65-dev libboost-context1.65.1\n","  libboost-coroutine-dev libboost-coroutine1.65-dev libboost-coroutine1.65.1\n","  libboost-date-time-dev libboost-date-time1.65-dev libboost-date-time1.65.1\n","  libboost-dev libboost-exception-dev libboost-exception1.65-dev\n","  libboost-fiber-dev libboost-fiber1.65-dev libboost-fiber1.65.1\n","  libboost-filesystem-dev libboost-filesystem1.65-dev\n","  libboost-filesystem1.65.1 libboost-graph-dev libboost-graph-parallel-dev\n","  libboost-graph-parallel1.65-dev libboost-graph-parallel1.65.1\n","  libboost-graph1.65-dev libboost-graph1.65.1 libboost-iostreams-dev\n","  libboost-iostreams1.65-dev libboost-iostreams1.65.1 libboost-locale-dev\n","  libboost-locale1.65-dev libboost-locale1.65.1 libboost-log-dev\n","  libboost-log1.65-dev libboost-log1.65.1 libboost-math-dev\n","  libboost-math1.65-dev libboost-math1.65.1 libboost-mpi-dev\n","  libboost-mpi1.65-dev libboost-mpi1.65.1 libboost-numpy-dev\n","  libboost-numpy1.65-dev libboost-numpy1.65.1 libboost-program-options-dev\n","  libboost-program-options1.65-dev libboost-program-options1.65.1\n","  libboost-python-dev libboost-python1.65-dev libboost-python1.65.1\n","  libboost-random-dev libboost-random1.65-dev libboost-random1.65.1\n","  libboost-regex-dev libboost-regex1.65-dev libboost-regex1.65.1\n","  libboost-serialization-dev libboost-serialization1.65-dev\n","  libboost-serialization1.65.1 libboost-signals-dev libboost-signals1.65-dev\n","  libboost-signals1.65.1 libboost-stacktrace-dev libboost-stacktrace1.65-dev\n","  libboost-stacktrace1.65.1 libboost-system-dev libboost-system1.65-dev\n","  libboost-system1.65.1 libboost-test-dev libboost-test1.65-dev\n","  libboost-test1.65.1 libboost-thread-dev libboost-thread1.65-dev\n","  libboost-thread1.65.1 libboost-timer-dev libboost-timer1.65-dev\n","  libboost-timer1.65.1 libboost-tools-dev libboost-type-erasure-dev\n","  libboost-type-erasure1.65-dev libboost-type-erasure1.65.1 libboost-wave-dev\n","  libboost-wave1.65-dev libboost-wave1.65.1 libboost1.65-dev\n","  libboost1.65-tools-dev libexpat1-dev libfabric1 libffi-dev libglib2.0-bin\n","  libglib2.0-dev libglib2.0-dev-bin libgraphite2-dev libharfbuzz-dev\n","  libharfbuzz-gobject0 libhwloc-dev libhwloc-plugins libhwloc5 libibverbs-dev\n","  libibverbs1 libicu-dev libicu-le-hb-dev libicu-le-hb0 libiculx60\n","  libmount-dev libnl-3-200 libnl-route-3-200 libnuma-dev libnuma1\n","  libopenmpi-dev libopenmpi2 libpcre16-3 libpcre2-16-0 libpcre2-32-0\n","  libpcre2-dev libpcre2-posix2 libpcre3-dev libpcre32-3 libpcrecpp0v5\n","  libperl5.26 libpsm-infinipath1 libpython-dev libpython-stdlib libpython2.7\n","  libpython2.7-dev libpython2.7-minimal libpython2.7-stdlib libpython3-dev\n","  libpython3.6-dev libpython3.8 libpython3.8-dev librdmacm1 libselinux1-dev\n","  libsepol1-dev mpi-default-bin mpi-default-dev ocl-icd-libopencl1 openmpi-bin\n","  openmpi-common perl-modules-5.26 python python-dev python-minimal python2.7\n","  python2.7-dev python2.7-minimal python3-dev python3-distutils\n","  python3-lib2to3 python3.6 python3.6-dev python3.6-minimal python3.8-dev\n","  uuid-dev zlib1g-dev\n","Use 'sudo apt autoremove' to remove them.\n","0 upgraded, 0 newly installed, 0 to remove and 532 not upgraded.\n","Reading package lists...\n","Building dependency tree...\n","Reading state information...\n"]},{"name":"stderr","output_type":"stream","text":["\r\n"," 60%|██████    | 3/5 [00:03<00:02,  1.18s/it]"]},{"name":"stdout","output_type":"stream","text":["texlive-latex-extra is already the newest version (2019.202000218-1).\n","The following packages were automatically installed and are no longer required:\n","  dh-python gir1.2-harfbuzz-0.0 ibverbs-providers icu-devtools libblkid-dev\n","  libboost-atomic-dev libboost-atomic1.65-dev libboost-atomic1.65.1\n","  libboost-chrono-dev libboost-chrono1.65-dev libboost-chrono1.65.1\n","  libboost-container-dev libboost-container1.65-dev libboost-container1.65.1\n","  libboost-context-dev libboost-context1.65-dev libboost-context1.65.1\n","  libboost-coroutine-dev libboost-coroutine1.65-dev libboost-coroutine1.65.1\n","  libboost-date-time-dev libboost-date-time1.65-dev libboost-date-time1.65.1\n","  libboost-dev libboost-exception-dev libboost-exception1.65-dev\n","  libboost-fiber-dev libboost-fiber1.65-dev libboost-fiber1.65.1\n","  libboost-filesystem-dev libboost-filesystem1.65-dev\n","  libboost-filesystem1.65.1 libboost-graph-dev libboost-graph-parallel-dev\n","  libboost-graph-parallel1.65-dev libboost-graph-parallel1.65.1\n","  libboost-graph1.65-dev libboost-graph1.65.1 libboost-iostreams-dev\n","  libboost-iostreams1.65-dev libboost-iostreams1.65.1 libboost-locale-dev\n","  libboost-locale1.65-dev libboost-locale1.65.1 libboost-log-dev\n","  libboost-log1.65-dev libboost-log1.65.1 libboost-math-dev\n","  libboost-math1.65-dev libboost-math1.65.1 libboost-mpi-dev\n","  libboost-mpi1.65-dev libboost-mpi1.65.1 libboost-numpy-dev\n","  libboost-numpy1.65-dev libboost-numpy1.65.1 libboost-program-options-dev\n","  libboost-program-options1.65-dev libboost-program-options1.65.1\n","  libboost-python-dev libboost-python1.65-dev libboost-python1.65.1\n","  libboost-random-dev libboost-random1.65-dev libboost-random1.65.1\n","  libboost-regex-dev libboost-regex1.65-dev libboost-regex1.65.1\n","  libboost-serialization-dev libboost-serialization1.65-dev\n","  libboost-serialization1.65.1 libboost-signals-dev libboost-signals1.65-dev\n","  libboost-signals1.65.1 libboost-stacktrace-dev libboost-stacktrace1.65-dev\n","  libboost-stacktrace1.65.1 libboost-system-dev libboost-system1.65-dev\n","  libboost-system1.65.1 libboost-test-dev libboost-test1.65-dev\n","  libboost-test1.65.1 libboost-thread-dev libboost-thread1.65-dev\n","  libboost-thread1.65.1 libboost-timer-dev libboost-timer1.65-dev\n","  libboost-timer1.65.1 libboost-tools-dev libboost-type-erasure-dev\n","  libboost-type-erasure1.65-dev libboost-type-erasure1.65.1 libboost-wave-dev\n","  libboost-wave1.65-dev libboost-wave1.65.1 libboost1.65-dev\n","  libboost1.65-tools-dev libexpat1-dev libfabric1 libffi-dev libglib2.0-bin\n","  libglib2.0-dev libglib2.0-dev-bin libgraphite2-dev libharfbuzz-dev\n","  libharfbuzz-gobject0 libhwloc-dev libhwloc-plugins libhwloc5 libibverbs-dev\n","  libibverbs1 libicu-dev libicu-le-hb-dev libicu-le-hb0 libiculx60\n","  libmount-dev libnl-3-200 libnl-route-3-200 libnuma-dev libnuma1\n","  libopenmpi-dev libopenmpi2 libpcre16-3 libpcre2-16-0 libpcre2-32-0\n","  libpcre2-dev libpcre2-posix2 libpcre3-dev libpcre32-3 libpcrecpp0v5\n","  libperl5.26 libpsm-infinipath1 libpython-dev libpython-stdlib libpython2.7\n","  libpython2.7-dev libpython2.7-minimal libpython2.7-stdlib libpython3-dev\n","  libpython3.6-dev libpython3.8 libpython3.8-dev librdmacm1 libselinux1-dev\n","  libsepol1-dev mpi-default-bin mpi-default-dev ocl-icd-libopencl1 openmpi-bin\n","  openmpi-common perl-modules-5.26 python python-dev python-minimal python2.7\n","  python2.7-dev python2.7-minimal python3-dev python3-distutils\n","  python3-lib2to3 python3.6 python3.6-dev python3.6-minimal python3.8-dev\n","  uuid-dev zlib1g-dev\n","Use 'sudo apt autoremove' to remove them.\n","0 upgraded, 0 newly installed, 0 to remove and 532 not upgraded.\n","Reading package lists...\n","Building dependency tree...\n","Reading state information...\n"]},{"name":"stderr","output_type":"stream","text":["\r\n"," 80%|████████  | 4/5 [00:04<00:01,  1.18s/it]"]},{"name":"stdout","output_type":"stream","text":["texlive-fonts-recommended is already the newest version (2019.20200218-1).\n","The following packages were automatically installed and are no longer required:\n","  dh-python gir1.2-harfbuzz-0.0 ibverbs-providers icu-devtools libblkid-dev\n","  libboost-atomic-dev libboost-atomic1.65-dev libboost-atomic1.65.1\n","  libboost-chrono-dev libboost-chrono1.65-dev libboost-chrono1.65.1\n","  libboost-container-dev libboost-container1.65-dev libboost-container1.65.1\n","  libboost-context-dev libboost-context1.65-dev libboost-context1.65.1\n","  libboost-coroutine-dev libboost-coroutine1.65-dev libboost-coroutine1.65.1\n","  libboost-date-time-dev libboost-date-time1.65-dev libboost-date-time1.65.1\n","  libboost-dev libboost-exception-dev libboost-exception1.65-dev\n","  libboost-fiber-dev libboost-fiber1.65-dev libboost-fiber1.65.1\n","  libboost-filesystem-dev libboost-filesystem1.65-dev\n","  libboost-filesystem1.65.1 libboost-graph-dev libboost-graph-parallel-dev\n","  libboost-graph-parallel1.65-dev libboost-graph-parallel1.65.1\n","  libboost-graph1.65-dev libboost-graph1.65.1 libboost-iostreams-dev\n","  libboost-iostreams1.65-dev libboost-iostreams1.65.1 libboost-locale-dev\n","  libboost-locale1.65-dev libboost-locale1.65.1 libboost-log-dev\n","  libboost-log1.65-dev libboost-log1.65.1 libboost-math-dev\n","  libboost-math1.65-dev libboost-math1.65.1 libboost-mpi-dev\n","  libboost-mpi1.65-dev libboost-mpi1.65.1 libboost-numpy-dev\n","  libboost-numpy1.65-dev libboost-numpy1.65.1 libboost-program-options-dev\n","  libboost-program-options1.65-dev libboost-program-options1.65.1\n","  libboost-python-dev libboost-python1.65-dev libboost-python1.65.1\n","  libboost-random-dev libboost-random1.65-dev libboost-random1.65.1\n","  libboost-regex-dev libboost-regex1.65-dev libboost-regex1.65.1\n","  libboost-serialization-dev libboost-serialization1.65-dev\n","  libboost-serialization1.65.1 libboost-signals-dev libboost-signals1.65-dev\n","  libboost-signals1.65.1 libboost-stacktrace-dev libboost-stacktrace1.65-dev\n","  libboost-stacktrace1.65.1 libboost-system-dev libboost-system1.65-dev\n","  libboost-system1.65.1 libboost-test-dev libboost-test1.65-dev\n","  libboost-test1.65.1 libboost-thread-dev libboost-thread1.65-dev\n","  libboost-thread1.65.1 libboost-timer-dev libboost-timer1.65-dev\n","  libboost-timer1.65.1 libboost-tools-dev libboost-type-erasure-dev\n","  libboost-type-erasure1.65-dev libboost-type-erasure1.65.1 libboost-wave-dev\n","  libboost-wave1.65-dev libboost-wave1.65.1 libboost1.65-dev\n","  libboost1.65-tools-dev libexpat1-dev libfabric1 libffi-dev libglib2.0-bin\n","  libglib2.0-dev libglib2.0-dev-bin libgraphite2-dev libharfbuzz-dev\n","  libharfbuzz-gobject0 libhwloc-dev libhwloc-plugins libhwloc5 libibverbs-dev\n","  libibverbs1 libicu-dev libicu-le-hb-dev libicu-le-hb0 libiculx60\n","  libmount-dev libnl-3-200 libnl-route-3-200 libnuma-dev libnuma1\n","  libopenmpi-dev libopenmpi2 libpcre16-3 libpcre2-16-0 libpcre2-32-0\n","  libpcre2-dev libpcre2-posix2 libpcre3-dev libpcre32-3 libpcrecpp0v5\n","  libperl5.26 libpsm-infinipath1 libpython-dev libpython-stdlib libpython2.7\n","  libpython2.7-dev libpython2.7-minimal libpython2.7-stdlib libpython3-dev\n","  libpython3.6-dev libpython3.8 libpython3.8-dev librdmacm1 libselinux1-dev\n","  libsepol1-dev mpi-default-bin mpi-default-dev ocl-icd-libopencl1 openmpi-bin\n","  openmpi-common perl-modules-5.26 python python-dev python-minimal python2.7\n","  python2.7-dev python2.7-minimal python3-dev python3-distutils\n","  python3-lib2to3 python3.6 python3.6-dev python3.6-minimal python3.8-dev\n","  uuid-dev zlib1g-dev\n","Use 'sudo apt autoremove' to remove them.\n","0 upgraded, 0 newly installed, 0 to remove and 532 not upgraded.\n","Reading package lists...\n","Building dependency tree...\n","Reading state information...\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 5/5 [00:05<00:00,  1.18s/it]\n"]},{"name":"stdout","output_type":"stream","text":["cm-super is already the newest version (0.3.4-15).\n","The following packages were automatically installed and are no longer required:\n","  dh-python gir1.2-harfbuzz-0.0 ibverbs-providers icu-devtools libblkid-dev\n","  libboost-atomic-dev libboost-atomic1.65-dev libboost-atomic1.65.1\n","  libboost-chrono-dev libboost-chrono1.65-dev libboost-chrono1.65.1\n","  libboost-container-dev libboost-container1.65-dev libboost-container1.65.1\n","  libboost-context-dev libboost-context1.65-dev libboost-context1.65.1\n","  libboost-coroutine-dev libboost-coroutine1.65-dev libboost-coroutine1.65.1\n","  libboost-date-time-dev libboost-date-time1.65-dev libboost-date-time1.65.1\n","  libboost-dev libboost-exception-dev libboost-exception1.65-dev\n","  libboost-fiber-dev libboost-fiber1.65-dev libboost-fiber1.65.1\n","  libboost-filesystem-dev libboost-filesystem1.65-dev\n","  libboost-filesystem1.65.1 libboost-graph-dev libboost-graph-parallel-dev\n","  libboost-graph-parallel1.65-dev libboost-graph-parallel1.65.1\n","  libboost-graph1.65-dev libboost-graph1.65.1 libboost-iostreams-dev\n","  libboost-iostreams1.65-dev libboost-iostreams1.65.1 libboost-locale-dev\n","  libboost-locale1.65-dev libboost-locale1.65.1 libboost-log-dev\n","  libboost-log1.65-dev libboost-log1.65.1 libboost-math-dev\n","  libboost-math1.65-dev libboost-math1.65.1 libboost-mpi-dev\n","  libboost-mpi1.65-dev libboost-mpi1.65.1 libboost-numpy-dev\n","  libboost-numpy1.65-dev libboost-numpy1.65.1 libboost-program-options-dev\n","  libboost-program-options1.65-dev libboost-program-options1.65.1\n","  libboost-python-dev libboost-python1.65-dev libboost-python1.65.1\n","  libboost-random-dev libboost-random1.65-dev libboost-random1.65.1\n","  libboost-regex-dev libboost-regex1.65-dev libboost-regex1.65.1\n","  libboost-serialization-dev libboost-serialization1.65-dev\n","  libboost-serialization1.65.1 libboost-signals-dev libboost-signals1.65-dev\n","  libboost-signals1.65.1 libboost-stacktrace-dev libboost-stacktrace1.65-dev\n","  libboost-stacktrace1.65.1 libboost-system-dev libboost-system1.65-dev\n","  libboost-system1.65.1 libboost-test-dev libboost-test1.65-dev\n","  libboost-test1.65.1 libboost-thread-dev libboost-thread1.65-dev\n","  libboost-thread1.65.1 libboost-timer-dev libboost-timer1.65-dev\n","  libboost-timer1.65.1 libboost-tools-dev libboost-type-erasure-dev\n","  libboost-type-erasure1.65-dev libboost-type-erasure1.65.1 libboost-wave-dev\n","  libboost-wave1.65-dev libboost-wave1.65.1 libboost1.65-dev\n","  libboost1.65-tools-dev libexpat1-dev libfabric1 libffi-dev libglib2.0-bin\n","  libglib2.0-dev libglib2.0-dev-bin libgraphite2-dev libharfbuzz-dev\n","  libharfbuzz-gobject0 libhwloc-dev libhwloc-plugins libhwloc5 libibverbs-dev\n","  libibverbs1 libicu-dev libicu-le-hb-dev libicu-le-hb0 libiculx60\n","  libmount-dev libnl-3-200 libnl-route-3-200 libnuma-dev libnuma1\n","  libopenmpi-dev libopenmpi2 libpcre16-3 libpcre2-16-0 libpcre2-32-0\n","  libpcre2-dev libpcre2-posix2 libpcre3-dev libpcre32-3 libpcrecpp0v5\n","  libperl5.26 libpsm-infinipath1 libpython-dev libpython-stdlib libpython2.7\n","  libpython2.7-dev libpython2.7-minimal libpython2.7-stdlib libpython3-dev\n","  libpython3.6-dev libpython3.8 libpython3.8-dev librdmacm1 libselinux1-dev\n","  libsepol1-dev mpi-default-bin mpi-default-dev ocl-icd-libopencl1 openmpi-bin\n","  openmpi-common perl-modules-5.26 python python-dev python-minimal python2.7\n","  python2.7-dev python2.7-minimal python3-dev python3-distutils\n","  python3-lib2to3 python3.6 python3.6-dev python3.6-minimal python3.8-dev\n","  uuid-dev zlib1g-dev\n","Use 'sudo apt autoremove' to remove them.\n","0 upgraded, 0 newly installed, 0 to remove and 532 not upgraded.\n"]},{"name":"stderr","output_type":"stream","text":["\r\n","  0%|          | 0/21 [00:00<?, ?it/s]"]},{"name":"stdout","output_type":"stream","text":["Defaulting to user installation because normal site-packages is not writeable\n","Requirement already satisfied: Pillow in /home/u200810216/.local/lib/python3.7/site-packages (9.4.0)\n"]},{"name":"stderr","output_type":"stream","text":["\r\n","  5%|▍         | 1/21 [00:03<01:06,  3.30s/it]"]},{"name":"stdout","output_type":"stream","text":["Defaulting to user installation because normal site-packages is not writeable\n","Requirement already satisfied: umap in /home/u200810216/.local/lib/python3.7/site-packages (0.1.1)\n"]},{"name":"stderr","output_type":"stream","text":["\r\n"," 10%|▉         | 2/21 [00:06<01:02,  3.31s/it]"]},{"name":"stdout","output_type":"stream","text":["Defaulting to user installation because normal site-packages is not writeable\n","Requirement already satisfied: xgboost in /home/u200810216/.local/lib/python3.7/site-packages (1.6.2)\n","Requirement already satisfied: numpy in /home/u200810216/.local/lib/python3.7/site-packages (from xgboost) (1.21.6)\n","Requirement already satisfied: scipy in /home/u200810216/.local/lib/python3.7/site-packages (from xgboost) (1.7.3)\n"]},{"name":"stderr","output_type":"stream","text":["\r\n"," 14%|█▍        | 3/21 [00:09<00:59,  3.32s/it]"]},{"name":"stdout","output_type":"stream","text":["Defaulting to user installation because normal site-packages is not writeable\n","Requirement already satisfied: seaborn in /home/u200810216/.local/lib/python3.7/site-packages (0.11.2)\n","Requirement already satisfied: numpy>=1.15 in /home/u200810216/.local/lib/python3.7/site-packages (from seaborn) (1.21.6)\n","Requirement already satisfied: scipy>=1.0 in /home/u200810216/.local/lib/python3.7/site-packages (from seaborn) (1.7.3)\n","Requirement already satisfied: pandas>=0.23 in /home/u200810216/.local/lib/python3.7/site-packages (from seaborn) (1.3.5)\n","Requirement already satisfied: matplotlib>=2.2 in /home/u200810216/.local/lib/python3.7/site-packages (from seaborn) (3.0.3)\n","Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.7/site-packages (from matplotlib>=2.2->seaborn) (0.10.0)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib>=2.2->seaborn) (1.1.0)\n","Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib>=2.2->seaborn) (2.4.0)\n","Requirement already satisfied: python-dateutil>=2.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib>=2.2->seaborn) (2.8.2)\n","Requirement already satisfied: pytz>=2017.3 in /home/u200810216/.local/lib/python3.7/site-packages (from pandas>=0.23->seaborn) (2022.7.1)\n","Requirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from cycler>=0.10->matplotlib>=2.2->seaborn) (1.15.0)\n","Requirement already satisfied: setuptools in /opt/conda/lib/python3.7/site-packages (from kiwisolver>=1.0.1->matplotlib>=2.2->seaborn) (58.0.4)\n"]},{"name":"stderr","output_type":"stream","text":["\r\n"," 19%|█▉        | 4/21 [00:13<00:56,  3.32s/it]"]},{"name":"stdout","output_type":"stream","text":["Defaulting to user installation because normal site-packages is not writeable\n","Requirement already satisfied: SciencePlots in /home/u200810216/.local/lib/python3.7/site-packages (2.0.1)\n","Requirement already satisfied: matplotlib in /home/u200810216/.local/lib/python3.7/site-packages (from SciencePlots) (3.0.3)\n","Requirement already satisfied: numpy>=1.10.0 in /home/u200810216/.local/lib/python3.7/site-packages (from matplotlib->SciencePlots) (1.21.6)\n","Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.7/site-packages (from matplotlib->SciencePlots) (0.10.0)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib->SciencePlots) (1.1.0)\n","Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib->SciencePlots) (2.4.0)\n","Requirement already satisfied: python-dateutil>=2.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib->SciencePlots) (2.8.2)\n","Requirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from cycler>=0.10->matplotlib->SciencePlots) (1.15.0)\n","Requirement already satisfied: setuptools in /opt/conda/lib/python3.7/site-packages (from kiwisolver>=1.0.1->matplotlib->SciencePlots) (58.0.4)\n"]},{"name":"stderr","output_type":"stream","text":["\r\n"," 24%|██▍       | 5/21 [00:16<00:53,  3.32s/it]"]},{"name":"stdout","output_type":"stream","text":["Defaulting to user installation because normal site-packages is not writeable\n","Requirement already satisfied: pytorch-tabnet in /home/u200810216/.local/lib/python3.7/site-packages (4.0)\n","Requirement already satisfied: numpy<2.0,>=1.17 in /home/u200810216/.local/lib/python3.7/site-packages (from pytorch-tabnet) (1.21.6)\n","Requirement already satisfied: scikit_learn>0.21 in /home/u200810216/.local/lib/python3.7/site-packages (from pytorch-tabnet) (1.0.2)\n","Requirement already satisfied: scipy>1.4 in /home/u200810216/.local/lib/python3.7/site-packages (from pytorch-tabnet) (1.7.3)\n","Requirement already satisfied: torch<2.0,>=1.2 in /home/u200810216/.local/lib/python3.7/site-packages (from pytorch-tabnet) (1.13.1)\n","Requirement already satisfied: tqdm<5.0,>=4.36 in /opt/conda/lib/python3.7/site-packages (from pytorch-tabnet) (4.62.3)\n","Requirement already satisfied: joblib>=0.11 in /home/u200810216/.local/lib/python3.7/site-packages (from scikit_learn>0.21->pytorch-tabnet) (1.2.0)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /home/u200810216/.local/lib/python3.7/site-packages (from scikit_learn>0.21->pytorch-tabnet) (3.1.0)\n","Requirement already satisfied: typing-extensions in /home/u200810216/.local/lib/python3.7/site-packages (from torch<2.0,>=1.2->pytorch-tabnet) (4.5.0)\n","Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /home/u200810216/.local/lib/python3.7/site-packages (from torch<2.0,>=1.2->pytorch-tabnet) (11.7.99)\n","Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /home/u200810216/.local/lib/python3.7/site-packages (from torch<2.0,>=1.2->pytorch-tabnet) (8.5.0.96)\n","Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /home/u200810216/.local/lib/python3.7/site-packages (from torch<2.0,>=1.2->pytorch-tabnet) (11.10.3.66)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /home/u200810216/.local/lib/python3.7/site-packages (from torch<2.0,>=1.2->pytorch-tabnet) (11.7.99)\n","Requirement already satisfied: setuptools in /opt/conda/lib/python3.7/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch<2.0,>=1.2->pytorch-tabnet) (58.0.4)\n","Requirement already satisfied: wheel in /opt/conda/lib/python3.7/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch<2.0,>=1.2->pytorch-tabnet) (0.37.0)\n"]},{"name":"stderr","output_type":"stream","text":["\r\n"," 29%|██▊       | 6/21 [00:19<00:49,  3.33s/it]"]},{"name":"stdout","output_type":"stream","text":["Defaulting to user installation because normal site-packages is not writeable\n","Requirement already satisfied: pycountry in /home/u200810216/.local/lib/python3.7/site-packages (22.3.5)\n","Requirement already satisfied: setuptools in /opt/conda/lib/python3.7/site-packages (from pycountry) (58.0.4)\n"]},{"name":"stderr","output_type":"stream","text":["\r\n"," 33%|███▎      | 7/21 [00:23<00:46,  3.32s/it]"]},{"name":"stdout","output_type":"stream","text":["Defaulting to user installation because normal site-packages is not writeable\n","Requirement already satisfied: us in /home/u200810216/.local/lib/python3.7/site-packages (2.0.2)\n","Requirement already satisfied: jellyfish==0.6.1 in /home/u200810216/.local/lib/python3.7/site-packages (from us) (0.6.1)\n"]},{"name":"stderr","output_type":"stream","text":["\r\n"," 38%|███▊      | 8/21 [00:26<00:43,  3.31s/it]"]},{"name":"stdout","output_type":"stream","text":["Defaulting to user installation because normal site-packages is not writeable\n","Requirement already satisfied: uszipcode in /home/u200810216/.local/lib/python3.7/site-packages (1.0.1)\n","Requirement already satisfied: attrs in /opt/conda/lib/python3.7/site-packages (from uszipcode) (21.2.0)\n","Requirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from uszipcode) (2.26.0)\n","Requirement already satisfied: pathlib-mate in /home/u200810216/.local/lib/python3.7/site-packages (from uszipcode) (1.2.1)\n","Requirement already satisfied: atomicwrites in /home/u200810216/.local/lib/python3.7/site-packages (from uszipcode) (1.4.1)\n","Requirement already satisfied: fuzzywuzzy in /home/u200810216/.local/lib/python3.7/site-packages (from uszipcode) (0.18.0)\n","Requirement already satisfied: haversine>=2.5.0 in /home/u200810216/.local/lib/python3.7/site-packages (from uszipcode) (2.8.0)\n","Requirement already satisfied: SQLAlchemy>=1.4.0 in /home/u200810216/.local/lib/python3.7/site-packages (from uszipcode) (1.4.47)\n","Requirement already satisfied: sqlalchemy-mate>=1.4.28.3 in /home/u200810216/.local/lib/python3.7/site-packages (from uszipcode) (1.4.28.4)\n","Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from SQLAlchemy>=1.4.0->uszipcode) (4.8.1)\n","Requirement already satisfied: greenlet!=0.4.17 in /home/u200810216/.local/lib/python3.7/site-packages (from SQLAlchemy>=1.4.0->uszipcode) (2.0.2)\n","Requirement already satisfied: prettytable in /home/u200810216/.local/lib/python3.7/site-packages (from sqlalchemy-mate>=1.4.28.3->uszipcode) (3.7.0)\n","Requirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from pathlib-mate->uszipcode) (1.15.0)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->uszipcode) (1.26.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->uszipcode) (2020.6.20)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.7/site-packages (from requests->uszipcode) (2.0.4)\n","Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->uszipcode) (3.3)\n","Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->SQLAlchemy>=1.4.0->uszipcode) (3.6.0)\n","Requirement already satisfied: typing-extensions>=3.6.4 in /home/u200810216/.local/lib/python3.7/site-packages (from importlib-metadata->SQLAlchemy>=1.4.0->uszipcode) (4.5.0)\n","Requirement already satisfied: wcwidth in /opt/conda/lib/python3.7/site-packages (from prettytable->sqlalchemy-mate>=1.4.28.3->uszipcode) (0.2.5)\n"]},{"name":"stderr","output_type":"stream","text":["\r\n"," 43%|████▎     | 9/21 [00:29<00:40,  3.33s/it]"]},{"name":"stdout","output_type":"stream","text":["Defaulting to user installation because normal site-packages is not writeable\n","Requirement already satisfied: torch in /home/u200810216/.local/lib/python3.7/site-packages (1.13.1)\n","Requirement already satisfied: transformers in /home/u200810216/.local/lib/python3.7/site-packages (4.27.3)\n","Requirement already satisfied: typing-extensions in /home/u200810216/.local/lib/python3.7/site-packages (from torch) (4.5.0)\n","Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /home/u200810216/.local/lib/python3.7/site-packages (from torch) (11.7.99)\n","Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /home/u200810216/.local/lib/python3.7/site-packages (from torch) (8.5.0.96)\n","Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /home/u200810216/.local/lib/python3.7/site-packages (from torch) (11.10.3.66)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /home/u200810216/.local/lib/python3.7/site-packages (from torch) (11.7.99)\n","Requirement already satisfied: setuptools in /opt/conda/lib/python3.7/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch) (58.0.4)\n","Requirement already satisfied: wheel in /opt/conda/lib/python3.7/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch) (0.37.0)\n","Requirement already satisfied: filelock in /home/u200810216/.local/lib/python3.7/site-packages (from transformers) (3.10.4)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /home/u200810216/.local/lib/python3.7/site-packages (from transformers) (0.13.3)\n","Requirement already satisfied: numpy>=1.17 in /home/u200810216/.local/lib/python3.7/site-packages (from transformers) (1.21.6)\n","Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.7/site-packages (from transformers) (21.3)\n","Requirement already satisfied: pyyaml>=5.1 in /home/u200810216/.local/lib/python3.7/site-packages (from transformers) (6.0)\n","Requirement already satisfied: regex!=2019.12.17 in /home/u200810216/.local/lib/python3.7/site-packages (from transformers) (2022.10.31)\n","Requirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from transformers) (2.26.0)\n","Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /home/u200810216/.local/lib/python3.7/site-packages (from transformers) (0.13.2)\n","Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.7/site-packages (from transformers) (4.62.3)\n","Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from transformers) (4.8.1)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging>=20.0->transformers) (2.4.0)\n","Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->transformers) (3.6.0)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (1.26.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (2020.6.20)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (2.0.4)\n","Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (3.3)\n"]},{"name":"stderr","output_type":"stream","text":["\r\n"," 48%|████▊     | 10/21 [00:33<00:36,  3.34s/it]"]},{"name":"stdout","output_type":"stream","text":["Defaulting to user installation because normal site-packages is not writeable\n","Requirement already satisfied: geopandas in /home/u200810216/.local/lib/python3.7/site-packages (0.10.2)\n","Requirement already satisfied: matplotlib in /home/u200810216/.local/lib/python3.7/site-packages (3.0.3)\n","Requirement already satisfied: pandas in /home/u200810216/.local/lib/python3.7/site-packages (1.3.5)\n","Requirement already satisfied: shapely>=1.6 in /home/u200810216/.local/lib/python3.7/site-packages (from geopandas) (2.0.1)\n","Requirement already satisfied: fiona>=1.8 in /home/u200810216/.local/lib/python3.7/site-packages (from geopandas) (1.9.3)\n","Requirement already satisfied: pyproj>=2.2.0 in /home/u200810216/.local/lib/python3.7/site-packages (from geopandas) (3.2.1)\n","Requirement already satisfied: numpy>=1.10.0 in /home/u200810216/.local/lib/python3.7/site-packages (from matplotlib) (1.21.6)\n","Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.7/site-packages (from matplotlib) (0.10.0)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib) (1.1.0)\n","Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib) (2.4.0)\n","Requirement already satisfied: python-dateutil>=2.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib) (2.8.2)\n","Requirement already satisfied: pytz>=2017.3 in /home/u200810216/.local/lib/python3.7/site-packages (from pandas) (2022.7.1)\n","Requirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from cycler>=0.10->matplotlib) (1.15.0)\n","Requirement already satisfied: attrs>=19.2.0 in /opt/conda/lib/python3.7/site-packages (from fiona>=1.8->geopandas) (21.2.0)\n","Requirement already satisfied: certifi in /opt/conda/lib/python3.7/site-packages (from fiona>=1.8->geopandas) (2020.6.20)\n","Requirement already satisfied: click~=8.0 in /home/u200810216/.local/lib/python3.7/site-packages (from fiona>=1.8->geopandas) (8.1.2)\n","Requirement already satisfied: click-plugins>=1.0 in /home/u200810216/.local/lib/python3.7/site-packages (from fiona>=1.8->geopandas) (1.1.1)\n","Requirement already satisfied: cligj>=0.5 in /home/u200810216/.local/lib/python3.7/site-packages (from fiona>=1.8->geopandas) (0.7.2)\n","Requirement already satisfied: munch>=2.3.2 in /home/u200810216/.local/lib/python3.7/site-packages (from fiona>=1.8->geopandas) (2.5.0)\n","Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from fiona>=1.8->geopandas) (4.8.1)\n","Requirement already satisfied: setuptools in /opt/conda/lib/python3.7/site-packages (from kiwisolver>=1.0.1->matplotlib) (58.0.4)\n","Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->fiona>=1.8->geopandas) (3.6.0)\n","Requirement already satisfied: typing-extensions>=3.6.4 in /home/u200810216/.local/lib/python3.7/site-packages (from importlib-metadata->fiona>=1.8->geopandas) (4.5.0)\n"]},{"name":"stderr","output_type":"stream","text":["\r\n"," 52%|█████▏    | 11/21 [00:36<00:33,  3.36s/it]"]},{"name":"stdout","output_type":"stream","text":["Defaulting to user installation because normal site-packages is not writeable\n","Requirement already satisfied: geopandas in /home/u200810216/.local/lib/python3.7/site-packages (0.10.2)\n","Requirement already satisfied: descartes in /home/u200810216/.local/lib/python3.7/site-packages (1.1.0)\n","Requirement already satisfied: mapclassify in /home/u200810216/.local/lib/python3.7/site-packages (2.5.0)\n","Requirement already satisfied: pandas>=0.25.0 in /home/u200810216/.local/lib/python3.7/site-packages (from geopandas) (1.3.5)\n","Requirement already satisfied: shapely>=1.6 in /home/u200810216/.local/lib/python3.7/site-packages (from geopandas) (2.0.1)\n","Requirement already satisfied: fiona>=1.8 in /home/u200810216/.local/lib/python3.7/site-packages (from geopandas) (1.9.3)\n","Requirement already satisfied: pyproj>=2.2.0 in /home/u200810216/.local/lib/python3.7/site-packages (from geopandas) (3.2.1)\n","Requirement already satisfied: matplotlib in /home/u200810216/.local/lib/python3.7/site-packages (from descartes) (3.0.3)\n","Requirement already satisfied: scipy>=1.0 in /home/u200810216/.local/lib/python3.7/site-packages (from mapclassify) (1.7.3)\n","Requirement already satisfied: numpy>=1.3 in /home/u200810216/.local/lib/python3.7/site-packages (from mapclassify) (1.21.6)\n","Requirement already satisfied: scikit-learn in /home/u200810216/.local/lib/python3.7/site-packages (from mapclassify) (1.0.2)\n","Requirement already satisfied: networkx in /home/u200810216/.local/lib/python3.7/site-packages (from mapclassify) (2.6.3)\n","Requirement already satisfied: attrs>=19.2.0 in /opt/conda/lib/python3.7/site-packages (from fiona>=1.8->geopandas) (21.2.0)\n","Requirement already satisfied: certifi in /opt/conda/lib/python3.7/site-packages (from fiona>=1.8->geopandas) (2020.6.20)\n","Requirement already satisfied: click~=8.0 in /home/u200810216/.local/lib/python3.7/site-packages (from fiona>=1.8->geopandas) (8.1.2)\n","Requirement already satisfied: click-plugins>=1.0 in /home/u200810216/.local/lib/python3.7/site-packages (from fiona>=1.8->geopandas) (1.1.1)\n","Requirement already satisfied: cligj>=0.5 in /home/u200810216/.local/lib/python3.7/site-packages (from fiona>=1.8->geopandas) (0.7.2)\n","Requirement already satisfied: munch>=2.3.2 in /home/u200810216/.local/lib/python3.7/site-packages (from fiona>=1.8->geopandas) (2.5.0)\n","Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from fiona>=1.8->geopandas) (4.8.1)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.7/site-packages (from pandas>=0.25.0->geopandas) (2.8.2)\n","Requirement already satisfied: pytz>=2017.3 in /home/u200810216/.local/lib/python3.7/site-packages (from pandas>=0.25.0->geopandas) (2022.7.1)\n","Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.7/site-packages (from matplotlib->descartes) (0.10.0)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib->descartes) (1.1.0)\n","Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib->descartes) (2.4.0)\n","Requirement already satisfied: joblib>=0.11 in /home/u200810216/.local/lib/python3.7/site-packages (from scikit-learn->mapclassify) (1.2.0)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /home/u200810216/.local/lib/python3.7/site-packages (from scikit-learn->mapclassify) (3.1.0)\n","Requirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from cycler>=0.10->matplotlib->descartes) (1.15.0)\n","Requirement already satisfied: setuptools in /opt/conda/lib/python3.7/site-packages (from kiwisolver>=1.0.1->matplotlib->descartes) (58.0.4)\n","Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->fiona>=1.8->geopandas) (3.6.0)\n","Requirement already satisfied: typing-extensions>=3.6.4 in /home/u200810216/.local/lib/python3.7/site-packages (from importlib-metadata->fiona>=1.8->geopandas) (4.5.0)\n"]},{"name":"stderr","output_type":"stream","text":["\r\n"," 57%|█████▋    | 12/21 [00:40<00:30,  3.40s/it]"]},{"name":"stdout","output_type":"stream","text":["Defaulting to user installation because normal site-packages is not writeable\n","Requirement already satisfied: SciencePlots in /home/u200810216/.local/lib/python3.7/site-packages (2.0.1)\n","Requirement already satisfied: matplotlib in /home/u200810216/.local/lib/python3.7/site-packages (from SciencePlots) (3.0.3)\n","Requirement already satisfied: numpy>=1.10.0 in /home/u200810216/.local/lib/python3.7/site-packages (from matplotlib->SciencePlots) (1.21.6)\n","Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.7/site-packages (from matplotlib->SciencePlots) (0.10.0)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib->SciencePlots) (1.1.0)\n","Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib->SciencePlots) (2.4.0)\n","Requirement already satisfied: python-dateutil>=2.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib->SciencePlots) (2.8.2)\n","Requirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from cycler>=0.10->matplotlib->SciencePlots) (1.15.0)\n","Requirement already satisfied: setuptools in /opt/conda/lib/python3.7/site-packages (from kiwisolver>=1.0.1->matplotlib->SciencePlots) (58.0.4)\n"]},{"name":"stderr","output_type":"stream","text":["\r\n"," 62%|██████▏   | 13/21 [00:43<00:26,  3.37s/it]"]},{"name":"stdout","output_type":"stream","text":["Defaulting to user installation because normal site-packages is not writeable\n","Requirement already satisfied: geopandas in /home/u200810216/.local/lib/python3.7/site-packages (0.10.2)\n","Requirement already satisfied: matplotlib in /home/u200810216/.local/lib/python3.7/site-packages (3.0.3)\n","Requirement already satisfied: pandas in /home/u200810216/.local/lib/python3.7/site-packages (1.3.5)\n","Requirement already satisfied: shapely>=1.6 in /home/u200810216/.local/lib/python3.7/site-packages (from geopandas) (2.0.1)\n","Requirement already satisfied: fiona>=1.8 in /home/u200810216/.local/lib/python3.7/site-packages (from geopandas) (1.9.3)\n","Requirement already satisfied: pyproj>=2.2.0 in /home/u200810216/.local/lib/python3.7/site-packages (from geopandas) (3.2.1)\n","Requirement already satisfied: numpy>=1.10.0 in /home/u200810216/.local/lib/python3.7/site-packages (from matplotlib) (1.21.6)\n","Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.7/site-packages (from matplotlib) (0.10.0)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib) (1.1.0)\n","Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib) (2.4.0)\n","Requirement already satisfied: python-dateutil>=2.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib) (2.8.2)\n","Requirement already satisfied: pytz>=2017.3 in /home/u200810216/.local/lib/python3.7/site-packages (from pandas) (2022.7.1)\n","Requirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from cycler>=0.10->matplotlib) (1.15.0)\n","Requirement already satisfied: attrs>=19.2.0 in /opt/conda/lib/python3.7/site-packages (from fiona>=1.8->geopandas) (21.2.0)\n","Requirement already satisfied: certifi in /opt/conda/lib/python3.7/site-packages (from fiona>=1.8->geopandas) (2020.6.20)\n","Requirement already satisfied: click~=8.0 in /home/u200810216/.local/lib/python3.7/site-packages (from fiona>=1.8->geopandas) (8.1.2)\n","Requirement already satisfied: click-plugins>=1.0 in /home/u200810216/.local/lib/python3.7/site-packages (from fiona>=1.8->geopandas) (1.1.1)\n","Requirement already satisfied: cligj>=0.5 in /home/u200810216/.local/lib/python3.7/site-packages (from fiona>=1.8->geopandas) (0.7.2)\n","Requirement already satisfied: munch>=2.3.2 in /home/u200810216/.local/lib/python3.7/site-packages (from fiona>=1.8->geopandas) (2.5.0)\n","Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from fiona>=1.8->geopandas) (4.8.1)\n","Requirement already satisfied: setuptools in /opt/conda/lib/python3.7/site-packages (from kiwisolver>=1.0.1->matplotlib) (58.0.4)\n","Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->fiona>=1.8->geopandas) (3.6.0)\n","Requirement already satisfied: typing-extensions>=3.6.4 in /home/u200810216/.local/lib/python3.7/site-packages (from importlib-metadata->fiona>=1.8->geopandas) (4.5.0)\n"]},{"name":"stderr","output_type":"stream","text":["\r\n"," 67%|██████▋   | 14/21 [00:46<00:23,  3.38s/it]"]},{"name":"stdout","output_type":"stream","text":["Defaulting to user installation because normal site-packages is not writeable\n","Requirement already satisfied: ta in /home/u200810216/.local/lib/python3.7/site-packages (0.10.2)\n","Requirement already satisfied: numpy in /home/u200810216/.local/lib/python3.7/site-packages (from ta) (1.21.6)\n","Requirement already satisfied: pandas in /home/u200810216/.local/lib/python3.7/site-packages (from ta) (1.3.5)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.7/site-packages (from pandas->ta) (2.8.2)\n","Requirement already satisfied: pytz>=2017.3 in /home/u200810216/.local/lib/python3.7/site-packages (from pandas->ta) (2022.7.1)\n","Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.7/site-packages (from python-dateutil>=2.7.3->pandas->ta) (1.15.0)\n"]},{"name":"stderr","output_type":"stream","text":["\r\n"," 71%|███████▏  | 15/21 [00:50<00:20,  3.37s/it]"]},{"name":"stdout","output_type":"stream","text":["Defaulting to user installation because normal site-packages is not writeable\n","Requirement already satisfied: pytrends in /home/u200810216/.local/lib/python3.7/site-packages (4.9.2)\n","Requirement already satisfied: requests>=2.0 in /opt/conda/lib/python3.7/site-packages (from pytrends) (2.26.0)\n","Requirement already satisfied: pandas>=0.25 in /home/u200810216/.local/lib/python3.7/site-packages (from pytrends) (1.3.5)\n","Requirement already satisfied: lxml in /home/u200810216/.local/lib/python3.7/site-packages (from pytrends) (4.9.2)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.7/site-packages (from pandas>=0.25->pytrends) (2.8.2)\n","Requirement already satisfied: pytz>=2017.3 in /home/u200810216/.local/lib/python3.7/site-packages (from pandas>=0.25->pytrends) (2022.7.1)\n","Requirement already satisfied: numpy>=1.17.3 in /home/u200810216/.local/lib/python3.7/site-packages (from pandas>=0.25->pytrends) (1.21.6)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests>=2.0->pytrends) (1.26.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests>=2.0->pytrends) (2020.6.20)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.7/site-packages (from requests>=2.0->pytrends) (2.0.4)\n","Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests>=2.0->pytrends) (3.3)\n","Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.7/site-packages (from python-dateutil>=2.7.3->pandas>=0.25->pytrends) (1.15.0)\n"]},{"name":"stderr","output_type":"stream","text":["\r\n"," 76%|███████▌  | 16/21 [00:53<00:16,  3.36s/it]"]},{"name":"stdout","output_type":"stream","text":["Defaulting to user installation because normal site-packages is not writeable\n","Requirement already satisfied: chinese-calendar in /home/u200810216/.local/lib/python3.7/site-packages (1.8.0)\n"]},{"name":"stderr","output_type":"stream","text":["\r\n"," 81%|████████  | 17/21 [00:56<00:13,  3.34s/it]"]},{"name":"stdout","output_type":"stream","text":["Defaulting to user installation because normal site-packages is not writeable\n","Requirement already satisfied: ephem in /home/u200810216/.local/lib/python3.7/site-packages (4.1.4)\n"]},{"name":"stderr","output_type":"stream","text":["\r\n"," 86%|████████▌ | 18/21 [01:00<00:09,  3.33s/it]"]},{"name":"stdout","output_type":"stream","text":["Defaulting to user installation because normal site-packages is not writeable\n","Requirement already satisfied: category_encoders in /home/u200810216/.local/lib/python3.7/site-packages (2.6.0)\n","Requirement already satisfied: numpy>=1.14.0 in /home/u200810216/.local/lib/python3.7/site-packages (from category_encoders) (1.21.6)\n","Requirement already satisfied: scikit-learn>=0.20.0 in /home/u200810216/.local/lib/python3.7/site-packages (from category_encoders) (1.0.2)\n","Requirement already satisfied: scipy>=1.0.0 in /home/u200810216/.local/lib/python3.7/site-packages (from category_encoders) (1.7.3)\n","Requirement already satisfied: statsmodels>=0.9.0 in /home/u200810216/.local/lib/python3.7/site-packages (from category_encoders) (0.13.5)\n","Requirement already satisfied: pandas>=1.0.5 in /home/u200810216/.local/lib/python3.7/site-packages (from category_encoders) (1.3.5)\n","Requirement already satisfied: patsy>=0.5.1 in /home/u200810216/.local/lib/python3.7/site-packages (from category_encoders) (0.5.3)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.7/site-packages (from pandas>=1.0.5->category_encoders) (2.8.2)\n","Requirement already satisfied: pytz>=2017.3 in /home/u200810216/.local/lib/python3.7/site-packages (from pandas>=1.0.5->category_encoders) (2022.7.1)\n","Requirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from patsy>=0.5.1->category_encoders) (1.15.0)\n","Requirement already satisfied: joblib>=0.11 in /home/u200810216/.local/lib/python3.7/site-packages (from scikit-learn>=0.20.0->category_encoders) (1.2.0)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /home/u200810216/.local/lib/python3.7/site-packages (from scikit-learn>=0.20.0->category_encoders) (3.1.0)\n","Requirement already satisfied: packaging>=21.3 in /opt/conda/lib/python3.7/site-packages (from statsmodels>=0.9.0->category_encoders) (21.3)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging>=21.3->statsmodels>=0.9.0->category_encoders) (2.4.0)\n"]},{"name":"stderr","output_type":"stream","text":["\r\n"," 90%|█████████ | 19/21 [01:03<00:06,  3.33s/it]"]},{"name":"stdout","output_type":"stream","text":["Defaulting to user installation because normal site-packages is not writeable\n","Collecting fbprophet\n","  Downloading fbprophet-0.7.1.tar.gz (64 kB)\n","     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 64.0/64.0 kB 94.8 kB/s eta 0:00:00\n","  Preparing metadata (setup.py): started\n","  Preparing metadata (setup.py): finished with status 'done'\n","Collecting Cython>=0.22 (from fbprophet)\n","  Downloading Cython-0.29.34-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl (1.9 MB)\n","     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.9/1.9 MB 1.6 MB/s eta 0:00:00\n","Collecting cmdstanpy==0.9.5 (from fbprophet)\n","  Downloading cmdstanpy-0.9.5-py3-none-any.whl (37 kB)\n","Collecting pystan>=2.14 (from fbprophet)\n","  Downloading pystan-3.3.0-py3-none-any.whl (13 kB)\n","Requirement already satisfied: numpy>=1.15.4 in /home/u200810216/.local/lib/python3.7/site-packages (from fbprophet) (1.21.6)\n","Requirement already satisfied: pandas>=1.0.4 in /home/u200810216/.local/lib/python3.7/site-packages (from fbprophet) (1.3.5)\n","Requirement already satisfied: matplotlib>=2.0.0 in /home/u200810216/.local/lib/python3.7/site-packages (from fbprophet) (3.0.3)\n","Collecting LunarCalendar>=0.0.9 (from fbprophet)\n","  Downloading LunarCalendar-0.0.9-py2.py3-none-any.whl (18 kB)\n","Collecting convertdate>=2.1.2 (from fbprophet)\n","  Downloading convertdate-2.4.0-py3-none-any.whl (47 kB)\n","     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 47.9/47.9 kB 1.6 MB/s eta 0:00:00\n","Collecting holidays>=0.10.2 (from fbprophet)\n","  Downloading holidays-0.24-py3-none-any.whl (499 kB)\n","     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 499.9/499.9 kB 3.2 MB/s eta 0:00:00\n","Collecting setuptools-git>=1.2 (from fbprophet)\n","  Downloading setuptools_git-1.2-py2.py3-none-any.whl (10 kB)\n","Requirement already satisfied: python-dateutil>=2.8.0 in /opt/conda/lib/python3.7/site-packages (from fbprophet) (2.8.2)\n","Requirement already satisfied: tqdm>=4.36.1 in /opt/conda/lib/python3.7/site-packages (from fbprophet) (4.62.3)\n","Collecting pymeeus<=1,>=0.3.13 (from convertdate>=2.1.2->fbprophet)\n","  Downloading PyMeeus-0.5.12.tar.gz (5.8 MB)\n","     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 5.8/5.8 MB 8.3 MB/s eta 0:00:00\n","  Preparing metadata (setup.py): started\n","  Preparing metadata (setup.py): finished with status 'done'\n","Collecting hijri-converter (from holidays>=0.10.2->fbprophet)\n","  Downloading hijri_converter-2.3.1-py3-none-any.whl (13 kB)\n","Collecting korean-lunar-calendar (from holidays>=0.10.2->fbprophet)\n","  Downloading korean_lunar_calendar-0.3.1-py3-none-any.whl (9.0 kB)\n","Requirement already satisfied: ephem>=3.7.5.3 in /home/u200810216/.local/lib/python3.7/site-packages (from LunarCalendar>=0.0.9->fbprophet) (4.1.4)\n","Requirement already satisfied: pytz in /home/u200810216/.local/lib/python3.7/site-packages (from LunarCalendar>=0.0.9->fbprophet) (2022.7.1)\n","Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.7/site-packages (from matplotlib>=2.0.0->fbprophet) (0.10.0)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib>=2.0.0->fbprophet) (1.1.0)\n","Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib>=2.0.0->fbprophet) (2.4.0)\n","Requirement already satisfied: aiohttp<4.0,>=3.6 in /home/u200810216/.local/lib/python3.7/site-packages (from pystan>=2.14->fbprophet) (3.8.4)\n","Collecting clikit<0.7,>=0.6 (from pystan>=2.14->fbprophet)\n","  Downloading clikit-0.6.2-py2.py3-none-any.whl (91 kB)\n","     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 91.8/91.8 kB 272.6 kB/s eta 0:00:00\n","Collecting httpstan<4.7,>=4.6 (from pystan>=2.14->fbprophet)\n","  Downloading httpstan-4.6.1-cp37-cp37m-manylinux_2_24_x86_64.whl (40.1 MB)\n","     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 40.1/40.1 MB 9.3 MB/s eta 0:00:00\n","Collecting pysimdjson<4.0,>=3.2 (from pystan>=2.14->fbprophet)\n","  Downloading pysimdjson-3.2.0-cp37-cp37m-manylinux2014_x86_64.whl (2.4 MB)\n","     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.4/2.4 MB 16.3 MB/s eta 0:00:00\n","Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.7/site-packages (from python-dateutil>=2.8.0->fbprophet) (1.15.0)\n","Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp<4.0,>=3.6->pystan>=2.14->fbprophet) (21.2.0)\n","Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp<4.0,>=3.6->pystan>=2.14->fbprophet) (2.0.4)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /home/u200810216/.local/lib/python3.7/site-packages (from aiohttp<4.0,>=3.6->pystan>=2.14->fbprophet) (6.0.4)\n","Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /home/u200810216/.local/lib/python3.7/site-packages (from aiohttp<4.0,>=3.6->pystan>=2.14->fbprophet) (4.0.2)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /home/u200810216/.local/lib/python3.7/site-packages (from aiohttp<4.0,>=3.6->pystan>=2.14->fbprophet) (1.8.2)\n","Requirement already satisfied: frozenlist>=1.1.1 in /home/u200810216/.local/lib/python3.7/site-packages (from aiohttp<4.0,>=3.6->pystan>=2.14->fbprophet) (1.3.3)\n","Requirement already satisfied: aiosignal>=1.1.2 in /home/u200810216/.local/lib/python3.7/site-packages (from aiohttp<4.0,>=3.6->pystan>=2.14->fbprophet) (1.3.1)\n","Requirement already satisfied: asynctest==0.13.0 in /home/u200810216/.local/lib/python3.7/site-packages (from aiohttp<4.0,>=3.6->pystan>=2.14->fbprophet) (0.13.0)\n","Requirement already satisfied: typing-extensions>=3.7.4 in /home/u200810216/.local/lib/python3.7/site-packages (from aiohttp<4.0,>=3.6->pystan>=2.14->fbprophet) (4.5.0)\n","Collecting crashtest<0.4.0,>=0.3.0 (from clikit<0.7,>=0.6->pystan>=2.14->fbprophet)\n","  Downloading crashtest-0.3.1-py3-none-any.whl (7.0 kB)\n","Collecting pastel<0.3.0,>=0.2.0 (from clikit<0.7,>=0.6->pystan>=2.14->fbprophet)\n","  Downloading pastel-0.2.1-py2.py3-none-any.whl (6.0 kB)\n","Collecting pylev<2.0,>=1.3 (from clikit<0.7,>=0.6->pystan>=2.14->fbprophet)\n","  Downloading pylev-1.4.0-py2.py3-none-any.whl (6.1 kB)\n","Collecting appdirs<2.0,>=1.4 (from httpstan<4.7,>=4.6->pystan>=2.14->fbprophet)\n","  Downloading appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)\n","Collecting marshmallow<4.0,>=3.10 (from httpstan<4.7,>=4.6->pystan>=2.14->fbprophet)\n","  Downloading marshmallow-3.19.0-py3-none-any.whl (49 kB)\n","     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 49.1/49.1 kB 335.0 kB/s eta 0:00:00\n","Requirement already satisfied: setuptools>=41.0 in /opt/conda/lib/python3.7/site-packages (from httpstan<4.7,>=4.6->pystan>=2.14->fbprophet) (58.0.4)\n","Collecting webargs<9.0,>=8.0 (from httpstan<4.7,>=4.6->pystan>=2.14->fbprophet)\n","  Downloading webargs-8.2.0-py3-none-any.whl (30 kB)\n","Requirement already satisfied: packaging>=17.0 in /opt/conda/lib/python3.7/site-packages (from marshmallow<4.0,>=3.10->httpstan<4.7,>=4.6->pystan>=2.14->fbprophet) (21.3)\n","Requirement already satisfied: idna>=2.0 in /opt/conda/lib/python3.7/site-packages (from yarl<2.0,>=1.0->aiohttp<4.0,>=3.6->pystan>=2.14->fbprophet) (3.3)\n","Building wheels for collected packages: fbprophet, pymeeus\n","  Building wheel for fbprophet (setup.py): started\n","  Building wheel for fbprophet (setup.py): finished with status 'error'\n","  Running setup.py clean for fbprophet\n"]},{"name":"stderr","output_type":"stream","text":["  error: subprocess-exited-with-error\n","  \n","  × python setup.py bdist_wheel did not run successfully.\n","  │ exit code: 1\n","  ╰─> [45 lines of output]\n","      running bdist_wheel\n","      running build\n","      running build_py\n","      creating build\n","      creating build/lib\n","      creating build/lib/fbprophet\n","      creating build/lib/fbprophet/stan_model\n","      Traceback (most recent call last):\n","        File \"<string>\", line 36, in <module>\n","        File \"<pip-setuptools-caller>\", line 34, in <module>\n","        File \"/tmp/pip-install-vogm942y/fbprophet_cacf5a34fc3f4cceb3d36d6a373cef7a/setup.py\", line 149, in <module>\n","          long_description_content_type='text/markdown',\n","        File \"/opt/conda/lib/python3.7/site-packages/setuptools/__init__.py\", line 153, in setup\n","          return distutils.core.setup(**attrs)\n","        File \"/opt/conda/lib/python3.7/distutils/core.py\", line 148, in setup\n","          dist.run_commands()\n","        File \"/opt/conda/lib/python3.7/distutils/dist.py\", line 966, in run_commands\n","          self.run_command(cmd)\n","        File \"/opt/conda/lib/python3.7/distutils/dist.py\", line 985, in run_command\n","          cmd_obj.run()\n","        File \"/opt/conda/lib/python3.7/site-packages/wheel/bdist_wheel.py\", line 299, in run\n","          self.run_command('build')\n","        File \"/opt/conda/lib/python3.7/distutils/cmd.py\", line 313, in run_command\n","          self.distribution.run_command(command)\n","        File \"/opt/conda/lib/python3.7/distutils/dist.py\", line 985, in run_command\n","          cmd_obj.run()\n","        File \"/opt/conda/lib/python3.7/distutils/command/build.py\", line 135, in run\n","          self.run_command(cmd_name)\n","        File \"/opt/conda/lib/python3.7/distutils/cmd.py\", line 313, in run_command\n","          self.distribution.run_command(command)\n","        File \"/opt/conda/lib/python3.7/distutils/dist.py\", line 985, in run_command\n","          cmd_obj.run()\n","        File \"/tmp/pip-install-vogm942y/fbprophet_cacf5a34fc3f4cceb3d36d6a373cef7a/setup.py\", line 48, in run\n","          build_models(target_dir)\n","        File \"/tmp/pip-install-vogm942y/fbprophet_cacf5a34fc3f4cceb3d36d6a373cef7a/setup.py\", line 36, in build_models\n","          from fbprophet.models import StanBackendEnum\n","        File \"/tmp/pip-install-vogm942y/fbprophet_cacf5a34fc3f4cceb3d36d6a373cef7a/fbprophet/__init__.py\", line 8, in <module>\n","          from fbprophet.forecaster import Prophet\n","        File \"/tmp/pip-install-vogm942y/fbprophet_cacf5a34fc3f4cceb3d36d6a373cef7a/fbprophet/forecaster.py\", line 17, in <module>\n","          from fbprophet.make_holidays import get_holiday_names, make_holidays_df\n","        File \"/tmp/pip-install-vogm942y/fbprophet_cacf5a34fc3f4cceb3d36d6a373cef7a/fbprophet/make_holidays.py\", line 14, in <module>\n","          import fbprophet.hdays as hdays_part2\n","        File \"/tmp/pip-install-vogm942y/fbprophet_cacf5a34fc3f4cceb3d36d6a373cef7a/fbprophet/hdays.py\", line 13, in <module>\n","          from convertdate.islamic import from_gregorian, to_gregorian\n","      ModuleNotFoundError: No module named 'convertdate'\n","      [end of output]\n","  \n","  note: This error originates from a subprocess, and is likely not a problem with pip.\n","  ERROR: Failed building wheel for fbprophet\n"]},{"name":"stdout","output_type":"stream","text":["  Building wheel for pymeeus (setup.py): started\n"]},{"name":"stderr","output_type":"stream","text":["ERROR: Could not build wheels for fbprophet, which is required to install pyproject.toml-based projects\n","\r\n"," 95%|█████████▌| 20/21 [01:57<00:18, 18.43s/it]"]},{"name":"stdout","output_type":"stream","text":["  Building wheel for pymeeus (setup.py): finished with status 'done'\n","  Created wheel for pymeeus: filename=PyMeeus-0.5.12-py3-none-any.whl size=732018 sha256=d65d09aed1a8c0895788557e771f919ff4ce122a0e9d61bbe01a490d31ce9456\n","  Stored in directory: /home/u200810216/.cache/pip/wheels/1e/5a/c0/fca16a6d08089e38ac5a71f3f8ea7a9a12b3356ff3debd8c59\n","Successfully built pymeeus\n","Failed to build fbprophet\n","Defaulting to user installation because normal site-packages is not writeable\n","Requirement already satisfied: statsmodels in /home/u200810216/.local/lib/python3.7/site-packages (0.13.5)\n","Requirement already satisfied: pandas>=0.25 in /home/u200810216/.local/lib/python3.7/site-packages (from statsmodels) (1.3.5)\n","Requirement already satisfied: patsy>=0.5.2 in /home/u200810216/.local/lib/python3.7/site-packages (from statsmodels) (0.5.3)\n","Requirement already satisfied: packaging>=21.3 in /opt/conda/lib/python3.7/site-packages (from statsmodels) (21.3)\n","Requirement already satisfied: scipy>=1.3 in /home/u200810216/.local/lib/python3.7/site-packages (from statsmodels) (1.7.3)\n","Requirement already satisfied: numpy>=1.17 in /home/u200810216/.local/lib/python3.7/site-packages (from statsmodels) (1.21.6)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging>=21.3->statsmodels) (2.4.0)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.7/site-packages (from pandas>=0.25->statsmodels) (2.8.2)\n","Requirement already satisfied: pytz>=2017.3 in /home/u200810216/.local/lib/python3.7/site-packages (from pandas>=0.25->statsmodels) (2022.7.1)\n","Requirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from patsy>=0.5.2->statsmodels) (1.15.0)\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 21/21 [02:00<00:00,  5.74s/it]\n"]}],"source":["! pip install tqdm\n","! sudo apt-get update\n","import os\n","from tqdm import tqdm\n","\n","def install_package_with_apt(package_name):\n","    os.system(f\"sudo apt-get install --allow-change-held-packages --yes {package_name}\")\n","\n","def install_package_with_pip(package_name):\n","    os.system(f\"pip install {package_name}\")\n","\n","apt_packages = [\n","    \"texlive-latex-recommended\",\n","    \"dvipng\",\n","    \"texlive-latex-extra\",\n","    \"texlive-fonts-recommended\",\n","    \"cm-super\",\n","]\n","\n","pip_packages = [\n","    \"Pillow\",\n","    \"umap\",\n","    \"xgboost\",\n","    \"seaborn\",\n","    \"SciencePlots\",\n","    \"pytorch-tabnet\",\n","    \"pycountry\",\n","    \"us\",\n","    \"uszipcode\",\n","    \"torch transformers\",\n","    \"geopandas matplotlib pandas\",\n","    \"geopandas descartes mapclassify\",\n","    \"SciencePlots\",\n","    \"geopandas matplotlib pandas\",\n","    \"ta\",\n","    \"pytrends\",\n","    \"chinese-calendar\",\n","    \"ephem\",\n","    \"category_encoders\",\n","    \"fbprophet\",\n","    \"statsmodels\",\n","]\n","\n","for package in tqdm(apt_packages):\n","    install_package_with_apt(package)\n","\n","for package in tqdm(pip_packages):\n","    install_package_with_pip(package)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ztN89UbhJXzm"},"outputs":[],"source":["import tensorflow as tf\n","print(tf.config.list_physical_devices('GPU'))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eKT7EVfgTiNj"},"outputs":[],"source":["# !pip uninstall tensorflow\n","# !pip install tensorflow-gpu\n","# !pip uninstall SciencePlots -y\n","# !pip install SciencePlots"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aJdsrp1VTiNj"},"outputs":[],"source":["import tensorflow as tf\n","print(tf.__version__)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"H9EW3f8pTiNk"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","print(plt.style.available)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nchjIHjzTiNk","outputId":"522a6e80-47cd-4eeb-ca3a-5d523f29ac04"},"outputs":[{"name":"stdout","output_type":"stream","text":["当前Python解释器的地址： /opt/conda/bin/python\n"]}],"source":["import sys\n","\n","print(\"当前Python解释器的地址：\", sys.executable)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wKDTLEhttliL","outputId":"23b5e697-2943-41bc-9e4d-83215b592654"},"outputs":[{"name":"stdout","output_type":"stream","text":["Currently running servers:\r\n","http://0.0.0.0:8888/proxy/service/2d3d1f0804005/ :: /home/u200810216/jupyter\r\n"]}],"source":["!jupyter notebook list\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4MPbHuDWtliL"},"outputs":[],"source":["# 确保已安装jupyter_contrib_nbextensions。如果您还没有安装它，可以使用以下命令进行安装：\n","# 建议打开putty su - u200810216切换到普通用户，然后执行以下命令\n","# pip install jupyter_contrib_nbextensions\n","# jupyter contrib nbextension install --user\n","# jupyter nbextension enable toc2/main\n","# jupyter nbextension enable toc2/main --user --section=notebook"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wNVhhpPrtliL"},"outputs":[],"source":["!pip install transformers datasets evaluate"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jVrgKt9FJXzo"},"outputs":[],"source":["!pip install SciencePlots"]},{"cell_type":"markdown","metadata":{"id":"nSx72OyBtliL"},"source":["# Model Parameter Search"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FcKR_h08tliM"},"outputs":[],"source":["os.chdir('/home/u200810216/jupyter/')"]},{"cell_type":"markdown","metadata":{"id":"DMsXqXpBtliM"},"source":["## 简陋版本"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"s7_c9lJbtliM"},"outputs":[],"source":["import os\n","import itertools\n","import numpy as np\n","import tensorflow as tf\n","import keras\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Dropout, Flatten, LSTM\n","from tensorflow.keras.layers import Conv2D\n","from tensorflow.keras.layers import BatchNormalization, Activation, Reshape\n","from tensorflow.keras.layers import AveragePooling2D, Dropout\n","from tensorflow.keras.callbacks import ModelCheckpoint\n","import matplotlib.pyplot as plt\n","import pickle\n","import json\n","\n","# Set up your model\n","def create_model(use_layer_7, use_layer_8):\n","    model = Sequential()\n","\n","    #layer1\n","    model.add(Conv2D(128, kernel_size=(n_ch, 1), padding=\"valid\", input_shape=input_shape))\n","    model.add(BatchNormalization())\n","    model.add(Activation('relu'))\n","    model.add(Reshape((model.output_shape[3],model.output_shape[2],model.output_shape[1])))\n","    # model.add(AveragePooling2D(pool_size=(1, 2)))\n","    # model.add(Dropout(0.1))\n","\n","    ## Layer 2\n","    model.add(Conv2D(256, kernel_size=(128, 1), padding=\"valid\"))\n","    model.add(BatchNormalization())\n","    model.add(Activation('relu'))\n","    model.add(Reshape((model.output_shape[3],model.output_shape[2],model.output_shape[1])))\n","    # model.add(AveragePooling2D(pool_size=(1, 2)))\n","    # model.add(Dropout(0.1))\n","\n","    ## Layer 3\n","    model.add(Conv2D(512, kernel_size=(256, 1), padding=\"valid\"))\n","    model.add(BatchNormalization())\n","    model.add(Activation('relu'))\n","    model.add(Reshape((model.output_shape[3],model.output_shape[2],model.output_shape[1])))\n","    # model.add(AveragePooling2D(pool_size=(1, 2)))\n","    # model.add(Dropout(0.1))\n","\n","    ## Layer 4\n","    model.add(Conv2D(1024, kernel_size=(512, 2), padding=\"valid\"))\n","    model.add(BatchNormalization())\n","    model.add(Activation('relu'))\n","    model.add(Reshape((model.output_shape[3],model.output_shape[2],model.output_shape[1])))\n","    # model.add(AveragePooling2D(pool_size=(4, 1)))\n","    model.add(Flatten())\n","\n","\n","    # layer 5\n","    model.add(Dense(192))\n","    model.add(Activation('relu'))\n","    # model.add(Dropout(0.4))\n","\n","\n","    model.add(Reshape((model.output_shape[1], 1)))\n","    # layer 6\n","    model.add(LSTM(192))\n","\n","    if use_layer_7:\n","        model.add(Reshape((model.output_shape[1], 1)))\n","        model.add(LSTM(192))\n","\n","    if use_layer_8:\n","        model.add(Dense(192))\n","\n","    # Add output layer\n","    # ...\n","\n","    # layer 9\n","    model.add(Dense(192))\n","\n","\n","    #output layer\n","    model.add(Dense(num_classes))\n","    model.add(Activation('softmax'))\n","\n","    model.summary()\n","    return model\n","\n","# Set up experiment parameters\n","batch_sizes = [128, 256]\n","learning_rates = [0.01, 0.1, 1]\n","decays = [0.01, 0.1, 1]\n","shuffles = [True, False]\n","use_layer_7_options = [True, False]\n","use_layer_8_options = [True, False]\n","epochs = 200\n","\n","# Calculate total combinations\n","total_combinations = len(batch_sizes) * len(learning_rates) * len(decays) * len(shuffles) * len(use_layer_7_options) * len(use_layer_8_options)\n","print(f'Total parameter combinations: {total_combinations}')\n","\n","# Create combinations of parameters\n","combinations = list(itertools.product(batch_sizes, learning_rates, decays, shuffles, use_layer_7_options, use_layer_8_options))\n","\n","# Set up experiment state\n","experiment_state_path = 'experiment_state.pickle'\n","if os.path.exists(experiment_state_path):\n","    with open(experiment_state_path, 'rb') as f:\n","        experiment_state = pickle.load(f)\n","    start_index = experiment_state['completed_combinations']\n","else:\n","    experiment_state = {'completed_combinations': 0}\n","    start_index = 0\n","try:\n","    # Iterate through combinations\n","    for idx, (batch_size, lr, decay, shuffle, use_layer_7, use_layer_8) in enumerate(combinations[start_index:], start_index):\n","        # Set up model name and file names\n","        model_name = f'model_LSTMCNN_epochs200_batch{batch_size}_shuffle{shuffle}_lr{lr}_decay{decay}_layer7_{use_layer_7}_layer8_{use_layer_8}'\n","        log_dir = f\"logs/fit/{model_name}\"\n","        os.makedirs(log_dir, exist_ok=True)\n","\n","        checkpoint_path = os.path.join(log_dir, f'{model_name}_checkpoint.h5')\n","        final_model_path = os.path.join(log_dir, f'{model_name}_final.h5')\n","        acc_plot_path = os.path.join(log_dir, f'{model_name}_accuracy.png')\n","        loss_plot_path = os.path.join(log_dir, f'{model_name}_loss.png')\n","        history_data_path = os.path.join(log_dir, f'{model_name}_history.json')\n","\n","        # Create model\n","        model = create_model(use_layer_7, use_layer_8)\n","\n","        # Compile the model\n","        model.compile(loss=keras.losses.categorical_crossentropy,\n","                      optimizer=tf.keras.optimizers.legacy.Adam(learning_rate=lr, decay=decay),\n","                      metrics=['accuracy', 'FalsePositives', 'FalseNegatives'])\n","\n","        # Set up callbacks\n","        log_dir = f\"logs/fit/{model_name}\"\n","        tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n","        checkpoint_callback = ModelCheckpoint(\n","            filepath=checkpoint_path,\n","            save_weights_only=False,\n","            save_best_only=True,\n","            monitor='val_loss',\n","            verbose=1,\n","        )\n","\n","\n","        # Check if the checkpoint file exists\n","        if os.path.exists(checkpoint_path):\n","            # Load the model from the checkpoint file\n","            model = tf.keras.models.load_model(checkpoint_path)\n","            print(f\"Loaded model from {checkpoint_path}\")\n","            initial_epoch = model.optimizer.iterations.numpy() // (len(x_train) // batch_size)\n","        else:\n","            print(f\"No checkpoint file found at {checkpoint_path}, training from scratch.\")\n","            initial_epoch = 0\n","\n","\n","        # Train the model\n","        history = model.fit(x_train, y_train,\n","                            batch_size=batch_size,\n","                            epochs=epochs,\n","                            shuffle=shuffle,\n","                            verbose=1,\n","                            validation_split=0.2,\n","                            callbacks=[tensorboard_callback, checkpoint_callback],\n","                            initial_epoch=initial_epoch)\n","\n","        # Save the final model\n","        model.save(final_model_path)\n","\n","        # Evaluate the model\n","        score_train = model.evaluate(x_train, y_train, verbose=0)\n","        score_test = model.evaluate(x_test, y_test, verbose=0)\n","        print(\"mean train accuracy is:   \", (score_train[1]))\n","        print(\"mean test accuracy is:   \", (score_test[1]))\n","        print(\"FAR is:   \", (score_train[2] + score_test[2]) / (np.size(y_train, 0) + np.size(y_test, 0)))\n","        print(\"FRR is:   \", (score_train[3] + score_test[3]) / (np.size(y_train, 0) + np.size(y_test, 0)))\n","\n","        # Plot and save accuracy\n","        plt.plot(history.history['accuracy'])\n","        plt.plot(history.history['val_accuracy'])\n","        plt.title('model accuracy')\n","        plt.ylabel('accuracy')\n","        plt.xlabel('epoch')\n","        plt.legend(['train', 'validation'], loc='upper left')\n","        plt.grid(True)\n","        plt.savefig(acc_plot_path, dpi=600)\n","        plt.show()\n","\n","        # Plot and save loss\n","        plt.plot(history.history['loss'])\n","        plt.plot(history.history['val_loss'])\n","        plt.title('model loss')\n","        plt.ylabel('loss')\n","        plt.xlabel('epoch')\n","        plt.legend(['train', 'validation'], loc='upper left')\n","        plt.grid(True)\n","        plt.savefig(loss_plot_path, dpi=600)\n","        plt.show()\n","        # Save history data to a JSON file\n","\n","        with open(history_data_path, 'w') as history_file:\n","            json.dump(history.history, history_file)\n","        # Update experiment state\n","        experiment_state['completed_combinations'] = idx + 1\n","        with open(experiment_state_path, 'wb') as f:\n","            pickle.dump(experiment_state, f)\n","\n","\n","\n","\n","except KeyboardInterrupt:\n","\n","    print(\"\\n\\n\\nInterrupted, saving experiment state...\")\n","    with open(experiment_state_path, 'wb') as f:\n","        pickle.dump(experiment_state, f)\n","    print(\"Experiment state saved. You can resume the experiment later.\")"]},{"cell_type":"markdown","metadata":{"id":"1MOYOJ6btliM"},"source":["## TensorBoard调参版本"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"k5P4kbedtliM","pycharm":{"name":"#%%\n"}},"outputs":[],"source":["import os\n","import itertools\n","import numpy as np\n","import tensorflow as tf\n","import keras\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Dropout, Flatten, LSTM\n","from tensorflow.keras.layers import Conv2D\n","from tensorflow.keras.layers import BatchNormalization, Activation, Reshape\n","from tensorflow.keras.layers import AveragePooling2D, Dropout\n","from tensorflow.keras.callbacks import ModelCheckpoint\n","import matplotlib.pyplot as plt\n","import pickle\n","import json\n","from tensorboard.plugins.hparams import api as hp\n","\n","HP_BATCH_SIZE = hp.HParam('batch_size', hp.Discrete([128, 256]))\n","HP_LEARNING_RATE = hp.HParam('learning_rate', hp.Discrete([0.01, 0.1, 1.0]))\n","HP_DECAY = hp.HParam('decay', hp.Discrete([0.01, 0.1, 1.0]))\n","HP_SHUFFLE = hp.HParam('shuffle', hp.Discrete([True, False]))\n","HP_USE_LAYER_7 = hp.HParam('use_layer_7', hp.Discrete([True, False]))\n","HP_USE_LAYER_8 = hp.HParam('use_layer_8', hp.Discrete([True, False]))\n","batch_sizes = HP_BATCH_SIZE.domain.values\n","learning_rates = HP_LEARNING_RATE.domain.values\n","decays = HP_DECAY.domain.values\n","shuffles = HP_SHUFFLE.domain.values\n","use_layer_7_options = HP_USE_LAYER_7.domain.values\n","use_layer_8_options = HP_USE_LAYER_8.domain.values\n","\n","epochs = 10\n","params = [HP_BATCH_SIZE, HP_LEARNING_RATE, HP_DECAY, HP_SHUFFLE, HP_USE_LAYER_7, HP_USE_LAYER_8]\n","\n","def create_model(hparams):\n","    use_layer_7 = hparams[HP_USE_LAYER_7]\n","    use_layer_8 = hparams[HP_USE_LAYER_8]\n","\n","    model = Sequential()\n","\n","    #layer1\n","    model.add(Conv2D(128, kernel_size=(n_ch, 1), padding=\"valid\", input_shape=input_shape))\n","    model.add(BatchNormalization())\n","    model.add(Activation('relu'))\n","    model.add(Reshape((model.output_shape[3],model.output_shape[2],model.output_shape[1])))\n","    # model.add(AveragePooling2D(pool_size=(1, 2)))\n","    # model.add(Dropout(0.1))\n","\n","    ## Layer 2\n","    model.add(Conv2D(256, kernel_size=(128, 1), padding=\"valid\"))\n","    model.add(BatchNormalization())\n","    model.add(Activation('relu'))\n","    model.add(Reshape((model.output_shape[3],model.output_shape[2],model.output_shape[1])))\n","    # model.add(AveragePooling2D(pool_size=(1, 2)))\n","    # model.add(Dropout(0.1))\n","\n","    ## Layer 3\n","    model.add(Conv2D(512, kernel_size=(256, 1), padding=\"valid\"))\n","    model.add(BatchNormalization())\n","    model.add(Activation('relu'))\n","    model.add(Reshape((model.output_shape[3],model.output_shape[2],model.output_shape[1])))\n","    # model.add(AveragePooling2D(pool_size=(1, 2)))\n","    # model.add(Dropout(0.1))\n","\n","    ## Layer 4\n","    model.add(Conv2D(1024, kernel_size=(512, 2), padding=\"valid\"))\n","    model.add(BatchNormalization())\n","    model.add(Activation('relu'))\n","    model.add(Reshape((model.output_shape[3],model.output_shape[2],model.output_shape[1])))\n","    # model.add(AveragePooling2D(pool_size=(4, 1)))\n","    model.add(Flatten())\n","\n","\n","    # layer 5\n","    model.add(Dense(192))\n","    model.add(Activation('relu'))\n","    # model.add(Dropout(0.4))\n","\n","\n","    model.add(Reshape((model.output_shape[1], 1)))\n","    # layer 6\n","    model.add(LSTM(192))\n","\n","    if use_layer_7:\n","        model.add(Reshape((model.output_shape[1], 1)))\n","        model.add(LSTM(192))\n","\n","    if use_layer_8:\n","        model.add(Dense(192))\n","\n","    # Add output layer\n","    # ...\n","\n","    # layer 9\n","    model.add(Dense(192))\n","\n","\n","    #output layer\n","    model.add(Dense(num_classes))\n","    model.add(Activation('softmax'))\n","\n","    model.summary()\n","    return model\n","    return model\n","\n","def run_experiment(hparams, logdir):\n","    model_name = f'model_LSTMCNN_epochs200_batch{hparams[HP_BATCH_SIZE]}_shuffle{hparams[HP_SHUFFLE]}_lr{hparams[HP_LEARNING_RATE]}_decay{hparams[HP_DECAY]}_layer7_{hparams[HP_USE_LAYER_7]}_layer8_{hparams[HP_USE_LAYER_8]}'\n","    os.makedirs(logdir, exist_ok=True)\n","\n","    checkpoint_path = os.path.join(logdir, f'{model_name}_checkpoint.h5')\n","    final_model_path = os.path.join(logdir, f'{model_name}_final.h5')\n","    acc_plot_path = os.path.join(logdir, f'{model_name}_accuracy.png')\n","    loss_plot_path = os.path.join(logdir, f'{model_name}_loss.png')\n","    history_data_path = os.path.join(logdir, f'{model_name}_history.json')\n","\n","    # Create model\n","    model = create_model(hparams)\n","\n","    # Compile the model\n","    model.compile(loss=keras.losses.categorical_crossentropy,\n","                  optimizer=tf.keras.optimizers.legacy.Adam(learning_rate=hparams[HP_LEARNING_RATE], decay=hparams[HP_DECAY]),\n","                  metrics=['accuracy', 'FalsePositives', 'FalseNegatives'])\n","\n","    # Set up callbacks\n","    tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=logdir, histogram_freq=1)\n","    hparams_callback = hp.KerasCallback(logdir, hparams)\n","    checkpoint_callback = ModelCheckpoint(\n","        filepath=checkpoint_path,\n","        save_weights_only=False,\n","        save_best_only=True,\n","        monitor='val_loss',\n","        verbose=1,\n","    )\n","\n","    # Check if the checkpoint file exists\n","    if os.path.exists(checkpoint_path):\n","        # Load the model from the checkpoint file\n","        model = tf.keras.models.load_model(checkpoint_path)\n","        print(f\"Loaded model from {checkpoint_path}\")\n","        initial_epoch = model.optimizer.iterations.numpy() // (len(x_train) // hparams[HP_BATCH_SIZE])\n","    else:\n","        print(f\"No checkpoint file found at {checkpoint_path}, training from scratch.\")\n","        initial_epoch = 0\n","\n","    # Train the model\n","    history = model.fit(x_train, y_train,\n","                        batch_size=hparams[HP_BATCH_SIZE],\n","                        epochs=epochs,\n","                        shuffle=hparams[HP_SHUFFLE],\n","                        verbose=1,\n","                        validation_split=0.2,\n","                        callbacks=[tensorboard_callback, hparams_callback, checkpoint_callback],\n","                        initial_epoch=initial_epoch)\n","\n","    # Save the final model\n","    model.save(final_model_path)\n","\n","    # Evaluate the model\n","    score_train = model.evaluate(x_train, y_train, verbose=0)\n","    score_test = model.evaluate(x_test, y_test, verbose=0)\n","    print(\"mean train accuracy is:   \", (score_train[1]))\n","    print(\"mean test accuracy is:   \", (score_test[1]))\n","    print(\"FAR is:   \", (score_train[2] + score_test[2]) / (np.size(y_train, 0) + np.size(y_test, 0)))\n","    print(\"FRR is:   \", (score_train[3] + score_test[3]) / (np.size(y_train, 0) + np.size(y_test, 0)))\n","\n","    # Plot and save accuracy\n","    plt.plot(history.history['accuracy'])\n","    plt.plot(history.history['val_accuracy'])\n","    plt.title('model accuracy')\n","    plt.ylabel('accuracy')\n","    plt.xlabel('epoch')\n","    plt.legend(['train', 'validation'], loc='upper left')\n","    plt.grid(True)\n","    plt.savefig(acc_plot_path, dpi=600)\n","    plt.show()\n","    # Plot and save loss\n","    plt.plot(history.history['loss'])\n","    plt.plot(history.history['val_loss'])\n","    plt.title('model loss')\n","    plt.ylabel('loss')\n","    plt.xlabel('epoch')\n","    plt.legend(['train', 'validation'], loc='upper left')\n","    plt.grid(True)\n","    plt.savefig(loss_plot_path, dpi=600)\n","    plt.show()\n","\n","    # Save history data to a JSON file\n","    with open(history_data_path, 'w') as history_file:\n","        json.dump(history.history, history_file)\n","\n","    return history\n","\n","\n","# Generate the combinations list\n","combinations = list(itertools.product(batch_sizes, learning_rates, decays, shuffles, use_layer_7_options, use_layer_8_options))\n","\n","try:\n","    start_index = 0\n","    experiment_state = {'completed_combinations': 0}\n","    experiment_state_path = 'experiment_state.pkl'\n","\n","    # Check if the experiment state file exists\n","    if os.path.exists(experiment_state_path):\n","        with open(experiment_state_path, 'rb') as f:\n","            experiment_state = pickle.load(f)\n","        start_index = experiment_state['completed_combinations']\n","\n","    # Iterate through combinations\n","    for idx, hparams_tuple in enumerate(combinations[start_index:], start_index):\n","        hparams = {param: value for param, value in zip(params, hparams_tuple)}\n","        logdir = f\"logs/hparam_tuning/model_LSTMCNN_epochs200_batch{hparams[HP_BATCH_SIZE]}_shuffle{hparams[HP_SHUFFLE]}_lr{hparams[HP_LEARNING_RATE]}_decay{hparams[HP_DECAY]}_layer7_{hparams[HP_USE_LAYER_7]}_layer8_{hparams[HP_USE_LAYER_8]}\"\n","        run_experiment(hparams, logdir)\n","\n","        # Update experiment state\n","        experiment_state['completed_combinations'] = idx + 1\n","        with open(experiment_state_path, 'wb') as f:\n","            pickle.dump(experiment_state, f)\n","\n","            # write bubble sort function\n","\n","\n","\n","\n","\n","\n","except KeyboardInterrupt:\n","    print(\"\\n\\n\\nInterrupted, saving experiment state...\")\n","    with open(experiment_state_path, 'wb') as f:\n","        pickle.dump(experiment_state, f)\n","    print(\"Experiment state saved. You can resume the experiment later.\")\n"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":["GmwMXz0UWCqy"],"provenance":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.4"},"toc":{"base_numbering":1,"nav_menu":{},"number_sections":true,"sideBar":true,"skip_h1_title":false,"title_cell":"Table of Contents","title_sidebar":"Contents","toc_cell":true,"toc_position":{"height":"calc(100% - 180px)","left":"10px","top":"150px","width":"284px"},"toc_section_display":true,"toc_window_display":true}},"nbformat":4,"nbformat_minor":0}
