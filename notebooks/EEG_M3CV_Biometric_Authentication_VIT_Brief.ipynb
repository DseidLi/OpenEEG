{"cells":[{"cell_type":"markdown","metadata":{"id":"JBJSj7jKwVgO"},"source":["# Dataset Preparation"]},{"cell_type":"markdown","metadata":{"id":"u8JIrXTlGurM"},"source":["### M3CV database"]},{"cell_type":"markdown","metadata":{"id":"H3RyCpq2Gurg"},"source":["#### 动态加载"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"u6SDMUJ1JYni","outputId":"cbed5829-93d8-41ed-d93f-dba70f1526c4"},"outputs":[{"name":"stdout","output_type":"stream","text":["Defaulting to user installation because normal site-packages is not writeable\n","Collecting opencv-python\n","  Downloading opencv_python-4.7.0.72-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (61.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.8/61.8 MB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>=1.17.0 in /home/u200810216/.local/lib/python3.7/site-packages (from opencv-python) (1.21.6)\n","Installing collected packages: opencv-python\n","Successfully installed opencv-python-4.7.0.72\n"]}],"source":["# !pip install librosa\n","# !pip install PyWavelets\n","!pip install opencv-python"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Fzm4bILwJYnk"},"outputs":[],"source":["!pip uninstall protobuf\n","!pip install protobuf==3.20.0\n"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"tmv08C-LJYnk","executionInfo":{"status":"ok","timestamp":1697991134603,"user_tz":-480,"elapsed":15,"user":{"displayName":"李墨","userId":"15382123608868165360"}}},"outputs":[],"source":["import os\n","os.chdir('D:\\EEG-ESRT\\eeg-biometric-competition')"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"FciInXxDGurh","outputId":"a92aadb4-9b02-4099-f0ae-a0f64c5a1c2e","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1697991147784,"user_tz":-480,"elapsed":6145,"user":{"displayName":"李墨","userId":"15382123608868165360"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["           EpochID subject  session  condition  usage\n","44959  epoch044960  sub074        1          7      1\n","26978  epoch026979  sub045        1          3      1\n","35644  epoch035645  sub059        1         10      1\n","36966  epoch036967  sub061        1          8      1\n","50637  epoch050638  sub084        1         12      1\n","['epoch044960' 'epoch026979' 'epoch035645' ... 'epoch042614' 'epoch043568'\n"," 'epoch002733'] ['sub074' 'sub045' 'sub059' ... 'sub070' 'sub072' 'sub005']\n"]}],"source":["import numpy as np\n","import mne\n","import tensorflow.keras\n","import tensorflow as tf\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Dropout, Flatten, LSTM\n","from tensorflow.keras.layers import Conv2D\n","from tensorflow.keras.layers import BatchNormalization, Activation, Reshape\n","import os\n","import numpy as np\n","import scipy.io as scio\n","import pandas as pd\n","from sklearn.utils import shuffle\n","from tensorflow.keras.utils import to_categorical\n","\n","\n","# 读取数据\n","train_images = pd.read_csv('Enrollment_Info.csv')\n","val_images = pd.read_csv('Calibration_Info.csv')\n","\n","train_images = shuffle(train_images, random_state=0)\n","val_images = shuffle(val_images)\n","\n","train_image_list = train_images\n","val_image_list = val_images\n","\n","df = train_image_list\n","train_img_list = train_image_list['EpochID'].values\n","print(train_image_list.head())\n","train_label_list = train_image_list['subject'].values\n","\n","\n","val_img_list = val_image_list['EpochID'].values\n","val_label_list = val_image_list['SubjectID'].values\n","print(train_img_list,train_label_list)"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PxFoaYwRJYnl","executionInfo":{"status":"ok","timestamp":1697991151659,"user_tz":-480,"elapsed":285,"user":{"displayName":"李墨","userId":"15382123608868165360"}},"outputId":"5c31bd13-c2e0-4085-850c-48afd7e555ad"},"outputs":[{"output_type":"stream","name":"stdout","text":["Original shape: (65, 1000)\n","Final data shape: (260, 250)\n"]}],"source":["import numpy as np\n","import pywt\n","\n","def wavelet_transform(x):\n","    coeffs = pywt.dwt(x, wavelet='haar')\n","    return coeffs\n","\n","# 创建一个示例 65x1000 的脑电信号数据\n","data = np.random.rand(65, 1000)\n","\n","# 对数据进行小波变换\n","coeffs_list = [wavelet_transform(channel_data) for channel_data in data]\n","\n","# 提取高频和低频系数\n","high_freq_coeffs = np.array([coeffs[0] for coeffs in coeffs_list])\n","low_freq_coeffs = np.array([coeffs[1] for coeffs in coeffs_list])\n","\n","# 再次进行小波变换\n","high_freq_coeffs_list = [wavelet_transform(channel_data) for channel_data in high_freq_coeffs]\n","low_freq_coeffs_list = [wavelet_transform(channel_data) for channel_data in low_freq_coeffs]\n","\n","# 提取二次小波变换后的高频和低频系数\n","high_freq_coeffs_1 = np.array([coeffs[0] for coeffs in high_freq_coeffs_list])\n","high_freq_coeffs_2 = np.array([coeffs[1] for coeffs in high_freq_coeffs_list])\n","low_freq_coeffs_1 = np.array([coeffs[0] for coeffs in low_freq_coeffs_list])\n","low_freq_coeffs_2 = np.array([coeffs[1] for coeffs in low_freq_coeffs_list])\n","\n","# 将四个 65x250 的系数堆叠在一起，得到 (4*65)x250 的矩阵\n","final_data = np.concatenate((high_freq_coeffs_1, high_freq_coeffs_2, low_freq_coeffs_1, low_freq_coeffs_2), axis=0)\n","\n","print(\"Original shape:\", data.shape)\n","print(\"Final data shape:\", final_data.shape)"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"1vNpdVp9JYnl","executionInfo":{"status":"error","timestamp":1697991172193,"user_tz":-480,"elapsed":999,"user":{"displayName":"李墨","userId":"15382123608868165360"}},"outputId":"b2f362c5-ccdb-40c3-b136-ee1d22d6be2e"},"outputs":[{"output_type":"error","ename":"ModuleNotFoundError","evalue":"ignored","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","Input \u001b[1;32mIn [4]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mlibrosa\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtransform\u001b[39m(data, n_mfcc\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m13\u001b[39m, hop_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m15\u001b[39m, n_fft\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m60\u001b[39m):\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;66;03m# 初始化一个空的 MFCC 矩阵\u001b[39;00m\n\u001b[0;32m      6\u001b[0m     mfcc_matrix \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mempty((\u001b[38;5;241m0\u001b[39m,))\n","File \u001b[1;32mC:\\Program Files\\Python310\\lib\\site-packages\\librosa\\__init__.py:209\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;66;03m# And all the librosa sub-modules\u001b[39;00m\n\u001b[0;32m    208\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_cache\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m cache\n\u001b[1;32m--> 209\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m core\n\u001b[0;32m    210\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m beat\n\u001b[0;32m    211\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m decompose\n","File \u001b[1;32mC:\\Program Files\\Python310\\lib\\site-packages\\librosa\\core\\__init__.py:5\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#!/usr/bin/env python\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# -*- coding: utf-8 -*-\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;03m\"\"\" Core IO and DSP functions\"\"\"\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconvert\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m  \u001b[38;5;66;03m# pylint: disable=wildcard-import\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01maudio\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m  \u001b[38;5;66;03m# pylint: disable=wildcard-import\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mspectrum\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m  \u001b[38;5;66;03m# pylint: disable=wildcard-import\u001b[39;00m\n","File \u001b[1;32mC:\\Program Files\\Python310\\lib\\site-packages\\librosa\\core\\convert.py:7\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mre\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m notation\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexceptions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ParameterError\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdecorators\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m deprecate_positional_args\n","File \u001b[1;32mC:\\Program Files\\Python310\\lib\\site-packages\\librosa\\core\\notation.py:8\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_cache\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m cache\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexceptions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ParameterError\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdecorators\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m deprecate_positional_args\n\u001b[0;32m     11\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkey_to_degrees\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkey_to_notes\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlist_thaat\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     19\u001b[0m ]\n","File \u001b[1;32mC:\\Program Files\\Python310\\lib\\site-packages\\librosa\\util\\__init__.py:77\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#!/usr/bin/env python\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# -*- coding: utf-8 -*-\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;124;03mUtilities\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;124;03m=========\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     74\u001b[0m \n\u001b[0;32m     75\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m---> 77\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m  \u001b[38;5;66;03m# pylint: disable=wildcard-import\u001b[39;00m\n\u001b[0;32m     78\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfiles\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m  \u001b[38;5;66;03m# pylint: disable=wildcard-import\u001b[39;00m\n\u001b[0;32m     79\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmatching\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m  \u001b[38;5;66;03m# pylint: disable=wildcard-import\u001b[39;00m\n","File \u001b[1;32mC:\\Program Files\\Python310\\lib\\site-packages\\librosa\\util\\utils.py:9\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msparse\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumba\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstride_tricks\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m as_strided\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_cache\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m cache\n","\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'numba'"]}],"source":["import numpy as np\n","import librosa\n","\n","def transform(data, n_mfcc=13, hop_length=15, n_fft=60):\n","    # 初始化一个空的 MFCC 矩阵\n","    mfcc_matrix = np.empty((0,))\n","\n","    # 遍历每个通道，并计算 MFCC\n","    for channel_data in data:\n","        channel_mfcc = librosa.feature.mfcc(y=channel_data, n_mfcc=n_mfcc, hop_length=hop_length, n_fft=n_fft)\n","        print(\"Channel MFCC shape:\", channel_mfcc.shape)\n","\n","        # 展平 channel_mfcc 并添加到 mfcc_matrix\n","        mfcc_matrix = np.hstack([mfcc_matrix, channel_mfcc.flatten()])\n","\n","    print(\"Flattened MFCC shape:\", mfcc_matrix.shape)\n","\n","    # 计算需要填充的元素数量以使元素总数可以被 224 整除\n","    padding_size = int(np.ceil(mfcc_matrix.size / 224) * 224) - mfcc_matrix.size\n","\n","    # 使用零填充 mfcc_matrix\n","    padded_mfcc_matrix = np.pad(mfcc_matrix, (0, padding_size), mode='constant', constant_values=0)\n","    print(\"Padded MFCC shape:\", padded_mfcc_matrix.shape)\n","\n","    # 将填充后的 mfcc_matrix 转换为形状为 (-1, 224) 的矩阵\n","    reshaped_data = np.reshape(padded_mfcc_matrix, (-1, 224))\n","\n","    return reshaped_data\n","\n","# 创建一个示例 65x1000 的脑电信号数据\n","data = np.random.rand(65, 1000)\n","\n","# 将数据转换为 MFCC 矩阵\n","transformed_data = transform(data)\n","\n","print(\"Original shape:\", data.shape)\n","print(\"Transformed shape:\", transformed_data.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ej5Zxo_3JYnm"},"outputs":[],"source":["import numpy as np\n","import pywt\n","import librosa\n","import cv2\n","\n","def wavelet_transform(x):\n","    coeffs = pywt.dwt(x, wavelet='haar')\n","    return coeffs\n","\n","def process_coeffs(coeffs, target_shape=(224, 224)):\n","    resized_data = cv2.resize(coeffs, target_shape)\n","    return resized_data\n","\n","def transform_mfcc(data, n_mfcc=13, hop_length=15, n_fft=60):\n","    # 初始化一个空的 MFCC 矩阵\n","    mfcc_matrix = np.empty((0,))\n","\n","    # 遍历每个通道，并计算 MFCC\n","    for channel_data in data:\n","        channel_mfcc = librosa.feature.mfcc(y=channel_data, n_mfcc=n_mfcc, hop_length=hop_length, n_fft=n_fft)\n","\n","        # 展平 channel_mfcc 并添加到 mfcc_matrix\n","        mfcc_matrix = np.hstack([mfcc_matrix, channel_mfcc.flatten()])\n","\n","    # 计算需要填充的元素数量以使元素总数可以被 224 整除\n","    padding_size = int(np.ceil(mfcc_matrix.size / 224) * 224) - mfcc_matrix.size\n","\n","    # 使用零填充 mfcc_matrix\n","    padded_mfcc_matrix = np.pad(mfcc_matrix, (0, padding_size), mode='constant', constant_values=0)\n","\n","    # 将填充后的 mfcc_matrix 转换为形状为 (-1, 224) 的矩阵\n","    reshaped_data = np.reshape(padded_mfcc_matrix, (-1, 224))\n","    return reshaped_data\n","\n","\n","\n","\n","def transform(data):\n","    # 对数据进行小波变换\n","    coeffs_list = [wavelet_transform(channel_data) for channel_data in data]\n","\n","    # 提取高频和低频系数\n","    high_freq_coeffs = np.array([coeffs[0] for coeffs in coeffs_list])\n","    low_freq_coeffs = np.array([coeffs[1] for coeffs in coeffs_list])\n","\n","    # 再次进行小波变换\n","    high_freq_coeffs_list = [wavelet_transform(channel_data) for channel_data in high_freq_coeffs]\n","    low_freq_coeffs_list = [wavelet_transform(channel_data) for channel_data in low_freq_coeffs]\n","\n","    # 提取二次小波变换后的高频和低频系数\n","    high_freq_coeffs_1 = np.array([coeffs[0] for coeffs in high_freq_coeffs_list])\n","    high_freq_coeffs_2 = np.array([coeffs[1] for coeffs in high_freq_coeffs_list])\n","    low_freq_coeffs_1 = np.array([coeffs[0] for coeffs in low_freq_coeffs_list])\n","    low_freq_coeffs_2 = np.array([coeffs[1] for coeffs in low_freq_coeffs_list])\n","\n","    # 将两个高频系数矩阵连接在一起，形成130x250的矩阵\n","    high_freq_matrix = np.concatenate((high_freq_coeffs_1, high_freq_coeffs_2), axis=0)\n","\n","    # 将两个低频系数矩阵连接在一起，形成130x250的矩阵\n","    low_freq_matrix = np.concatenate((low_freq_coeffs_1, low_freq_coeffs_2), axis=0)\n","\n","    # 将130x250的高频和低频矩阵调整为224x224的图像\n","    high_freq_image = process_coeffs(high_freq_matrix)\n","    low_freq_image = process_coeffs(low_freq_matrix)\n","\n","    # 计算 MFCC\n","    mfcc_data = transform_mfcc(data)\n","\n","    # 将 MFCC 数据处理为224x224的图像\n","    mfcc_image = process_coeffs(mfcc_data)\n","\n","    # 将三层图像堆叠在一起得到 224x224x3 的图像\n","    final_image = np.stack([high_freq_image, low_freq_image, mfcc_image], axis=2)\n","\n","    return final_image"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XNsiats6Gurh"},"outputs":[],"source":["import os\n","import tensorflow as tf\n","import pandas as pd\n","import numpy as np\n","import scipy.io as scio\n","from tensorflow.data import Dataset\n","from tqdm import tqdm\n","\n","\n","class MyDataset:\n","    def __init__(self, train_img_list, val_img_list, train_label_list, val_label_list, mode='train', n_samples=10):\n","        self.mode = mode\n","        self.train_images = train_img_list[:n_samples]\n","        self.test_images = val_img_list[:n_samples]\n","        self.train_label = train_label_list[:n_samples]\n","        self.test_label = val_label_list[:n_samples]\n","\n","    def load_eeg(self, eeg_path):\n","        data = scio.loadmat(eeg_path)\n","        return data['epoch_data']\n","\n","    def __len__(self):\n","        if self.mode == 'train':\n","            return len(self.train_images)\n","        else:\n","            return len(self.test_images)\n","    def get_data_generator(self):\n","        if self.mode == 'train':\n","            image_list = self.train_images\n","            label_list = self.train_label\n","        else:\n","            image_list = self.test_images\n","            label_list = self.test_label\n","        for img, la in zip(image_list, label_list):\n","            if self.mode == 'train':\n","                eeg_path = os.path.join('/home/u200810216/jupyter/kaggle_m3cv/train/', img + '.mat')\n","            else:\n","                eeg_path = os.path.join('/home/u200810216/jupyter/kaggle_m3cv/val/', img + '.mat')\n","\n","            data = self.load_eeg(eeg_path)\n","\n","            data = transform(data)\n","\n","\n","#             data = data[..., np.newaxis]\n","            label = to_categorical((int(la[4:])-1), 95)\n","            yield data, label\n","\n","n_samples= None\n","# 创建MyDataset实例\n","train_dataset = MyDataset(train_img_list, val_img_list, train_label_list, val_label_list, mode='train', n_samples=n_samples)\n","val_dataset = MyDataset(train_img_list, val_img_list, train_label_list, val_label_list, mode='test', n_samples=n_samples)\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_Le1lYbWJYno","outputId":"c4f607bc-9ceb-4453-a830-64fc6fe48781"},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|██████████| 57851/57851 [2:12:19<00:00,  7.29it/s]  \n","100%|██████████| 12168/12168 [27:11<00:00,  7.46it/s]\n"]}],"source":["import os\n","import numpy as np\n","import scipy.io as scio\n","\n","def load_eeg(eeg_path):\n","    data = scio.loadmat(eeg_path)\n","    return data['epoch_data']\n","\n","def transform_and_save_data(images_list, images_dir, save_dir):\n","    for img in tqdm(images_list):\n","        eeg_path = os.path.join(images_dir, img + '.mat')\n","        data = load_eeg(eeg_path)\n","\n","        data = transform(data)  # 假设你有一个transform函数来进行数据预处理\n","\n","        np.save(os.path.join(save_dir, img), data)  # 保存预处理过的数据为.npy文件\n","\n","# 为训练集和验证集进行预处理\n","train_images_dir = '/home/u200810216/jupyter/kaggle_m3cv/train/'\n","val_images_dir = '/home/u200810216/jupyter/kaggle_m3cv/val/'\n","\n","save_train_dir = '/home/u200810216/jupyter/kaggle_m3cv/train_npy/'\n","save_val_dir = '/home/u200810216/jupyter/kaggle_m3cv/val_npy/'\n","\n","# 创建保存的文件夹\n","os.makedirs(save_train_dir, exist_ok=True)\n","os.makedirs(save_val_dir, exist_ok=True)\n","\n","# 执行预处理并保存\n","transform_and_save_data(train_img_list, train_images_dir, save_train_dir)\n","transform_and_save_data(val_img_list, val_images_dir, save_val_dir)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sLpcnNlnJYnp"},"outputs":[],"source":["from tensorflow.keras.utils import to_categorical\n","from tensorflow.data import Dataset\n","\n","class MyDataset:\n","    def __init__(self, train_img_list, val_img_list, train_label_list, val_label_list, mode='train', n_samples=10):\n","        self.mode = mode\n","        self.train_images = train_img_list[:n_samples]\n","        self.test_images = val_img_list[:n_samples]\n","        self.train_label = train_label_list[:n_samples]\n","        self.test_label = val_label_list[:n_samples]\n","\n","    def load_data(self, data_path):\n","        data = np.load(data_path)\n","        return data\n","\n","    def __len__(self):\n","        if self.mode == 'train':\n","            return len(self.train_images)\n","        else:\n","            return len(self.test_images)\n","\n","    def get_data_generator(self):\n","        if self.mode == 'train':\n","            image_list = self.train_images\n","            label_list = self.train_label\n","            data_dir = '/home/u200810216/jupyter/kaggle_m3cv/train_npy/'\n","        else:\n","            image_list = self.test_images\n","            label_list = self.test_label\n","            data_dir = '/home/u200810216/jupyter/kaggle_m3cv/val_npy/'\n","\n","        for img, la in zip(image_list, label_list):\n","            data_path = os.path.join(data_dir, img + '.npy')\n","            data = self.load_data(data_path)\n","\n","            label = to_categorical((int(la[4:])-1), 95)\n","            yield data, label\n","\n","n_samples= None\n","# 创建MyDataset实例\n","train_dataset = MyDataset(train_img_list, val_img_list, train_label_list, val_label_list, mode='train', n_samples=n_samples)\n","val_dataset = MyDataset(train_img_list, val_img_list, train_label_list, val_label_list, mode='test', n_samples=n_samples)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DCfOXDjBGurh","outputId":"0a19f323-7f75-46d7-f084-b11002eb9744"},"outputs":[{"name":"stderr","output_type":"stream","text":["2023-06-03 01:46:53.435907: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n","To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","2023-06-03 01:46:56.054392: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 38400 MB memory:  -> device: 0, name: A100-SXM4-40GB, pci bus id: 0000:51:00.0, compute capability: 8.0\n"]}],"source":["# train_data = Dataset.from_generator(\n","#     train_dataset.get_data_generator,\n","#     output_signature=(\n","#         tf.TensorSpec(shape=(65, 1000,1), dtype=tf.float32),\n","#         tf.TensorSpec(shape=(95), dtype=tf.int32)\n","#     )\n","# )\n","# val_data = Dataset.from_generator(\n","#     val_dataset.get_data_generator,\n","#     output_signature=(\n","#         tf.TensorSpec(shape=(65, 1000,1), dtype=tf.float32),\n","#         tf.TensorSpec(shape=(95), dtype=tf.int32)\n","#     )\n","# )\n","train_data = Dataset.from_generator(\n","    train_dataset.get_data_generator,\n","    output_signature=(\n","        tf.TensorSpec(shape=(224, 224, 3), dtype=tf.float32),\n","        tf.TensorSpec(shape=(95), dtype=tf.int32)\n","    )\n",")\n","val_data = Dataset.from_generator(\n","    val_dataset.get_data_generator,\n","    output_signature=(\n","        tf.TensorSpec(shape=(224, 224, 3), dtype=tf.float32),\n","        tf.TensorSpec(shape=(95), dtype=tf.int32)\n","    )\n",")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-TIAN_hcGurh","outputId":"d8d25298-6556-490f-9321-5e08dcfa8b15"},"outputs":[{"data":{"text/plain":["<BatchDataset element_spec=(TensorSpec(shape=(None, 224, 224, 3), dtype=tf.float32, name=None), TensorSpec(shape=(None, 95), dtype=tf.int32, name=None))>"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["# train_data = train_data.shuffle(buffer_size=len(train_dataset)).batch(256)\n","train_data = train_data.batch(128)\n","val_data = val_data.batch(128)\n","val_data"]},{"cell_type":"markdown","metadata":{"id":"6H6rPoyqUu3p"},"source":["# Network Architecture"]},{"cell_type":"markdown","metadata":{"id":"bafzy43fGurr"},"source":["## Bert-Vision Transfomer"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qx51-b6IGurr"},"outputs":[],"source":["import os\n","import tensorflow as tf\n","from transformers import TFViTModel, ViTConfig\n","from tensorflow.keras.callbacks import ModelCheckpoint\n","\n","# 加载预先训练好的 ViT 模型的配置\n","config = ViTConfig.from_pretrained(\"google/vit-base-patch16-224-in21k\", num_labels=95)\n","\n","# 修改配置，使其适用于单通道图像分类任务\n","config.input_channels = 3\n","config.hidden_dropout_prob = 0.0\n","config.attention_probs_dropout_prob = 0.0"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"C4eCkJtEGurr"},"outputs":[],"source":["from transformers import create_optimizer\n","from tensorflow.keras.losses import SparseCategoricalCrossentropy\n","from tensorflow.keras.models import load_model\n","from tensorflow.keras.callbacks import ModelCheckpoint\n","import tensorflow as tf"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qUP15uriGurs","outputId":"0b154484-ec17-468c-ceb2-8e963150b1ab"},"outputs":[{"name":"stderr","output_type":"stream","text":["2023-06-03 01:47:50.223109: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:428] Loaded cuDNN version 8100\n","2023-06-03 01:47:51.052279: I tensorflow/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n","2023-06-03 01:47:57.803909: I tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:630] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n","All model checkpoint layers were used when initializing TFViTModel.\n","\n","All the layers of TFViTModel were initialized from the model checkpoint at google/vit-base-patch16-224-in21k.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use TFViTModel for predictions without further training.\n"]}],"source":["batch_size = 16\n","epochs = 5\n","num_train_steps = len(train_img_list) * epochs\n","learning_rate = 3e-5\n","weight_decay_rate = 0.01\n","\n","vit = TFViTModel.from_pretrained(\"google/vit-base-patch16-224-in21k\", config=config)\n","\n","# # 使用单通道图像进行分类\n","# inputs = tf.keras.layers.Input(shape=(224, 224, 3))# 这里等待修改\n","# x = tf.image.resize(inputs, (224, 224))\n","# x = tf.keras.layers.Concatenate(axis=-1)([x, x, x])\n","# x = tf.keras.layers.Lambda(lambda x: tf.transpose(x, [0, 3, 1, 2]))(x)\n","# outputs = vit(x).last_hidden_state[:, 0, :]\n","\n","\n","# 使用单通道图像进行分类\n","inputs = tf.keras.layers.Input(shape=(224, 224, 3))# 这里等待修改\n","x = tf.keras.layers.Lambda(lambda x: tf.transpose(x, [0, 3, 1, 2]))(inputs)\n","outputs = vit(x).last_hidden_state[:, 0, :]\n","\n","# 添加输出层\n","outputs = tf.keras.layers.Dense(95, activation='softmax')(outputs)\n","# outputs = tf.keras.layers.Lambda(lambda x: tf.argmax(x, axis=1))(outputs)\n","\n","model = tf.keras.models.Model(inputs=inputs, outputs=outputs)\n","optimizer, lr_schedule = create_optimizer(\n","    init_lr=learning_rate,\n","    num_train_steps=num_train_steps,\n","    weight_decay_rate=weight_decay_rate,\n","    num_warmup_steps=1,\n",")\n","loss = tf.keras.losses.CategoricalCrossentropy(from_logits=False)\n","# loss = tf.keras.losses.SparseCategoricalCrossentropy()\n","\n","\n","model.compile(loss=loss,\n","              optimizer=optimizer,\n","              metrics=['accuracy',\n","                       'FalsePositives',\n","                       'FalseNegatives'])\n","\n","\n","# 记录日志\n","log_dir = \"logs/transfomer\"\n","tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n","# 定义一个回调函数来保存模型和历史记录\n","checkpoint_callback = ModelCheckpoint(\n","    filepath='ViT-3-layer.h5',\n","    save_weights_only=True,\n","    save_best_only=False,\n","    monitor='val_loss',\n","    verbose=1,\n",")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xMXzc2fPGurt"},"outputs":[],"source":["len(train_img_list)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_lPm1wHbGurt","outputId":"3748d862-0b15-43ed-f157-d19206cafbd0"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/5\n","     96/Unknown - 328s 3s/step - loss: 1.1199 - accuracy: 0.7219 - false_positives: 1776.0000 - false_negatives: 4325.0000\n","Epoch 1: saving model to ViT-3-layer.h5\n","96/96 [==============================] - 356s 4s/step - loss: 1.1199 - accuracy: 0.7219 - false_positives: 1776.0000 - false_negatives: 4325.0000 - val_loss: 0.3185 - val_accuracy: 0.9053 - val_false_positives: 663.0000 - val_false_negatives: 1586.0000\n","Epoch 2/5\n","96/96 [==============================] - ETA: 0s - loss: 0.2464 - accuracy: 0.9280 - false_positives: 577.0000 - false_negatives: 1143.0000\n","Epoch 2: saving model to ViT-3-layer.h5\n","96/96 [==============================] - 69s 714ms/step - loss: 0.2464 - accuracy: 0.9280 - false_positives: 577.0000 - false_negatives: 1143.0000 - val_loss: 0.1327 - val_accuracy: 0.9652 - val_false_positives: 293.0000 - val_false_negatives: 581.0000\n","Epoch 3/5\n","96/96 [==============================] - ETA: 0s - loss: 0.1036 - accuracy: 0.9726 - false_positives: 229.0000 - false_negatives: 422.0000\n","Epoch 3: saving model to ViT-3-layer.h5\n","96/96 [==============================] - 68s 714ms/step - loss: 0.1036 - accuracy: 0.9726 - false_positives: 229.0000 - false_negatives: 422.0000 - val_loss: 0.0849 - val_accuracy: 0.9772 - val_false_positives: 195.0000 - val_false_negatives: 372.0000\n","Epoch 4/5\n","96/96 [==============================] - ETA: 0s - loss: 0.0593 - accuracy: 0.9851 - false_positives: 126.0000 - false_negatives: 225.0000\n","Epoch 4: saving model to ViT-3-layer.h5\n","96/96 [==============================] - 69s 715ms/step - loss: 0.0593 - accuracy: 0.9851 - false_positives: 126.0000 - false_negatives: 225.0000 - val_loss: 0.0324 - val_accuracy: 0.9925 - val_false_positives: 76.0000 - val_false_negatives: 114.0000\n","Epoch 5/5\n","96/96 [==============================] - ETA: 0s - loss: 0.0363 - accuracy: 0.9918 - false_positives: 82.0000 - false_negatives: 124.0000\n","Epoch 5: saving model to ViT-3-layer.h5\n","96/96 [==============================] - 69s 716ms/step - loss: 0.0363 - accuracy: 0.9918 - false_positives: 82.0000 - false_negatives: 124.0000 - val_loss: 0.0252 - val_accuracy: 0.9938 - val_false_positives: 66.0000 - val_false_negatives: 89.0000\n"]}],"source":["# 训练模型\n","model.load_weights('/home/u200810216/jupyter/kaggle_m3cv/ViT-3-layer.h5')\n","history = model.fit(val_data,\n","                    epochs=5,\n","                    verbose=1,\n","                    validation_data=val_data,\n","                    callbacks=[tensorboard_callback, checkpoint_callback])\n","# 保存模型权重而不是整个模型\n","model.save_weights('/home/u200810216/jupyter/kaggle_m3cv/ViT-3layer-val.h5')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WwS5DBzmJYns","outputId":"9fb1dc4e-c891-4edb-b230-9660481d50b2"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/25\n","    451/Unknown - 956s 2s/step - loss: 1.1704e-04 - accuracy: 1.0000 - false_positives: 0.0000e+00 - false_negatives: 0.0000e+00"]}],"source":["# 训练模型\n","history = model.fit(train_data,\n","                    epochs=25,\n","                    verbose=1,\n","                    validation_data=train_data,\n","                    callbacks=[tensorboard_callback, checkpoint_callback])\n","# 保存模型权重而不是整个模型\n","model.save_weights('/home/u200810216/jupyter/kaggle_m3cv/ViT-3layer-epoch62.h5')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0xxat8d8JYns"},"outputs":[],"source":["model.save_weights('/home/u200810216/jupyter/kaggle_m3cv/ViT-3layer-epoch12.h5')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-TKS6NfWJYnt","outputId":"d59cca5f-03da-44e9-d35f-97efa7406ddf"},"outputs":[{"name":"stdout","output_type":"stream","text":["     96/Unknown - 221s 2s/step - loss: 0.8837 - accuracy: 0.7643 - false_positives: 1116.0000 - false_negatives: 4220.0000\n","Epoch 1: saving model to ViT-3-layer.h5\n","96/96 [==============================] - 248s 3s/step - loss: 0.8837 - accuracy: 0.7643 - false_positives: 1116.0000 - false_negatives: 4220.0000 - val_loss: 0.3378 - val_accuracy: 0.9012 - val_false_positives: 686.0000 - val_false_negatives: 1650.0000\n"]}],"source":["\n","history = model.fit(val_data,\n","                    epochs=1,\n","                    verbose=1,\n","                    validation_data=val_data,\n","                    callbacks=[tensorboard_callback, checkpoint_callback])\n","model.save_weights('/home/u200810216/jupyter/kaggle_m3cv/ViT-3layer-epoch12-val1.h5')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"L36dF7FJGuru","outputId":"b846fede-c8c1-4a91-a0c0-e0e95ae1fe44"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/3\n","    452/Unknown - 1503s 3s/step - loss: 0.0604 - accuracy: 0.9852 - false_positives: 614.0000 - false_negatives: 1015.0000\n","Epoch 1: saving model to transfomer.h5\n","452/452 [==============================] - 1755s 4s/step - loss: 0.0604 - accuracy: 0.9852 - false_positives: 614.0000 - false_negatives: 1015.0000 - val_loss: 2.9653 - val_accuracy: 0.4057 - val_false_positives: 5381.0000 - val_false_negatives: 7663.0000\n","Epoch 2/3\n","452/452 [==============================] - ETA: 0s - loss: 0.0021 - accuracy: 1.0000 - false_positives: 2.0000 - false_negatives: 2.0000\n","Epoch 2: saving model to transfomer.h5\n","452/452 [==============================] - 1017s 2s/step - loss: 0.0021 - accuracy: 1.0000 - false_positives: 2.0000 - false_negatives: 2.0000 - val_loss: 3.0420 - val_accuracy: 0.4031 - val_false_positives: 5474.0000 - val_false_negatives: 7679.0000\n","Epoch 3/3\n","452/452 [==============================] - ETA: 0s - loss: 0.0011 - accuracy: 1.0000 - false_positives: 0.0000e+00 - false_negatives: 0.0000e+00\n","Epoch 3: saving model to transfomer.h5\n","452/452 [==============================] - 748s 2s/step - loss: 0.0011 - accuracy: 1.0000 - false_positives: 0.0000e+00 - false_negatives: 0.0000e+00 - val_loss: 3.1344 - val_accuracy: 0.3949 - val_false_positives: 5617.0000 - val_false_negatives: 7744.0000\n"]}],"source":["model.load_weights('/home/u200810216/jupyter/kaggle_m3cv/vision_transfomer_test_kaggle_batch128_weights_val_3.h5')\n","history = model.fit(train_data,\n","                    epochs=3,\n","                    verbose=1,\n","                    validation_data=val_data,\n","                    callbacks=[tensorboard_callback, checkpoint_callback])\n","\n","# 保存模型\n","model.save_weights('/home/u200810216/jupyter/kaggle_m3cv/vision_transfomer_test_kaggle_batch128_weights_val_3_train_3.h5')  # 保存为 .h5 文件格式\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bwGEpAQ_Guru","outputId":"bf201983-1f48-4c10-d95f-a1b6c76944af"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/64\n","Training on train_data\n","    452/Unknown - 1362s 3s/step - loss: 0.0280 - accuracy: 0.9936 - false_positives: 261.0000 - false_negatives: 500.0000\n","Epoch 1: saving model to transfomer.h5\n","452/452 [==============================] - 1369s 3s/step - loss: 0.0280 - accuracy: 0.9936 - false_positives: 261.0000 - false_negatives: 500.0000\n","Epoch 2/64\n","Training on val_data\n","     96/Unknown - 185s 2s/step - loss: 0.1188 - accuracy: 0.9651 - false_positives: 297.0000 - false_negatives: 555.0000\n","Epoch 1: saving model to transfomer.h5\n","96/96 [==============================] - 191s 2s/step - loss: 0.1188 - accuracy: 0.9651 - false_positives: 297.0000 - false_negatives: 555.0000\n","Epoch 3/64\n","Training on train_data\n","    452/Unknown - 716s 2s/step - loss: 0.0218 - accuracy: 0.9956 - false_positives: 191.0000 - false_negatives: 344.0000\n","Epoch 1: saving model to transfomer.h5\n","452/452 [==============================] - 722s 2s/step - loss: 0.0218 - accuracy: 0.9956 - false_positives: 191.0000 - false_negatives: 344.0000\n","Epoch 4/64\n","Training on val_data\n","     96/Unknown - 149s 2s/step - loss: 0.0775 - accuracy: 0.9784 - false_positives: 206.0000 - false_negatives: 339.0000\n","Epoch 1: saving model to transfomer.h5\n","96/96 [==============================] - 154s 2s/step - loss: 0.0775 - accuracy: 0.9784 - false_positives: 206.0000 - false_negatives: 339.0000\n","Epoch 5/64\n","Training on train_data\n","    452/Unknown - 854s 2s/step - loss: 0.0163 - accuracy: 0.9969 - false_positives: 135.0000 - false_negatives: 237.0000\n","Epoch 1: saving model to transfomer.h5\n","452/452 [==============================] - 859s 2s/step - loss: 0.0163 - accuracy: 0.9969 - false_positives: 135.0000 - false_negatives: 237.0000\n","Epoch 6/64\n","Training on val_data\n","     96/Unknown - 252s 3s/step - loss: 0.0587 - accuracy: 0.9835 - false_positives: 168.0000 - false_negatives: 248.0000\n","Epoch 1: saving model to transfomer.h5\n","96/96 [==============================] - 257s 3s/step - loss: 0.0587 - accuracy: 0.9835 - false_positives: 168.0000 - false_negatives: 248.0000\n","Epoch 7/64\n","Training on train_data\n","    452/Unknown - 869s 2s/step - loss: 0.0129 - accuracy: 0.9978 - false_positives: 91.0000 - false_negatives: 183.0000\n","Epoch 1: saving model to transfomer.h5\n","452/452 [==============================] - 875s 2s/step - loss: 0.0129 - accuracy: 0.9978 - false_positives: 91.0000 - false_negatives: 183.0000\n","Epoch 8/64\n","Training on val_data\n","     96/Unknown - 182s 2s/step - loss: 0.0413 - accuracy: 0.9892 - false_positives: 104.0000 - false_negatives: 171.0000\n","Epoch 1: saving model to transfomer.h5\n","96/96 [==============================] - 187s 2s/step - loss: 0.0413 - accuracy: 0.9892 - false_positives: 104.0000 - false_negatives: 171.0000\n","Epoch 9/64\n","Training on train_data\n","    452/Unknown - 715s 2s/step - loss: 0.0336 - accuracy: 0.9916 - false_positives: 322.0000 - false_negatives: 622.0000\n","Epoch 1: saving model to transfomer.h5\n","452/452 [==============================] - 721s 2s/step - loss: 0.0336 - accuracy: 0.9916 - false_positives: 322.0000 - false_negatives: 622.0000\n","Epoch 10/64\n","Training on val_data\n","     96/Unknown - 141s 1s/step - loss: 0.0358 - accuracy: 0.9917 - false_positives: 77.0000 - false_negatives: 132.0000\n","Epoch 1: saving model to transfomer.h5\n","96/96 [==============================] - 146s 2s/step - loss: 0.0358 - accuracy: 0.9917 - false_positives: 77.0000 - false_negatives: 132.0000\n","Epoch 11/64\n","Training on train_data\n","    452/Unknown - 585s 1s/step - loss: 0.0098 - accuracy: 0.9980 - false_positives: 95.0000 - false_negatives: 142.0000\n","Epoch 1: saving model to transfomer.h5\n","452/452 [==============================] - 591s 1s/step - loss: 0.0098 - accuracy: 0.9980 - false_positives: 95.0000 - false_negatives: 142.0000\n","Epoch 12/64\n","Training on val_data\n","     96/Unknown - 114s 1s/step - loss: 0.0228 - accuracy: 0.9951 - false_positives: 45.0000 - false_negatives: 71.0000\n","Epoch 1: saving model to transfomer.h5\n","96/96 [==============================] - 119s 1s/step - loss: 0.0228 - accuracy: 0.9951 - false_positives: 45.0000 - false_negatives: 71.0000\n","Epoch 13/64\n","Training on train_data\n","    452/Unknown - 553s 1s/step - loss: 0.0172 - accuracy: 0.9957 - false_positives: 212.0000 - false_negatives: 289.0000\n","Epoch 1: saving model to transfomer.h5\n","452/452 [==============================] - 558s 1s/step - loss: 0.0172 - accuracy: 0.9957 - false_positives: 212.0000 - false_negatives: 289.0000\n","Epoch 14/64\n","Training on val_data\n","     96/Unknown - 115s 1s/step - loss: 0.0225 - accuracy: 0.9952 - false_positives: 48.0000 - false_negatives: 78.0000\n","Epoch 1: saving model to transfomer.h5\n","96/96 [==============================] - 120s 1s/step - loss: 0.0225 - accuracy: 0.9952 - false_positives: 48.0000 - false_negatives: 78.0000\n","Epoch 15/64\n","Training on train_data\n","    452/Unknown - 557s 1s/step - loss: 0.0133 - accuracy: 0.9969 - false_positives: 156.0000 - false_negatives: 215.0000\n","Epoch 1: saving model to transfomer.h5\n","452/452 [==============================] - 563s 1s/step - loss: 0.0133 - accuracy: 0.9969 - false_positives: 156.0000 - false_negatives: 215.0000\n","Epoch 16/64\n","Training on val_data\n","     96/Unknown - 114s 1s/step - loss: 0.0225 - accuracy: 0.9945 - false_positives: 53.0000 - false_negatives: 78.0000\n","Epoch 1: saving model to transfomer.h5\n","96/96 [==============================] - 120s 1s/step - loss: 0.0225 - accuracy: 0.9945 - false_positives: 53.0000 - false_negatives: 78.0000\n","Epoch 17/64\n","Training on train_data\n","    452/Unknown - 759s 2s/step - loss: 0.0090 - accuracy: 0.9982 - false_positives: 86.0000 - false_negatives: 128.0000\n","Epoch 1: saving model to transfomer.h5\n","452/452 [==============================] - 765s 2s/step - loss: 0.0090 - accuracy: 0.9982 - false_positives: 86.0000 - false_negatives: 128.0000\n","Epoch 18/64\n","Training on val_data\n","     96/Unknown - 235s 2s/step - loss: 0.0195 - accuracy: 0.9962 - false_positives: 38.0000 - false_negatives: 66.0000\n","Epoch 1: saving model to transfomer.h5\n","96/96 [==============================] - 240s 2s/step - loss: 0.0195 - accuracy: 0.9962 - false_positives: 38.0000 - false_negatives: 66.0000\n","Epoch 19/64\n","Training on train_data\n","    452/Unknown - 884s 2s/step - loss: 0.0182 - accuracy: 0.9953 - false_positives: 221.0000 - false_negatives: 323.0000\n","Epoch 1: saving model to transfomer.h5\n","452/452 [==============================] - 890s 2s/step - loss: 0.0182 - accuracy: 0.9953 - false_positives: 221.0000 - false_negatives: 323.0000\n","Epoch 20/64\n","Training on val_data\n","     96/Unknown - 141s 1s/step - loss: 0.0288 - accuracy: 0.9923 - false_positives: 76.0000 - false_negatives: 113.0000\n","Epoch 1: saving model to transfomer.h5\n","96/96 [==============================] - 146s 2s/step - loss: 0.0288 - accuracy: 0.9923 - false_positives: 76.0000 - false_negatives: 113.0000\n","Epoch 21/64\n","Training on train_data\n","    452/Unknown - 712s 2s/step - loss: 0.0142 - accuracy: 0.9964 - false_positives: 174.0000 - false_negatives: 247.0000\n","Epoch 1: saving model to transfomer.h5\n","452/452 [==============================] - 717s 2s/step - loss: 0.0142 - accuracy: 0.9964 - false_positives: 174.0000 - false_negatives: 247.0000\n","Epoch 22/64\n","Training on val_data\n","     96/Unknown - 142s 1s/step - loss: 0.0169 - accuracy: 0.9963 - false_positives: 38.0000 - false_negatives: 53.0000\n","Epoch 1: saving model to transfomer.h5\n","96/96 [==============================] - 147s 2s/step - loss: 0.0169 - accuracy: 0.9963 - false_positives: 38.0000 - false_negatives: 53.0000\n","Epoch 23/64\n","Training on train_data\n","    452/Unknown - 817s 2s/step - loss: 0.0146 - accuracy: 0.9961 - false_positives: 188.0000 - false_negatives: 259.0000\n","Epoch 1: saving model to transfomer.h5\n","452/452 [==============================] - 823s 2s/step - loss: 0.0146 - accuracy: 0.9961 - false_positives: 188.0000 - false_negatives: 259.0000\n","Epoch 24/64\n","Training on val_data\n","     96/Unknown - 148s 2s/step - loss: 0.0197 - accuracy: 0.9952 - false_positives: 46.0000 - false_negatives: 74.0000\n","Epoch 1: saving model to transfomer.h5\n","96/96 [==============================] - 153s 2s/step - loss: 0.0197 - accuracy: 0.9952 - false_positives: 46.0000 - false_negatives: 74.0000\n","Epoch 25/64\n","Training on train_data\n","    452/Unknown - 825s 2s/step - loss: 0.0136 - accuracy: 0.9964 - false_positives: 181.0000 - false_negatives: 234.0000\n","Epoch 1: saving model to transfomer.h5\n","452/452 [==============================] - 830s 2s/step - loss: 0.0136 - accuracy: 0.9964 - false_positives: 181.0000 - false_negatives: 234.0000\n","Epoch 26/64\n","Training on val_data\n","     96/Unknown - 168s 2s/step - loss: 0.0146 - accuracy: 0.9965 - false_positives: 34.0000 - false_negatives: 52.0000\n","Epoch 1: saving model to transfomer.h5\n","96/96 [==============================] - 173s 2s/step - loss: 0.0146 - accuracy: 0.9965 - false_positives: 34.0000 - false_negatives: 52.0000\n","Epoch 27/64\n","Training on train_data\n","     96/Unknown - 116s 1s/step - loss: 0.0142 - accuracy: 0.9968 - false_positives: 30.0000 - false_negatives: 45.0000\n","Epoch 1: saving model to transfomer.h5\n","96/96 [==============================] - 121s 1s/step - loss: 0.0142 - accuracy: 0.9968 - false_positives: 30.0000 - false_negatives: 45.0000\n","Epoch 29/64\n","Training on train_data\n","    452/Unknown - 673s 1s/step - loss: 0.0125 - accuracy: 0.9970 - false_positives: 148.0000 - false_negatives: 203.0000\n","Epoch 1: saving model to transfomer.h5\n","452/452 [==============================] - 679s 2s/step - loss: 0.0125 - accuracy: 0.9970 - false_positives: 148.0000 - false_negatives: 203.0000\n","Epoch 30/64\n","Training on val_data\n","     96/Unknown - 143s 1s/step - loss: 0.0156 - accuracy: 0.9965 - false_positives: 34.0000 - false_negatives: 49.0000\n","Epoch 1: saving model to transfomer.h5\n","96/96 [==============================] - 148s 2s/step - loss: 0.0156 - accuracy: 0.9965 - false_positives: 34.0000 - false_negatives: 49.0000\n","Epoch 31/64\n","Training on train_data\n","    452/Unknown - 726s 2s/step - loss: 0.0085 - accuracy: 0.9980 - false_positives: 105.0000 - false_negatives: 133.0000\n","Epoch 1: saving model to transfomer.h5\n","452/452 [==============================] - 732s 2s/step - loss: 0.0085 - accuracy: 0.9980 - false_positives: 105.0000 - false_negatives: 133.0000\n","Epoch 32/64\n","Training on val_data\n","     96/Unknown - 138s 1s/step - loss: 0.0098 - accuracy: 0.9981 - false_positives: 20.0000 - false_negatives: 25.0000\n","Epoch 1: saving model to transfomer.h5\n","96/96 [==============================] - 144s 1s/step - loss: 0.0098 - accuracy: 0.9981 - false_positives: 20.0000 - false_negatives: 25.0000\n","Epoch 33/64\n","Training on train_data\n","    452/Unknown - 647s 1s/step - loss: 0.0172 - accuracy: 0.9951 - false_positives: 236.0000 - false_negatives: 316.0000\n","Epoch 1: saving model to transfomer.h5\n","452/452 [==============================] - 652s 1s/step - loss: 0.0172 - accuracy: 0.9951 - false_positives: 236.0000 - false_negatives: 316.0000\n","Epoch 34/64\n","Training on val_data\n","     96/Unknown - 127s 1s/step - loss: 0.0162 - accuracy: 0.9959 - false_positives: 44.0000 - false_negatives: 61.0000\n","Epoch 1: saving model to transfomer.h5\n","96/96 [==============================] - 132s 1s/step - loss: 0.0162 - accuracy: 0.9959 - false_positives: 44.0000 - false_negatives: 61.0000\n","Epoch 35/64\n","Training on train_data\n","    452/Unknown - 631s 1s/step - loss: 0.0101 - accuracy: 0.9975 - false_positives: 126.0000 - false_negatives: 177.0000\n","Epoch 1: saving model to transfomer.h5\n","452/452 [==============================] - 637s 1s/step - loss: 0.0101 - accuracy: 0.9975 - false_positives: 126.0000 - false_negatives: 177.0000\n","Epoch 36/64\n","Training on val_data\n","     96/Unknown - 120s 1s/step - loss: 0.0115 - accuracy: 0.9969 - false_positives: 33.0000 - false_negatives: 45.0000\n","Epoch 1: saving model to transfomer.h5\n","96/96 [==============================] - 126s 1s/step - loss: 0.0115 - accuracy: 0.9969 - false_positives: 33.0000 - false_negatives: 45.0000\n","Epoch 37/64\n","Training on train_data\n","    452/Unknown - 851s 2s/step - loss: 0.0059 - accuracy: 0.9987 - false_positives: 65.0000 - false_negatives: 84.0000\n","Epoch 1: saving model to transfomer.h5\n","452/452 [==============================] - 856s 2s/step - loss: 0.0059 - accuracy: 0.9987 - false_positives: 65.0000 - false_negatives: 84.0000\n","Epoch 38/64\n","Training on val_data\n","     96/Unknown - 205s 2s/step - loss: 0.0135 - accuracy: 0.9961 - false_positives: 43.0000 - false_negatives: 56.0000\n","Epoch 1: saving model to transfomer.h5\n","96/96 [==============================] - 210s 2s/step - loss: 0.0135 - accuracy: 0.9961 - false_positives: 43.0000 - false_negatives: 56.0000\n","Epoch 39/64\n","Training on train_data\n","    452/Unknown - 829s 2s/step - loss: 0.0122 - accuracy: 0.9970 - false_positives: 148.0000 - false_negatives: 197.0000\n","Epoch 1: saving model to transfomer.h5\n","452/452 [==============================] - 835s 2s/step - loss: 0.0122 - accuracy: 0.9970 - false_positives: 148.0000 - false_negatives: 197.0000\n","Epoch 40/64\n","Training on val_data\n","     96/Unknown - 186s 2s/step - loss: 0.0193 - accuracy: 0.9944 - false_positives: 60.0000 - false_negatives: 76.0000\n","Epoch 1: saving model to transfomer.h5\n","96/96 [==============================] - 191s 2s/step - loss: 0.0193 - accuracy: 0.9944 - false_positives: 60.0000 - false_negatives: 76.0000\n","Epoch 41/64\n","Training on train_data\n","    452/Unknown - 774s 2s/step - loss: 0.0123 - accuracy: 0.9965 - false_positives: 168.0000 - false_negatives: 222.0000\n","Epoch 1: saving model to transfomer.h5\n","452/452 [==============================] - 779s 2s/step - loss: 0.0123 - accuracy: 0.9965 - false_positives: 168.0000 - false_negatives: 222.0000\n","Epoch 42/64\n","Training on val_data\n","     96/Unknown - 120s 1s/step - loss: 0.0120 - accuracy: 0.9965 - false_positives: 38.0000 - false_negatives: 44.0000\n","Epoch 1: saving model to transfomer.h5\n","96/96 [==============================] - 126s 1s/step - loss: 0.0120 - accuracy: 0.9965 - false_positives: 38.0000 - false_negatives: 44.0000\n","Epoch 43/64\n","Training on train_data\n","    452/Unknown - 680s 2s/step - loss: 0.0053 - accuracy: 0.9989 - false_positives: 54.0000 - false_negatives: 70.0000\n","Epoch 1: saving model to transfomer.h5\n","452/452 [==============================] - 685s 2s/step - loss: 0.0053 - accuracy: 0.9989 - false_positives: 54.0000 - false_negatives: 70.0000\n","Epoch 44/64\n","Training on val_data\n","     96/Unknown - 139s 1s/step - loss: 0.0110 - accuracy: 0.9975 - false_positives: 25.0000 - false_negatives: 39.0000\n","Epoch 1: saving model to transfomer.h5\n","96/96 [==============================] - 144s 1s/step - loss: 0.0110 - accuracy: 0.9975 - false_positives: 25.0000 - false_negatives: 39.0000\n","Epoch 45/64\n","Training on train_data\n","    452/Unknown - 606s 1s/step - loss: 0.0099 - accuracy: 0.9974 - false_positives: 129.0000 - false_negatives: 174.0000\n","Epoch 1: saving model to transfomer.h5\n","452/452 [==============================] - 612s 1s/step - loss: 0.0099 - accuracy: 0.9974 - false_positives: 129.0000 - false_negatives: 174.0000\n","Epoch 46/64\n","Training on val_data\n","     96/Unknown - 117s 1s/step - loss: 0.0097 - accuracy: 0.9977 - false_positives: 27.0000 - false_negatives: 29.0000\n","Epoch 1: saving model to transfomer.h5\n","96/96 [==============================] - 123s 1s/step - loss: 0.0097 - accuracy: 0.9977 - false_positives: 27.0000 - false_negatives: 29.0000\n","Epoch 47/64\n","Training on train_data\n","    452/Unknown - 555s 1s/step - loss: 0.0111 - accuracy: 0.9970 - false_positives: 153.0000 - false_negatives: 198.0000\n","Epoch 1: saving model to transfomer.h5\n","452/452 [==============================] - 561s 1s/step - loss: 0.0111 - accuracy: 0.9970 - false_positives: 153.0000 - false_negatives: 198.0000\n","Epoch 48/64\n","Training on val_data\n","     96/Unknown - 115s 1s/step - loss: 0.0116 - accuracy: 0.9971 - false_positives: 30.0000 - false_negatives: 41.0000\n","Epoch 1: saving model to transfomer.h5\n","96/96 [==============================] - 121s 1s/step - loss: 0.0116 - accuracy: 0.9971 - false_positives: 30.0000 - false_negatives: 41.0000\n","Epoch 49/64\n","Training on train_data\n","    452/Unknown - 574s 1s/step - loss: 0.0116 - accuracy: 0.9968 - false_positives: 160.0000 - false_negatives: 212.0000\n","Epoch 1: saving model to transfomer.h5\n","452/452 [==============================] - 580s 1s/step - loss: 0.0116 - accuracy: 0.9968 - false_positives: 160.0000 - false_negatives: 212.0000\n","Epoch 50/64\n","Training on val_data\n","     96/Unknown - 126s 1s/step - loss: 0.0090 - accuracy: 0.9973 - false_positives: 25.0000 - false_negatives: 35.0000\n","Epoch 1: saving model to transfomer.h5\n","96/96 [==============================] - 132s 1s/step - loss: 0.0090 - accuracy: 0.9973 - false_positives: 25.0000 - false_negatives: 35.0000\n","Epoch 51/64\n","Training on train_data\n","    452/Unknown - 626s 1s/step - loss: 0.0075 - accuracy: 0.9980 - false_positives: 100.0000 - false_negatives: 131.0000\n","Epoch 1: saving model to transfomer.h5\n","452/452 [==============================] - 631s 1s/step - loss: 0.0075 - accuracy: 0.9980 - false_positives: 100.0000 - false_negatives: 131.0000\n","Epoch 52/64\n","Training on val_data\n","     96/Unknown - 190s 2s/step - loss: 0.0093 - accuracy: 0.9975 - false_positives: 27.0000 - false_negatives: 35.0000\n","Epoch 1: saving model to transfomer.h5\n","96/96 [==============================] - 195s 2s/step - loss: 0.0093 - accuracy: 0.9975 - false_positives: 27.0000 - false_negatives: 35.0000\n","Epoch 53/64\n","Training on train_data\n","    452/Unknown - 726s 2s/step - loss: 0.0075 - accuracy: 0.9978 - false_positives: 116.0000 - false_negatives: 141.0000\n","Epoch 1: saving model to transfomer.h5\n","452/452 [==============================] - 732s 2s/step - loss: 0.0075 - accuracy: 0.9978 - false_positives: 116.0000 - false_negatives: 141.0000\n","Epoch 54/64\n","Training on val_data\n","     96/Unknown - 120s 1s/step - loss: 0.0090 - accuracy: 0.9973 - false_positives: 29.0000 - false_negatives: 36.0000\n","Epoch 1: saving model to transfomer.h5\n","96/96 [==============================] - 125s 1s/step - loss: 0.0090 - accuracy: 0.9973 - false_positives: 29.0000 - false_negatives: 36.0000\n","Epoch 55/64\n","Training on train_data\n","    452/Unknown - 557s 1s/step - loss: 0.0041 - accuracy: 0.9991 - false_positives: 44.0000 - false_negatives: 60.0000\n","Epoch 1: saving model to transfomer.h5\n","452/452 [==============================] - 562s 1s/step - loss: 0.0041 - accuracy: 0.9991 - false_positives: 44.0000 - false_negatives: 60.0000\n","Epoch 56/64\n","Training on val_data\n","     96/Unknown - 113s 1s/step - loss: 0.0180 - accuracy: 0.9953 - false_positives: 51.0000 - false_negatives: 64.0000\n","Epoch 1: saving model to transfomer.h5\n","96/96 [==============================] - 118s 1s/step - loss: 0.0180 - accuracy: 0.9953 - false_positives: 51.0000 - false_negatives: 64.0000\n","Epoch 57/64\n","Training on train_data\n","    452/Unknown - 768s 2s/step - loss: 0.0109 - accuracy: 0.9971 - false_positives: 151.0000 - false_negatives: 190.0000\n","Epoch 1: saving model to transfomer.h5\n","452/452 [==============================] - 773s 2s/step - loss: 0.0109 - accuracy: 0.9971 - false_positives: 151.0000 - false_negatives: 190.0000\n","Epoch 58/64\n","Training on val_data\n","     96/Unknown - 205s 2s/step - loss: 0.0098 - accuracy: 0.9973 - false_positives: 29.0000 - false_negatives: 35.0000\n","Epoch 1: saving model to transfomer.h5\n","96/96 [==============================] - 211s 2s/step - loss: 0.0098 - accuracy: 0.9973 - false_positives: 29.0000 - false_negatives: 35.0000\n","Epoch 59/64\n","Training on train_data\n","    452/Unknown - 909s 2s/step - loss: 0.0113 - accuracy: 0.9967 - false_positives: 162.0000 - false_negatives: 213.0000\n","Epoch 1: saving model to transfomer.h5\n","452/452 [==============================] - 915s 2s/step - loss: 0.0113 - accuracy: 0.9967 - false_positives: 162.0000 - false_negatives: 213.0000\n","Epoch 60/64\n","Training on val_data\n","     96/Unknown - 182s 2s/step - loss: 0.0101 - accuracy: 0.9968 - false_positives: 32.0000 - false_negatives: 45.0000\n","Epoch 1: saving model to transfomer.h5\n","96/96 [==============================] - 187s 2s/step - loss: 0.0101 - accuracy: 0.9968 - false_positives: 32.0000 - false_negatives: 45.0000\n","Epoch 61/64\n","Training on train_data\n","    452/Unknown - 777s 2s/step - loss: 0.0062 - accuracy: 0.9986 - false_positives: 72.0000 - false_negatives: 87.0000\n","Epoch 1: saving model to transfomer.h5\n","452/452 [==============================] - 783s 2s/step - loss: 0.0062 - accuracy: 0.9986 - false_positives: 72.0000 - false_negatives: 87.0000\n","Epoch 62/64\n","Training on val_data\n","     96/Unknown - 149s 2s/step - loss: 0.0074 - accuracy: 0.9982 - false_positives: 21.0000 - false_negatives: 24.0000\n","Epoch 1: saving model to transfomer.h5\n","96/96 [==============================] - 155s 2s/step - loss: 0.0074 - accuracy: 0.9982 - false_positives: 21.0000 - false_negatives: 24.0000\n","Epoch 63/64\n","Training on train_data\n","    452/Unknown - 766s 2s/step - loss: 0.0051 - accuracy: 0.9988 - false_positives: 60.0000 - false_negatives: 77.0000\n","Epoch 1: saving model to transfomer.h5\n","452/452 [==============================] - 771s 2s/step - loss: 0.0051 - accuracy: 0.9988 - false_positives: 60.0000 - false_negatives: 77.0000\n","Epoch 64/64\n","Training on val_data\n","     96/Unknown - 185s 2s/step - loss: 0.0088 - accuracy: 0.9977 - false_positives: 24.0000 - false_negatives: 31.0000\n","Epoch 1: saving model to transfomer.h5\n","96/96 [==============================] - 190s 2s/step - loss: 0.0088 - accuracy: 0.9977 - false_positives: 24.0000 - false_negatives: 31.0000\n"]}],"source":["import os\n","epochs = 64\n","\n","weights_path = '/home/u200810216/jupyter/kaggle_m3cv/'\n","initial_weights_file = 'vision_transfomer_test_kaggle_batch128_weights_val_3_train_3_roll_6.h5'\n","final_weights_file = f'vision_transfomer_test_kaggle_batch128_weights_val_3_train_3_roll_6_roll{epochs}.h5'\n","\n","# 加载预训练权重\n","model.load_weights(os.path.join(weights_path, initial_weights_file))\n","\n","# 训练循环\n","for epoch in range(epochs):\n","    print(f\"Epoch {epoch + 1}/{epochs}\")\n","\n","    if epoch % 2 == 0:\n","        print(\"Training on train_data\")\n","        data = train_data\n","    else:\n","        print(\"Training on val_data\")\n","        data = val_data\n","\n","    history = model.fit(data,\n","                        epochs=1,\n","                        verbose=1,\n","                        callbacks=[tensorboard_callback, checkpoint_callback])\n","\n","    # 保存模型参数\n","    model.save_weights(os.path.join(weights_path, \"{}_epoch_{}.h5\".format(final_weights_file.replace(\".h5\",\"\"), epoch+1)))\n","\n","# 保存最终模型参数\n","model.save_weights(os.path.join(weights_path, final_weights_file))\n"]},{"cell_type":"markdown","metadata":{"id":"O7FK8b6GJYnu"},"source":["## 结果预测与提交"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NXecYGX0JYnu","outputId":"e88ee59f-6853-46fc-e6f3-0e4ee89a3e6a"},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|██████████| 52342/52342 [2:00:04<00:00,  7.26it/s]  \n"]}],"source":["def transform_and_save_test_data(test_img_list, test_images_dir, save_dir):\n","    for img in tqdm(test_img_list):\n","        eeg_path = os.path.join(test_images_dir, img + '.mat')\n","        data = load_eeg(eeg_path)\n","\n","        data = transform(data)  # 假设你有一个transform函数来进行数据预处理\n","\n","        np.save(os.path.join(save_dir, img), data)  # 保存预处理过的数据为.npy文件\n","\n","test_images_dir = '/home/u200810216/jupyter/kaggle_m3cv/test/'\n","save_test_dir = '/home/u200810216/jupyter/kaggle_m3cv/test_npy/'\n","# Read the test data\n","test_image = pd.read_csv('/home/u200810216/jupyter/kaggle_m3cv/Testing_Info.csv')\n","test_image_path_list = test_image['Row'].values\n","\n","\n","# 创建保存的文件夹\n","os.makedirs(save_test_dir, exist_ok=True)\n","\n","# 执行预处理并保存\n","transform_and_save_test_data(test_image_path_list, test_images_dir, save_test_dir)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1UPxuQ3JJYnu","outputId":"b7d1c3ae-e387-486b-9b3a-7b05cecdbd1b"},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|██████████| 103/103 [22:39<00:00, 13.20s/it]\n"]},{"name":"stdout","output_type":"stream","text":["87\n","71\n","33\n","39\n","76\n","32\n","73\n","95\n","39\n","51\n","71\n","33\n","83\n","33\n","87\n","84\n","39\n","71\n","33\n","51\n","33\n","33\n","39\n","71\n","32\n","45\n","33\n","33\n","39\n","57\n","71\n","62\n","58\n","73\n","57\n","73\n","32\n","24\n","76\n","39\n","31\n","76\n","33\n","76\n","39\n","76\n","39\n","53\n","87\n","78\n","53\n","76\n","51\n","21\n","65\n","51\n","83\n","76\n","39\n","52\n","44\n","71\n"]}],"source":["from tqdm import tqdm\n","\n","\n","def load_data(data_path):\n","    data = np.load(data_path)\n","    return data\n","\n","batch_size = 512\n","\n","pre_list = []\n","# Read the test data\n","test_image = pd.read_csv('/home/u200810216/jupyter/kaggle_m3cv/Testing_Info.csv')\n","test_image_path_list = test_image['Row'].values\n","data_dir = '/home/u200810216/jupyter/kaggle_m3cv/test_npy/'\n","data_list = [os.path.join(data_dir, img + '.npy') for img in test_image_path_list]\n","\n","for i in tqdm(range(0, len(data_list), batch_size)):\n","    batch_data_list = data_list[i:i+batch_size]\n","    batch_data = np.array([load_data(data_path) for data_path in batch_data_list]).astype('float32')\n","    batch_data = batch_data[..., np.newaxis]\n","\n","    batch_data_tensor = tf.convert_to_tensor(batch_data)\n","    batch_out = model(batch_data_tensor)\n","    batch_pre_list = np.argmax(batch_out.numpy(), axis=-1) + 1\n","\n","    pre_list.extend(batch_pre_list)\n","\n","# 以下的后处理和保存结果的代码可以保持不变。\n","# Post-processing and saving results\n","img_test = pd.DataFrame(test_image_path_list, columns=[\"EpochID\"])\n","img_pre = pd.DataFrame(test_image_path_list, columns=[\"EpochID\"])\n","img_pre['SubjectID'] = pre_list\n","pre_info = img_pre['SubjectID'].values\n","test_info = test_image['SubjectID'].values\n","\n","result_cnn = list()\n","\n","for i, j in zip(test_info, pre_info):\n","    if i == 'None':\n","        result_cnn.append(j)\n","    elif int(i[4:])==j :\n","        print(i[4:])\n","        result_cnn.append(int(1))\n","    else:\n","        result_cnn.append(int(0))\n","\n","img_test['Prediction'] = result_cnn\n","img_test.to_csv('result_vit_kaggle_finetuning_37_5.csv', index=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mGbpSu2MJYnv"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":["GmwMXz0UWCqy"],"provenance":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.4"},"toc":{"base_numbering":1,"nav_menu":{},"number_sections":true,"sideBar":true,"skip_h1_title":false,"title_cell":"Table of Contents","title_sidebar":"Contents","toc_cell":true,"toc_position":{"height":"calc(100% - 180px)","left":"10px","top":"150px","width":"284px"},"toc_section_display":true,"toc_window_display":true}},"nbformat":4,"nbformat_minor":0}